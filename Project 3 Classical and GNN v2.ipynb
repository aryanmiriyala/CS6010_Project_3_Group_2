{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f74054a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.datasets import TUDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6983ecdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved under: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\results\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Central directory where all CSV outputs will go\n",
    "PROJECT_ROOT = Path(\".\").resolve()\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Results will be saved under:\", RESULTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1f91f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mutag_dataset(root_dir=\"data\"):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "      Load the MUTAG dataset as a PyG TUDataset.\n",
    "\n",
    "    HOW:\n",
    "      - TUDataset will download & preprocess MUTAG into PyG's Data objects.\n",
    "      - Each item is a small molecule graph with:\n",
    "          * data.x        : node features (7-dim one-hot atom type)\n",
    "          * data.edge_index: edges (2 x num_edges)\n",
    "          * data.edge_attr : edge features (4-dim one-hot bond type)\n",
    "          * data.y        : graph label (0/1)\n",
    "\n",
    "    WHY:\n",
    "      We need this representation so we can:\n",
    "        1) convert each graph to gSpan's text format\n",
    "        2) later align gSpan’s patterns back to graph indices.\n",
    "    \"\"\"\n",
    "    dataset = TUDataset(root=root_dir, name=\"MUTAG\")\n",
    "    print(f\"Loaded MUTAG with {len(dataset)} graphs\")\n",
    "    print(f\"Node feature dim:  {dataset.num_features}\")\n",
    "    print(f\"Edge feature dim:  {dataset.num_edge_features}\")\n",
    "    print(f\"Number of classes: {dataset.num_classes}\")\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "01342a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_node_label_vector(x_row):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "      Convert a node's feature vector into a single discrete label.\n",
    "\n",
    "    HOW:\n",
    "      - In MUTAG, each node feature row is one-hot over 7 atom types.\n",
    "      - argmax gives an integer in [0, 6] representing the atom type.\n",
    "\n",
    "    WHY:\n",
    "      gSpan only accepts one integer label per node.\n",
    "    \"\"\"\n",
    "    if x_row is None:\n",
    "        # Fallback if no node features exist (not the case for MUTAG)\n",
    "        return 0\n",
    "    return int(torch.argmax(x_row).item())\n",
    "\n",
    "\n",
    "def infer_edge_label_vector(edge_attr_row):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "      Convert an edge feature vector into a single discrete label.\n",
    "\n",
    "    HOW:\n",
    "      - In MUTAG, each edge feature row is one-hot over bond types.\n",
    "      - argmax gives an integer in [0, 3] representing the bond type.\n",
    "\n",
    "    WHY:\n",
    "      gSpan expects a single integer label per edge.\n",
    "    \"\"\"\n",
    "    if edge_attr_row is None:\n",
    "        return 0\n",
    "    return int(torch.argmax(edge_attr_row).item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7396fb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_graphs_to_gspan_file(dataset, out_path):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "      Write all graphs in the dataset into a single text file in gSpan format.\n",
    "\n",
    "    FORMAT (per graph):\n",
    "        t # <graph_id>\n",
    "        v <node_id> <node_label>\n",
    "        e <u> <v> <edge_label>\n",
    "\n",
    "    HOW:\n",
    "      - Iterate over all graphs (molecules) in MUTAG.\n",
    "      - For each:\n",
    "          * write 't # i'\n",
    "          * write one 'v' line per node\n",
    "          * deduplicate edges and write one 'e' line per undirected edge\n",
    "\n",
    "    WHY:\n",
    "      gSpan takes a \"graph database\" text file as input, not a Python object.\n",
    "    \"\"\"\n",
    "    out_path = Path(out_path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    with out_path.open(\"w\") as f:\n",
    "        for graph_id, data in enumerate(dataset):\n",
    "            # Start a new graph block in the file\n",
    "            f.write(f\"t # {graph_id}\\n\")\n",
    "\n",
    "            # ---------- Node lines ('v') ----------\n",
    "            num_nodes = data.num_nodes\n",
    "            x = getattr(data, \"x\", None)\n",
    "\n",
    "            for node_id in range(num_nodes):\n",
    "                if x is not None:\n",
    "                    node_label = infer_node_label_vector(x[node_id])\n",
    "                else:\n",
    "                    node_label = 0  # fallback if no features\n",
    "                f.write(f\"v {node_id} {node_label}\\n\")\n",
    "\n",
    "            # ---------- Edge lines ('e') ----------\n",
    "            edge_index = data.edge_index      # shape [2, num_edges]\n",
    "            edge_attr  = getattr(data, \"edge_attr\", None)\n",
    "\n",
    "            # Use a set to deduplicate undirected edges\n",
    "            undirected_edges = set()\n",
    "\n",
    "            num_edges = edge_index.size(1)\n",
    "            for e_idx in range(num_edges):\n",
    "                u = int(edge_index[0, e_idx])\n",
    "                v = int(edge_index[1, e_idx])\n",
    "\n",
    "                # Skip self-loops if any\n",
    "                if u == v:\n",
    "                    continue\n",
    "\n",
    "                # Enforce u < v for undirected representation\n",
    "                a, b = sorted((u, v))\n",
    "\n",
    "                if edge_attr is not None:\n",
    "                    edge_label = infer_edge_label_vector(edge_attr[e_idx])\n",
    "                else:\n",
    "                    edge_label = 0\n",
    "\n",
    "                undirected_edges.add((a, b, edge_label))\n",
    "\n",
    "            # Actually write the edge lines\n",
    "            for a, b, edge_label in sorted(undirected_edges):\n",
    "                f.write(f\"e {a} {b} {edge_label}\\n\")\n",
    "\n",
    "    print(f\"Wrote gSpan database file to: {out_path.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cf7b3e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded MUTAG with 188 graphs\n",
      "Node feature dim:  7\n",
      "Edge feature dim:  4\n",
      "Number of classes: 2\n",
      "Wrote gSpan database file to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan.txt\n"
     ]
    }
   ],
   "source": [
    "dataset = load_mutag_dataset(root_dir=\"data\")\n",
    "write_graphs_to_gspan_file(dataset, \"mutag_gspan.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c847736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6d4960b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\default.LAPTOP-D71TUC29\\anaconda3\\python.exe\n",
      "Name: gspan-mining\n",
      "Version: 0.2.3\n",
      "Summary: Implementation of frequent subgraph mining algorithm gSpan\n",
      "Home-page: https://github.com/betterenvi/gSpan\n",
      "Author: Qingying Chen\n",
      "Author-email: qychen.pku@gmail.com\n",
      "License: UNKNOWN\n",
      "Location: c:\\Users\\default.LAPTOP-D71TUC29\\anaconda3\\Lib\\site-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "usage: __main__.py [-h] [-s MIN_SUPPORT] [-n NUM_GRAPHS]\n",
      "                   [-l LOWER_BOUND_OF_NUM_VERTICES]\n",
      "                   [-u UPPER_BOUND_OF_NUM_VERTICES] [-d DIRECTED] [-v VERBOSE]\n",
      "                   [-p PLOT] [-w WHERE]\n",
      "                   database_file_name\n",
      "\n",
      "positional arguments:\n",
      "  database_file_name    str, database file name\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  -s MIN_SUPPORT, --min_support MIN_SUPPORT\n",
      "                        min support, default 5000\n",
      "  -n NUM_GRAPHS, --num_graphs NUM_GRAPHS\n",
      "                        only read the first n graphs in the given database,\n",
      "                        default inf, i.e. all graphs\n",
      "  -l LOWER_BOUND_OF_NUM_VERTICES, --lower_bound_of_num_vertices LOWER_BOUND_OF_NUM_VERTICES\n",
      "                        int, lower bound of number of vertices of output\n",
      "                        subgraph, default 2\n",
      "  -u UPPER_BOUND_OF_NUM_VERTICES, --upper_bound_of_num_vertices UPPER_BOUND_OF_NUM_VERTICES\n",
      "                        int, upper bound of number of vertices of output\n",
      "                        subgraph, default inf\n",
      "  -d DIRECTED, --directed DIRECTED\n",
      "                        bool, run for directed graphs, default off, i.e.\n",
      "                        undirected graphs\n",
      "  -v VERBOSE, --verbose VERBOSE\n",
      "                        bool, verbose output, default off\n",
      "  -p PLOT, --plot PLOT  bool, whether plot frequent subgraph, default off\n",
      "  -w WHERE, --where WHERE\n",
      "                        bool, output where one frequent subgraph appears in\n",
      "                        database, default off\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "!{sys.executable} -m pip show gspan-mining\n",
    "!{sys.executable} -m gspan_mining -h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "d041d95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def run_gspan(\n",
    "    input_file=\"mutag_gspan.txt\",\n",
    "    output_file=\"mutag_gspan_out.txt\",\n",
    "    min_support=20,\n",
    "    max_graphs=1000000,\n",
    "    undirected=True,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run gSpan on the given input file and save the output to output_file.\n",
    "\n",
    "    - Always writes stdout to output_file (even if gSpan exits with a non-zero code).\n",
    "    - Prints the first ~100 lines of gSpan's stdout/stderr for inspection.\n",
    "    - Warns if gSpan exits with a non-zero return code, but does NOT raise.\n",
    "\n",
    "    This keeps the pipeline moving so we can still parse patterns from the log.\n",
    "    \"\"\"\n",
    "    input_path = Path(input_file).resolve()\n",
    "    if not input_path.exists():\n",
    "        raise FileNotFoundError(f\"Input file not found: {input_path}\")\n",
    "\n",
    "    out_path = Path(output_file).resolve()\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Build the command: note the -w True so we get 'where:' lines in the output\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        \"-m\", \"gspan_mining\",\n",
    "        \"-s\", str(min_support),\n",
    "        \"-n\", str(max_graphs),\n",
    "        \"-d\", \"False\" if undirected else \"True\",  # False = treat as UNDIRECTED\n",
    "        \"-w\", \"True\",                             # IMPORTANT: output 'where' information\n",
    "        str(input_path)\n",
    "    ]\n",
    "\n",
    "    print(\"Running command:\")\n",
    "    print(\"  \" + \" \".join(cmd))\n",
    "\n",
    "    # Capture stdout and stderr together\n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True\n",
    "    )\n",
    "\n",
    "    # Always dump gSpan output to the requested file\n",
    "    with out_path.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        f_out.write(result.stdout)\n",
    "\n",
    "    # Show the first 100 lines in the notebook for debugging\n",
    "    print(\"\\n=== gSpan stdout/stderr (first 100 lines) ===\")\n",
    "    lines = result.stdout.splitlines()\n",
    "    for line in lines[:100]:\n",
    "        print(line)\n",
    "    if len(lines) > 100:\n",
    "        print(\"... (truncated) ...\")\n",
    "\n",
    "    # If returncode != 0, just warn but DO NOT raise\n",
    "    if result.returncode != 0:\n",
    "        print(f\"\\n[WARNING] gSpan exited with code {result.returncode}.\")\n",
    "        print(\"         Check the log above or in the output file if something looks off.\")\n",
    "    else:\n",
    "        print(\"\\n gSpan finished successfully.\")\n",
    "\n",
    "    print(f\"Output written to: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a3beb2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def parse_gspan_where_output_to_matrix(output_file, num_graphs):\n",
    "    output_file = Path(output_file)\n",
    "\n",
    "    if not output_file.exists():\n",
    "        # This catches “file not created” issues *right here*\n",
    "        raise FileNotFoundError(f\"gSpan output file not found: {output_file.resolve()}\")\n",
    "\n",
    "    pattern_graph_ids = []\n",
    "    current_pattern_idx = -1\n",
    "\n",
    "    with output_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for raw_line in f:\n",
    "            line = raw_line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            if line.startswith(\"t \"):\n",
    "                current_pattern_idx += 1\n",
    "                pattern_graph_ids.append([])\n",
    "\n",
    "            elif line.startswith(\"where:\"):\n",
    "                if current_pattern_idx < 0:\n",
    "                    continue\n",
    "                part = line.split(\"where:\", 1)[1].strip()\n",
    "                part = part.strip(\"[]\")\n",
    "                gids = []\n",
    "                if part:\n",
    "                    for token in part.split(\",\"):\n",
    "                        token = token.strip()\n",
    "                        if not token:\n",
    "                            continue\n",
    "                        gid = int(token)\n",
    "                        gids.append(gid)\n",
    "                pattern_graph_ids[current_pattern_idx] = gids\n",
    "\n",
    "    num_patterns = len(pattern_graph_ids)\n",
    "    print(f\"Parsed {num_patterns} patterns from {output_file}\")\n",
    "\n",
    "    X = np.zeros((num_graphs, num_patterns), dtype=np.float32)\n",
    "    for p_idx, gids in enumerate(pattern_graph_ids):\n",
    "        for gid in gids:\n",
    "            if gid < num_graphs:\n",
    "                X[gid, p_idx] = 1.0\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "94dc5a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num graphs in MUTAG: 188\n",
      "Parsed 39492 patterns from C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out.txt\n",
      "Feature DataFrame shape: (188, 39494)\n",
      "   graph_id  label  pattern_0  pattern_1  pattern_2  pattern_3  pattern_4  \\\n",
      "0         0      1        1.0        1.0        1.0        1.0        1.0   \n",
      "1         1      0        1.0        1.0        1.0        1.0        1.0   \n",
      "2         2      0        1.0        1.0        1.0        1.0        1.0   \n",
      "3         3      1        1.0        1.0        1.0        1.0        1.0   \n",
      "4         4      0        1.0        1.0        1.0        1.0        1.0   \n",
      "\n",
      "   pattern_5  pattern_6  pattern_7  ...  pattern_39482  pattern_39483  \\\n",
      "0        1.0        1.0        1.0  ...            1.0            1.0   \n",
      "1        1.0        1.0        1.0  ...            1.0            1.0   \n",
      "2        1.0        1.0        1.0  ...            1.0            1.0   \n",
      "3        1.0        1.0        1.0  ...            1.0            1.0   \n",
      "4        1.0        0.0        0.0  ...            1.0            1.0   \n",
      "\n",
      "   pattern_39484  pattern_39485  pattern_39486  pattern_39487  pattern_39488  \\\n",
      "0            0.0            0.0            0.0            0.0            0.0   \n",
      "1            0.0            0.0            0.0            0.0            0.0   \n",
      "2            0.0            0.0            0.0            0.0            0.0   \n",
      "3            1.0            0.0            0.0            0.0            0.0   \n",
      "4            0.0            0.0            0.0            0.0            0.0   \n",
      "\n",
      "   pattern_39489  pattern_39490  pattern_39491  \n",
      "0            0.0            0.0            0.0  \n",
      "1            0.0            0.0            0.0  \n",
      "2            0.0            0.0            0.0  \n",
      "3            0.0            0.0            0.0  \n",
      "4            0.0            0.0            0.0  \n",
      "\n",
      "[5 rows x 39494 columns]\n",
      "Saved feature CSV to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_features_support20.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "# 1) Load MUTAG to get labels\n",
    "dataset = TUDataset(root=\"data\", name=\"MUTAG\")\n",
    "num_graphs = len(dataset)\n",
    "print(\"Num graphs in MUTAG:\", num_graphs)\n",
    "\n",
    "# 2) Path to your gSpan output (the text file you already have)\n",
    "GSPAN_OUTPUT_FILE = r\"C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out.txt\"\n",
    "\n",
    "# 3) Parse gSpan output into feature matrix X\n",
    "X = parse_gspan_where_output_to_matrix(GSPAN_OUTPUT_FILE, num_graphs)\n",
    "\n",
    "# 4) Build label vector y from MUTAG\n",
    "y = np.array([int(data.y.item()) for data in dataset], dtype=np.int64)\n",
    "\n",
    "# 5) Build a DataFrame: graph_id, label, and feature columns\n",
    "feature_names = [f\"pattern_{j}\" for j in range(X.shape[1])]\n",
    "\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df.insert(0, \"graph_id\", np.arange(num_graphs))\n",
    "df.insert(1, \"label\", y)\n",
    "\n",
    "print(\"Feature DataFrame shape:\", df.shape)\n",
    "print(df.head())\n",
    "\n",
    "# 6) Save to CSV in your project folder\n",
    "FEATURE_CSV = r\"C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_features_support20.csv\"\n",
    "df.to_csv(FEATURE_CSV, index=False)\n",
    "print(\"Saved feature CSV to:\", FEATURE_CSV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c897a188",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_FILE = r\"C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_features_support20.csv\"\n",
    "\n",
    "GRAPH_ID_COL = \"graph_id\"\n",
    "LABEL_COL    = \"label\"\n",
    "\n",
    "df = pd.read_csv(FEATURE_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "92faf8b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating features for min_support = 19 ===\n",
      "Running command:\n",
      "  c:\\Users\\default.LAPTOP-D71TUC29\\anaconda3\\python.exe -m gspan_mining -s 19 -n 1000000 -d False -w True C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan.txt\n",
      "\n",
      "=== gSpan stdout/stderr (first 100 lines) ===\n",
      "t # 0\n",
      "v 0 0\n",
      "v 1 0\n",
      "e 0 1 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 1\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 2\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 3\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 4\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "e 4 5 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 5\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "e 0 1 0\n",
      "e 0 5 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "e 4 5 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 6\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "v 6 0\n",
      "e 0 1 0\n",
      "e 0 5 0\n",
      "e 1 2 0\n",
      "... (truncated) ...\n",
      "\n",
      "[WARNING] gSpan exited with code 1.\n",
      "         Check the log above or in the output file if something looks off.\n",
      "Output written to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out_support19.txt\n",
      "Exists after run?: True\n",
      "Parsed 40220 patterns from C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out_support19.txt\n",
      "Saved CSV for support=19 to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_features_support19.csv\n",
      "\n",
      "=== Generating features for min_support = 38 ===\n",
      "Running command:\n",
      "  c:\\Users\\default.LAPTOP-D71TUC29\\anaconda3\\python.exe -m gspan_mining -s 38 -n 1000000 -d False -w True C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan.txt\n",
      "\n",
      "=== gSpan stdout/stderr (first 100 lines) ===\n",
      "t # 0\n",
      "v 0 0\n",
      "v 1 0\n",
      "e 0 1 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 1\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 2\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 3\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 4\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "e 4 5 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 5\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "e 0 1 0\n",
      "e 0 5 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "e 4 5 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 6\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "v 6 0\n",
      "e 0 1 0\n",
      "e 0 5 0\n",
      "e 1 2 0\n",
      "... (truncated) ...\n",
      "\n",
      "[WARNING] gSpan exited with code 1.\n",
      "         Check the log above or in the output file if something looks off.\n",
      "Output written to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out_support38.txt\n",
      "Exists after run?: True\n",
      "Parsed 4751 patterns from C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out_support38.txt\n",
      "Saved CSV for support=38 to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_features_support38.csv\n",
      "\n",
      "=== Generating features for min_support = 57 ===\n",
      "Running command:\n",
      "  c:\\Users\\default.LAPTOP-D71TUC29\\anaconda3\\python.exe -m gspan_mining -s 57 -n 1000000 -d False -w True C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan.txt\n",
      "\n",
      "=== gSpan stdout/stderr (first 100 lines) ===\n",
      "t # 0\n",
      "v 0 0\n",
      "v 1 0\n",
      "e 0 1 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 1\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 2\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 3\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 4\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "e 4 5 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 5\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "e 0 1 0\n",
      "e 0 5 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "e 4 5 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 6\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "v 6 0\n",
      "e 0 1 0\n",
      "e 0 5 0\n",
      "e 1 2 0\n",
      "... (truncated) ...\n",
      "\n",
      "[WARNING] gSpan exited with code 1.\n",
      "         Check the log above or in the output file if something looks off.\n",
      "Output written to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out_support57.txt\n",
      "Exists after run?: True\n",
      "Parsed 637 patterns from C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out_support57.txt\n",
      "Saved CSV for support=57 to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_features_support57.csv\n",
      "\n",
      "=== Generating features for min_support = 76 ===\n",
      "Running command:\n",
      "  c:\\Users\\default.LAPTOP-D71TUC29\\anaconda3\\python.exe -m gspan_mining -s 76 -n 1000000 -d False -w True C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan.txt\n",
      "\n",
      "=== gSpan stdout/stderr (first 100 lines) ===\n",
      "t # 0\n",
      "v 0 0\n",
      "v 1 0\n",
      "e 0 1 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 1\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 2\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 3\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 4\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "e 4 5 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 5\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "e 0 1 0\n",
      "e 0 5 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "e 4 5 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 6\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "v 6 0\n",
      "e 0 1 0\n",
      "e 0 5 0\n",
      "e 1 2 0\n",
      "... (truncated) ...\n",
      "\n",
      "[WARNING] gSpan exited with code 1.\n",
      "         Check the log above or in the output file if something looks off.\n",
      "Output written to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out_support76.txt\n",
      "Exists after run?: True\n",
      "Parsed 110 patterns from C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out_support76.txt\n",
      "Saved CSV for support=76 to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_features_support76.csv\n",
      "\n",
      "=== Generating features for min_support = 94 ===\n",
      "Running command:\n",
      "  c:\\Users\\default.LAPTOP-D71TUC29\\anaconda3\\python.exe -m gspan_mining -s 94 -n 1000000 -d False -w True C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan.txt\n",
      "\n",
      "=== gSpan stdout/stderr (first 100 lines) ===\n",
      "t # 0\n",
      "v 0 0\n",
      "v 1 0\n",
      "e 0 1 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 1\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 2\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "\n",
      "Support: 174\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 3\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 4\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "e 0 1 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "e 4 5 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 5\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "e 0 1 0\n",
      "e 0 5 0\n",
      "e 1 2 0\n",
      "e 2 3 0\n",
      "e 3 4 0\n",
      "e 4 5 0\n",
      "\n",
      "Support: 173\n",
      "where: [0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 138, 139, 140, 141, 142, 143, 144, 145, 147, 148, 151, 152, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 172, 173, 174, 175, 176, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "\n",
      "-----------------\n",
      "\n",
      "t # 6\n",
      "v 0 0\n",
      "v 1 0\n",
      "v 2 0\n",
      "v 3 0\n",
      "v 4 0\n",
      "v 5 0\n",
      "v 6 1\n",
      "e 0 1 0\n",
      "e 0 5 0\n",
      "e 1 2 0\n",
      "... (truncated) ...\n",
      "\n",
      "[WARNING] gSpan exited with code 1.\n",
      "         Check the log above or in the output file if something looks off.\n",
      "Output written to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out_support94.txt\n",
      "Exists after run?: True\n",
      "Parsed 74 patterns from C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_gspan_out_support94.txt\n",
      "Saved CSV for support=94 to: C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_features_support94.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "# 1) Load MUTAG to get labels\n",
    "dataset = TUDataset(root=\"data\", name=\"MUTAG\")\n",
    "num_graphs = len(dataset)\n",
    "y = np.array([int(data.y.item()) for data in dataset], dtype=np.int64)\n",
    "\n",
    "# 2) Supports you want to test\n",
    "supports_to_generate = [19, 38, 57, 76, 94]\n",
    "\n",
    "# 3) Base directory where you want all outputs\n",
    "BASE_DIR = r\"C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\"\n",
    "\n",
    "for s in supports_to_generate:\n",
    "    print(f\"\\n=== Generating features for min_support = {s} ===\")\n",
    "\n",
    "    # (a) Output path for this run\n",
    "    gspan_out_file = Path(BASE_DIR) / f\"mutag_gspan_out_support{s}.txt\"\n",
    "\n",
    "    # (b) Run gSpan; this will always write to gspan_out_file now\n",
    "    run_gspan(\n",
    "        input_file=\"mutag_gspan.txt\",     # adjust if you moved this file\n",
    "        output_file=str(gspan_out_file),\n",
    "        min_support=s,\n",
    "        max_graphs=1000000,\n",
    "        undirected=True,\n",
    "    )\n",
    "\n",
    "    # Quick sanity check: the file should now exist\n",
    "    print(\"Exists after run?:\", gspan_out_file.exists())\n",
    "\n",
    "    # (c) Parse gSpan output into X\n",
    "    X_s = parse_gspan_where_output_to_matrix(gspan_out_file, num_graphs)\n",
    "\n",
    "    # (d) Build DataFrame and save CSV\n",
    "    feature_names = [f\"pattern_{j}\" for j in range(X_s.shape[1])]\n",
    "    df_s = pd.DataFrame(X_s, columns=feature_names)\n",
    "    df_s.insert(0, \"graph_id\", np.arange(num_graphs))\n",
    "    df_s.insert(1, \"label\", y)\n",
    "\n",
    "    feature_csv = Path(BASE_DIR) / f\"mutag_features_support{s}.csv\"\n",
    "    df_s.to_csv(feature_csv, index=False)\n",
    "    print(f\"Saved CSV for support={s} to: {feature_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "782d0952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X (graphs x subgraph features): (188, 39492)\n",
      "Number of labels: 188\n",
      "First 5 feature names: ['pattern_0', 'pattern_1', 'pattern_2', 'pattern_3', 'pattern_4']\n",
      "\n",
      "Train size: 150 Test size: 38\n",
      "\n",
      "Fitting SVM GridSearchCV...\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "SVM best params: {'svm__C': 25, 'svm__gamma': 'scale', 'svm__kernel': 'rbf'}\n",
      "SVM CV best F1: 0.8560\n",
      "SVM training time (s): 14.74\n",
      "\n",
      "========== SVM (best): Test metrics ==========\n",
      "Accuracy:  0.7632\n",
      "F1-score:  0.8000\n",
      "ROC-AUC:   0.8385\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.85      0.71        13\n",
      "           1       0.90      0.72      0.80        25\n",
      "\n",
      "    accuracy                           0.76        38\n",
      "   macro avg       0.76      0.78      0.75        38\n",
      "weighted avg       0.80      0.76      0.77        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[11  2]\n",
      " [ 7 18]]\n",
      "\n",
      "Fitting Random Forest GridSearchCV...\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "RF best params: {'rf__max_depth': 5, 'rf__max_features': 'sqrt', 'rf__min_samples_leaf': 1, 'rf__n_estimators': 400}\n",
      "RF CV best F1: 0.8674\n",
      "RF training time (s): 287.11\n",
      "\n",
      "========== Random Forest (best): Test metrics ==========\n",
      "Accuracy:  0.7105\n",
      "F1-score:  0.7556\n",
      "ROC-AUC:   0.8538\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.77      0.65        13\n",
      "           1       0.85      0.68      0.76        25\n",
      "\n",
      "    accuracy                           0.71        38\n",
      "   macro avg       0.70      0.72      0.70        38\n",
      "weighted avg       0.75      0.71      0.72        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[10  3]\n",
      " [ 8 17]]\n",
      "\n",
      "Top 10 important subgraphs from Random Forest:\n",
      "pattern_31717         importance = 0.0051\n",
      "pattern_38929         importance = 0.0038\n",
      "pattern_39452         importance = 0.0037\n",
      "pattern_31770         importance = 0.0033\n",
      "pattern_13230         importance = 0.0033\n",
      "pattern_37055         importance = 0.0030\n",
      "pattern_32034         importance = 0.0030\n",
      "pattern_31716         importance = 0.0029\n",
      "pattern_35566         importance = 0.0029\n",
      "pattern_13348         importance = 0.0027\n",
      "\n",
      "Best SVM kernel is not linear; skipping SVM coefficient-based importance.\n",
      "\n",
      "\n",
      "===== Ablation: min_support = 19 =====\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "\n",
      "========== SVM (support=19): Test metrics ==========\n",
      "Accuracy:  0.7368\n",
      "F1-score:  0.7727\n",
      "ROC-AUC:   0.8338\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.85      0.69        13\n",
      "           1       0.89      0.68      0.77        25\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.74      0.76      0.73        38\n",
      "weighted avg       0.79      0.74      0.74        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[11  2]\n",
      " [ 8 17]]\n",
      "\n",
      "========== RF  (support=19): Test metrics ==========\n",
      "Accuracy:  0.7368\n",
      "F1-score:  0.7826\n",
      "ROC-AUC:   0.8615\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.77      0.67        13\n",
      "           1       0.86      0.72      0.78        25\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.72      0.74      0.72        38\n",
      "weighted avg       0.77      0.74      0.74        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[10  3]\n",
      " [ 7 18]]\n",
      "\n",
      "\n",
      "===== Ablation: min_support = 38 =====\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "\n",
      "========== SVM (support=38): Test metrics ==========\n",
      "Accuracy:  0.7105\n",
      "F1-score:  0.7660\n",
      "ROC-AUC:   0.8385\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.69      0.62        13\n",
      "           1       0.82      0.72      0.77        25\n",
      "\n",
      "    accuracy                           0.71        38\n",
      "   macro avg       0.69      0.71      0.69        38\n",
      "weighted avg       0.73      0.71      0.72        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 9  4]\n",
      " [ 7 18]]\n",
      "\n",
      "========== RF  (support=38): Test metrics ==========\n",
      "Accuracy:  0.7632\n",
      "F1-score:  0.8085\n",
      "ROC-AUC:   0.8538\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.77      0.69        13\n",
      "           1       0.86      0.76      0.81        25\n",
      "\n",
      "    accuracy                           0.76        38\n",
      "   macro avg       0.74      0.76      0.75        38\n",
      "weighted avg       0.78      0.76      0.77        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[10  3]\n",
      " [ 6 19]]\n",
      "\n",
      "\n",
      "===== Ablation: min_support = 57 =====\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "\n",
      "========== SVM (support=57): Test metrics ==========\n",
      "Accuracy:  0.7105\n",
      "F1-score:  0.7660\n",
      "ROC-AUC:   0.7292\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.69      0.62        13\n",
      "           1       0.82      0.72      0.77        25\n",
      "\n",
      "    accuracy                           0.71        38\n",
      "   macro avg       0.69      0.71      0.69        38\n",
      "weighted avg       0.73      0.71      0.72        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 9  4]\n",
      " [ 7 18]]\n",
      "\n",
      "========== RF  (support=57): Test metrics ==========\n",
      "Accuracy:  0.7368\n",
      "F1-score:  0.7917\n",
      "ROC-AUC:   0.8031\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.69      0.64        13\n",
      "           1       0.83      0.76      0.79        25\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.71      0.73      0.72        38\n",
      "weighted avg       0.75      0.74      0.74        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 9  4]\n",
      " [ 6 19]]\n",
      "\n",
      "\n",
      "===== Ablation: min_support = 76 =====\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "\n",
      "========== SVM (support=76): Test metrics ==========\n",
      "Accuracy:  0.7368\n",
      "F1-score:  0.8077\n",
      "ROC-AUC:   0.7923\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.54      0.58        13\n",
      "           1       0.78      0.84      0.81        25\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.71      0.69      0.70        38\n",
      "weighted avg       0.73      0.74      0.73        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 7  6]\n",
      " [ 4 21]]\n",
      "\n",
      "========== RF  (support=76): Test metrics ==========\n",
      "Accuracy:  0.7368\n",
      "F1-score:  0.8077\n",
      "ROC-AUC:   0.7985\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.54      0.58        13\n",
      "           1       0.78      0.84      0.81        25\n",
      "\n",
      "    accuracy                           0.74        38\n",
      "   macro avg       0.71      0.69      0.70        38\n",
      "weighted avg       0.73      0.74      0.73        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 7  6]\n",
      " [ 4 21]]\n",
      "\n",
      "\n",
      "===== Ablation: min_support = 94 =====\n",
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "\n",
      "========== SVM (support=94): Test metrics ==========\n",
      "Accuracy:  0.6579\n",
      "F1-score:  0.7797\n",
      "ROC-AUC:   0.5846\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.15      0.24        13\n",
      "           1       0.68      0.92      0.78        25\n",
      "\n",
      "    accuracy                           0.66        38\n",
      "   macro avg       0.59      0.54      0.51        38\n",
      "weighted avg       0.62      0.66      0.59        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 2 11]\n",
      " [ 2 23]]\n",
      "\n",
      "========== RF  (support=94): Test metrics ==========\n",
      "Accuracy:  0.6579\n",
      "F1-score:  0.7797\n",
      "ROC-AUC:   0.5323\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.15      0.24        13\n",
      "           1       0.68      0.92      0.78        25\n",
      "\n",
      "    accuracy                           0.66        38\n",
      "   macro avg       0.59      0.54      0.51        38\n",
      "weighted avg       0.62      0.66      0.59        38\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 2 11]\n",
      " [ 2 23]]\n",
      "\n",
      "Ablation results:\n",
      "   support  n_features  svm_accuracy    svm_f1  svm_roc_auc  rf_accuracy  \\\n",
      "0       19       40220      0.736842  0.772727     0.833846     0.736842   \n",
      "1       38        4751      0.710526  0.765957     0.838462     0.763158   \n",
      "2       57         637      0.710526  0.765957     0.729231     0.736842   \n",
      "3       76         110      0.736842  0.807692     0.792308     0.736842   \n",
      "4       94          74      0.657895  0.779661     0.584615     0.657895   \n",
      "\n",
      "      rf_f1  rf_roc_auc  \n",
      "0  0.782609    0.861538  \n",
      "1  0.808511    0.853846  \n",
      "2  0.791667    0.803077  \n",
      "3  0.807692    0.798462  \n",
      "4  0.779661    0.532308  \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Classical ML on MUTAG with frequent subgraph features\n",
    "=====================================================\n",
    "\n",
    "WHAT:  Train and evaluate an SVM and a Random Forest classifier\n",
    "       on MUTAG graphs represented by frequent-subgraph features\n",
    "       (mined using gSpan or a similar algorithm).\n",
    "\n",
    "HOW:   1) Load a CSV where each row = a graph and columns = subgraph counts\n",
    "       2) Split into train/test\n",
    "       3) Build scikit-learn Pipelines for:\n",
    "            - SVM (with scaling + hyperparameter tuning)\n",
    "            - Random Forest (with hyperparameter tuning)\n",
    "       4) Evaluate with multiple metrics\n",
    "       5) Inspect feature importances / coefficients\n",
    "\n",
    "WHY:   This is exactly the \"classic ML\" part of Q-1 in your project:\n",
    "       you’re using mined subgraphs as features and training\n",
    "       two classic ML models on top of them.\n",
    "\"\"\"\n",
    "\n",
    "# ==============================\n",
    "# 0) Imports\n",
    "# ==============================\n",
    "\n",
    "import time  # for measuring runtime (efficiency trade-offs)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 1) Configuration\n",
    "# ==============================\n",
    "\n",
    "# WHAT: Path to your feature CSV created after running gSpan + feature construction.\n",
    "# HOW:  Replace this with the REAL path to your file.\n",
    "FEATURE_FILE = r\"C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_features_support20.csv\"\n",
    "\n",
    "# WHAT: Name of the columns that identify graph and label in your CSV.\n",
    "# WHY:  We will drop ID and use all remaining numeric columns as features.\n",
    "GRAPH_ID_COL = \"graph_id\"   # change if your column has a different name\n",
    "LABEL_COL    = \"label\"      # change if your label column has a different name\n",
    "\n",
    "# WHAT: Random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# WHAT: Proportion of data used for testing\n",
    "TEST_SIZE = 0.2  # 20% test, 80% train (you can tweak this)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 2) Load data and build X, y\n",
    "# ==============================\n",
    "\n",
    "def load_feature_matrix(\n",
    "    feature_file: str,\n",
    "    label_col: str = LABEL_COL,\n",
    "    id_col: str = GRAPH_ID_COL\n",
    "):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "        Load the subgraph-based feature matrix and labels from a CSV.\n",
    "\n",
    "    HOW:\n",
    "        - Reads a CSV with pandas.\n",
    "        - Extracts:\n",
    "            X: all numeric columns except (id_col, label_col)\n",
    "            y: the label column\n",
    "\n",
    "    WHY:\n",
    "        Classic ML models (SVM, RF) expect:\n",
    "            X: 2D numeric array [n_graphs, n_features]\n",
    "            y: 1D array of class labels\n",
    "    \"\"\"\n",
    "    # Read CSV into a pandas DataFrame\n",
    "    df = pd.read_csv(feature_file)\n",
    "\n",
    "    # Make a copy for safety (avoid accidental in-place changes)\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure label is not missing\n",
    "    if label_col not in df.columns:\n",
    "        raise ValueError(f\"Label column '{label_col}' not found in CSV.\")\n",
    "\n",
    "    # If graph_id column exists, drop it from feature set, keep it only for reference\n",
    "    drop_cols = [label_col]\n",
    "    if id_col in df.columns:\n",
    "        drop_cols.append(id_col)\n",
    "\n",
    "    # X = all columns except label and id\n",
    "    X = df.drop(columns=drop_cols)\n",
    "\n",
    "    # y = label vector\n",
    "    y = df[label_col]\n",
    "\n",
    "    # Optional: ensure X is numeric\n",
    "    X = X.apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    "\n",
    "    return df, X.values, y.values, X.columns.tolist()  # also return feature names\n",
    "\n",
    "\n",
    "# Actually load data\n",
    "full_df, X, y, feature_names = load_feature_matrix(FEATURE_FILE)\n",
    "\n",
    "print(\"Shape of X (graphs x subgraph features):\", X.shape)\n",
    "print(\"Number of labels:\", len(y))\n",
    "print(\"First 5 feature names:\", feature_names[:5])\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 3) Train/test split\n",
    "# ==============================\n",
    "\n",
    "\"\"\"\n",
    "WHAT:\n",
    "    Split dataset into train and test sets.\n",
    "\n",
    "HOW:\n",
    "    - Use train_test_split from sklearn.\n",
    "    - Use stratify=y to keep label balance similar in train and test.\n",
    "\n",
    "WHY:\n",
    "    We want an unbiased estimate of test performance and to avoid\n",
    "    accidentally training and testing on the same graphs.\n",
    "\"\"\"\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y  # important for classification with possibly imbalanced labels\n",
    ")\n",
    "\n",
    "print(\"\\nTrain size:\", X_train.shape[0], \"Test size:\", X_test.shape[0])\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 4) Helper: evaluation function\n",
    "# ==============================\n",
    "\n",
    "def evaluate_classifier(name, model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "        Compute and print evaluation metrics for a fitted classifier.\n",
    "\n",
    "    HOW:\n",
    "        - Use the model's predict() and predict_proba()/decision_function()\n",
    "          to compute:\n",
    "            * accuracy\n",
    "            * F1-score (macro or binary)\n",
    "            * ROC-AUC (if possible)\n",
    "        - Show a classification report and confusion matrix.\n",
    "\n",
    "    WHY:\n",
    "        Your project requires \"several metrics\" and a discussion of\n",
    "        quality vs efficiency. These metrics give you a detailed\n",
    "        picture of performance.\n",
    "    \"\"\"\n",
    "    print(f\"\\n========== {name}: Test metrics ==========\")\n",
    "\n",
    "    # Standard predictions (class)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # F1 (binary average by default; change to 'macro' if multi-class)\n",
    "    f1 = f1_score(y_test, y_pred, average=\"binary\")\n",
    "\n",
    "    # ROC-AUC: need probabilities or decision scores\n",
    "    try:\n",
    "        # If classifier has predict_proba (e.g., RF, probability=True SVM)\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_proba = model.predict_proba(X_test)[:, 1]\n",
    "        # Else use decision_function (e.g., SVM without probability=True)\n",
    "        elif hasattr(model, \"decision_function\"):\n",
    "            y_proba = model.decision_function(X_test)\n",
    "        else:\n",
    "            y_proba = None\n",
    "\n",
    "        if y_proba is not None:\n",
    "            auc = roc_auc_score(y_test, y_proba)\n",
    "        else:\n",
    "            auc = np.nan\n",
    "    except Exception as e:\n",
    "        print(\"Could not compute ROC-AUC:\", e)\n",
    "        auc = np.nan\n",
    "\n",
    "    print(f\"Accuracy:  {acc:.4f}\")\n",
    "    print(f\"F1-score:  {f1:.4f}\")\n",
    "    print(f\"ROC-AUC:   {auc:.4f}\" if not np.isnan(auc) else \"ROC-AUC: N/A\")\n",
    "\n",
    "    # Detailed per-class metrics\n",
    "    print(\"\\nClassification report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Return metrics in case you want to log them\n",
    "    return {\"accuracy\": acc, \"f1\": f1, \"roc_auc\": auc}\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 5) SVM model (with scaling)\n",
    "# ==============================\n",
    "\n",
    "\"\"\"\n",
    "WHAT:\n",
    "    Train an SVM classifier on subgraph features.\n",
    "\n",
    "WHY:\n",
    "    SVMs are strong baselines for high-dimensional feature spaces\n",
    "    like \"bags of subgraphs\".\n",
    "\n",
    "HOW:\n",
    "    - Use a Pipeline: StandardScaler -> SVC\n",
    "      * SVMs are sensitive to feature scale, so scaling is important.\n",
    "    - Use GridSearchCV to do hyperparameter tuning with cross-validation:\n",
    "      * kernel: 'rbf', 'linear'\n",
    "      * C: regularization strength\n",
    "      * gamma: RBF kernel width, etc.\n",
    "\"\"\"\n",
    "\n",
    "# Build pipeline: scaling + SVM\n",
    "svm_pipeline = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler(with_mean=False)),  # with_mean=False if features are sparse; safe for dense too\n",
    "    (\"svm\", SVC())\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for SVM\n",
    "svm_param_grid = {\n",
    "    \"svm__kernel\": [\"rbf\", \"linear\"],      # test both RBF and linear\n",
    "    \"svm__C\": [0.1, 1, 10, 25, 100, 500],           # regularization strength\n",
    "    \"svm__gamma\": [\"scale\", \"auto\"],       # only used for RBF but safe to include\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "svm_grid = GridSearchCV(\n",
    "    estimator=svm_pipeline,\n",
    "    param_grid=svm_param_grid,\n",
    "    scoring=\"f1\",          # choose your main metric (could be 'roc_auc')\n",
    "    cv=5,                  # 5-fold cross-validation\n",
    "    n_jobs=-1,             # use all cores\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nFitting SVM GridSearchCV...\")\n",
    "start_time = time.time()\n",
    "svm_grid.fit(X_train, y_train)\n",
    "svm_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"SVM best params: {svm_grid.best_params_}\")\n",
    "print(f\"SVM CV best F1: {svm_grid.best_score_:.4f}\")\n",
    "print(f\"SVM training time (s): {svm_train_time:.2f}\")\n",
    "\n",
    "# Evaluate best SVM on test data\n",
    "svm_metrics = evaluate_classifier(\"SVM (best)\", svm_grid.best_estimator_, X_test, y_test)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 6) Random Forest model\n",
    "# ==============================\n",
    "\n",
    "\"\"\"\n",
    "WHAT:\n",
    "    Train a Random Forest classifier.\n",
    "\n",
    "WHY:\n",
    "    - RFs work well out-of-the-box on tabular data.\n",
    "    - They provide feature importances, which is great for Q-4\n",
    "      (explainability via subgraph importance).\n",
    "\n",
    "HOW:\n",
    "    - Use Pipeline (scaler optional; RF is tree-based and does not\n",
    "      strictly need scaling, but including it does no harm).\n",
    "    - Use GridSearchCV for hyperparameters:\n",
    "        n_estimators, max_depth, max_features, etc.\n",
    "\"\"\"\n",
    "\n",
    "# Build pipeline: (scaling is optional; we keep it for consistency)\n",
    "rf_pipeline = Pipeline(steps=[\n",
    "    (\"rf\", RandomForestClassifier(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Define hyperparameter grid for RF\n",
    "rf_param_grid = {\n",
    "    \"rf__n_estimators\": [100, 250, 300, 400, 500],   # number of trees\n",
    "    \"rf__max_depth\": [None, 5, 10, 20],       # tree depth (None = full depth)\n",
    "    \"rf__max_features\": [\"sqrt\", \"log2\"],  # number of features to sample per split\n",
    "    \"rf__min_samples_leaf\": [1, 2, 3, 4, 5],\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "rf_grid = GridSearchCV(\n",
    "    estimator=rf_pipeline,\n",
    "    param_grid=rf_param_grid,\n",
    "    scoring=\"f1\",\n",
    "    cv=5,\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"\\nFitting Random Forest GridSearchCV...\")\n",
    "start_time = time.time()\n",
    "rf_grid.fit(X_train, y_train)\n",
    "rf_train_time = time.time() - start_time\n",
    "\n",
    "print(f\"RF best params: {rf_grid.best_params_}\")\n",
    "print(f\"RF CV best F1: {rf_grid.best_score_:.4f}\")\n",
    "print(f\"RF training time (s): {rf_train_time:.2f}\")\n",
    "\n",
    "# Evaluate best RF on test data\n",
    "rf_metrics = evaluate_classifier(\"Random Forest (best)\", rf_grid.best_estimator_, X_test, y_test)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# 7) Feature importance / explainability hooks\n",
    "# ==============================\n",
    "\n",
    "\"\"\"\n",
    "WHAT:\n",
    "    Inspect which subgraphs are important to each classifier.\n",
    "\n",
    "WHY:\n",
    "    Q-4 asks you to discuss explanations and compare to GNNExplainer.\n",
    "    For classic models, \"importance\" comes from:\n",
    "        - RandomForest: feature_importances_\n",
    "        - Linear SVM: coefficients\n",
    "\n",
    "HOW:\n",
    "    - For RF: directly read feature_importances_ from the 'rf' step.\n",
    "    - For SVM: if best kernel is 'linear', use coef_ of the 'svm' step.\n",
    "\"\"\"\n",
    "\n",
    "def top_k_features_from_rf(rf_pipeline, feature_names, k=10):\n",
    "    \"\"\"\n",
    "    Extract top-k most important features from a RandomForest Pipeline.\n",
    "\n",
    "    rf_pipeline: the best_estimator_ from rf_grid (Pipeline with 'rf' step).\n",
    "    feature_names: list of feature names (matching columns in X).\n",
    "    \"\"\"\n",
    "    # Get the actual RandomForestClassifier from the pipeline\n",
    "    rf_model = rf_pipeline.named_steps[\"rf\"]\n",
    "    importances = rf_model.feature_importances_\n",
    "\n",
    "    # Sort features by importance\n",
    "    idx = np.argsort(importances)[::-1][:k]\n",
    "    top_feats = [(feature_names[i], importances[i]) for i in idx]\n",
    "    return top_feats\n",
    "\n",
    "\n",
    "def top_k_features_from_linear_svm(svm_pipeline, feature_names, k=10):\n",
    "    \"\"\"\n",
    "    Extract top-k most influential features from a linear SVM Pipeline.\n",
    "\n",
    "    svm_pipeline: best_estimator_ from svm_grid (Pipeline with 'svm' step),\n",
    "                  must have kernel='linear'.\n",
    "    \"\"\"\n",
    "    svm_model = svm_pipeline.named_steps[\"svm\"]\n",
    "    if svm_model.kernel != \"linear\":\n",
    "        raise ValueError(\"SVM kernel is not linear; coefficients are not directly interpretable.\")\n",
    "\n",
    "    # coef_ shape: (n_classes, n_features). For binary, often (1, n_features)\n",
    "    coefs = svm_model.coef_.ravel()\n",
    "    # Use absolute value as a measure of importance\n",
    "    abs_coefs = np.abs(coefs)\n",
    "    idx = np.argsort(abs_coefs)[::-1][:k]\n",
    "    top_feats = [(feature_names[i], coefs[i]) for i in idx]\n",
    "    return top_feats\n",
    "\n",
    "\n",
    "# Example: show top-10 RF features\n",
    "print(\"\\nTop 10 important subgraphs from Random Forest:\")\n",
    "top_rf_feats = top_k_features_from_rf(rf_grid.best_estimator_, feature_names, k=10)\n",
    "for name, score in top_rf_feats:\n",
    "    print(f\"{name:20s}  importance = {score:.4f}\")\n",
    "\n",
    "# Example: show top-10 SVM features, if kernel is linear\n",
    "best_svm_kernel = svm_grid.best_estimator_.named_steps[\"svm\"].kernel\n",
    "if best_svm_kernel == \"linear\":\n",
    "    print(\"\\nTop 10 influential subgraphs from linear SVM (signed coefficients):\")\n",
    "    top_svm_feats = top_k_features_from_linear_svm(svm_grid.best_estimator_, feature_names, k=10)\n",
    "    for name, coef in top_svm_feats:\n",
    "        print(f\"{name:20s}  coef = {coef:.4f}\")\n",
    "else:\n",
    "    print(\"\\nBest SVM kernel is not linear; skipping SVM coefficient-based importance.\")\n",
    "    \n",
    "\n",
    "# ==============================\n",
    "# 8) (Optional) Ablation template\n",
    "# ==============================\n",
    "\n",
    "\"\"\"\n",
    "WHAT:\n",
    "    Ablation study for Q-1: vary the gSpan mining threshold and see\n",
    "    how it affects performance and runtime.\n",
    "\n",
    "HOW:\n",
    "    Suppose you run gSpan with different min_support values (e.g., 5%, 10%, 20%)\n",
    "    and each run creates a separate CSV:\n",
    "       - MUTAG_subgraphs_support_5.csv\n",
    "       - MUTAG_subgraphs_support_10.csv\n",
    "       - MUTAG_subgraphs_support_20.csv\n",
    "\n",
    "    Then we simply loop over these files, repeat the steps above, and\n",
    "    record metrics + training time.\n",
    "\n",
    "WHY:\n",
    "    This lets you quantify the trade-off:\n",
    "        - Lower support -> more subgraphs (higher dimensionality), potentially better accuracy,\n",
    "          but slower training and risk of overfitting.\n",
    "        - Higher support -> fewer features, faster but maybe lower accuracy.\n",
    "\"\"\"\n",
    "\n",
    "def ablation_over_supports(support_values, base_path=r\"C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\mutag_features_support{}.csv\"):\n",
    "    results = []\n",
    "\n",
    "    for s in support_values:\n",
    "        print(f\"\\n\\n===== Ablation: min_support = {s} =====\")\n",
    "        feature_file = base_path.format(s)\n",
    "\n",
    "        df, X_s, y_s, feat_names_s = load_feature_matrix(feature_file)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_s,\n",
    "            y_s,\n",
    "            test_size=TEST_SIZE,\n",
    "            random_state=RANDOM_STATE,\n",
    "            stratify=y_s\n",
    "        )\n",
    "\n",
    "        # Reuse the SVM and RF grids but re-fit for this feature set\n",
    "        svm_grid.fit(X_train, y_train)\n",
    "        rf_grid.fit(X_train, y_train)\n",
    "\n",
    "        svm_metrics_s = evaluate_classifier(f\"SVM (support={s})\", svm_grid.best_estimator_, X_test, y_test)\n",
    "        rf_metrics_s  = evaluate_classifier(f\"RF  (support={s})\", rf_grid.best_estimator_, X_test, y_test)\n",
    "\n",
    "        results.append({\n",
    "            \"support\": s,\n",
    "            \"n_features\": X_s.shape[1],\n",
    "            \"svm_accuracy\": svm_metrics_s[\"accuracy\"],\n",
    "            \"svm_f1\": svm_metrics_s[\"f1\"],\n",
    "            \"svm_roc_auc\": svm_metrics_s[\"roc_auc\"],\n",
    "            \"rf_accuracy\": rf_metrics_s[\"accuracy\"],\n",
    "            \"rf_f1\": rf_metrics_s[\"f1\"],\n",
    "            \"rf_roc_auc\": rf_metrics_s[\"roc_auc\"],\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Example usage (commented out so your script doesn't auto-run everything):\n",
    "supports_to_test = [19, 38, 57, 76, 94]\n",
    "ablation_df = ablation_over_supports(supports_to_test)\n",
    "print(\"\\nAblation results:\")\n",
    "print(ablation_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a743b798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classic ML ablation results -> C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\results\\classic_ml_ablation_results.csv\n"
     ]
    }
   ],
   "source": [
    "ablation_df.to_csv(r\"C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\classic_ml_ablation_results.csv\")\n",
    "\n",
    "classic_ablation_csv = RESULTS_DIR / \"classic_ml_ablation_results.csv\"\n",
    "ablation_df.to_csv(classic_ablation_csv, index=False)\n",
    "print(f\"Saved classic ML ablation results -> {classic_ablation_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9f6b145b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support</th>\n",
       "      <th>n_features</th>\n",
       "      <th>model</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>40220</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.833846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>4751</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.838462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>637</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>0.765957</td>\n",
       "      <td>0.729231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76</td>\n",
       "      <td>110</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.792308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94</td>\n",
       "      <td>74</td>\n",
       "      <td>SVM</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.584615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   support  n_features model  test_accuracy   test_f1  test_auc\n",
       "0       19       40220   SVM       0.736842  0.772727  0.833846\n",
       "1       38        4751   SVM       0.710526  0.765957  0.838462\n",
       "2       57         637   SVM       0.710526  0.765957  0.729231\n",
       "3       76         110   SVM       0.736842  0.807692  0.792308\n",
       "4       94          74   SVM       0.657895  0.779661  0.584615"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved long-format classic metrics -> C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\results\\classic_ml_metrics_long.csv\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# I.1.5:  Tidy version of our ablation\n",
    "# ======================================\n",
    "\n",
    "# Make a long-format table: one row = (support, model, test_accuracy, test_f1, test_auc, n_features)\n",
    "my_results_long = pd.concat([\n",
    "    ablation_df.assign(model=\"SVM\").rename(columns={\n",
    "        \"svm_accuracy\": \"test_accuracy\",\n",
    "        \"svm_f1\": \"test_f1\",\n",
    "        \"svm_roc_auc\": \"test_auc\",\n",
    "    })[[\"support\", \"n_features\", \"model\", \"test_accuracy\", \"test_f1\", \"test_auc\"]],\n",
    "    ablation_df.assign(model=\"RandomForest\").rename(columns={\n",
    "        \"rf_accuracy\": \"test_accuracy\",\n",
    "        \"rf_f1\": \"test_f1\",\n",
    "        \"rf_roc_auc\": \"test_auc\",\n",
    "    })[[\"support\", \"n_features\", \"model\", \"test_accuracy\", \"test_f1\", \"test_auc\"]],\n",
    "], ignore_index=True)\n",
    "\n",
    "display(my_results_long.head())\n",
    "\n",
    "classic_long_csv = RESULTS_DIR / \"classic_ml_metrics_long.csv\"\n",
    "my_results_long.to_csv(classic_long_csv, index=False)\n",
    "print(f\"Saved long-format classic metrics -> {classic_long_csv}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e2f9eee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>params</th>\n",
       "      <th>num_params</th>\n",
       "      <th>train_time_sec</th>\n",
       "      <th>feature_dim</th>\n",
       "      <th>seed</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_precision</th>\n",
       "      <th>val_recall</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_auc</th>\n",
       "      <th>val_inference_time_sec</th>\n",
       "      <th>test_accuracy</th>\n",
       "      <th>test_precision</th>\n",
       "      <th>test_recall</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_auc</th>\n",
       "      <th>test_inference_time_sec</th>\n",
       "      <th>support_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{\"n_estimators\": 100, \"max_depth\": null}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.118839</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.014990</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.911458</td>\n",
       "      <td>0.014467</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{\"n_estimators\": 200, \"max_depth\": null}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.179069</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.014936</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.014618</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{\"n_estimators\": 100, \"max_depth\": 10}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.108726</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.938462</td>\n",
       "      <td>0.014594</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.911458</td>\n",
       "      <td>0.014016</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>{\"n_estimators\": 200, \"max_depth\": 20}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.189322</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.964286</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.953846</td>\n",
       "      <td>0.026239</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.809524</td>\n",
       "      <td>0.770833</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.921875</td>\n",
       "      <td>0.014444</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LinearSVM</td>\n",
       "      <td>{\"type\": \"linear\", \"C\": 0.1}</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.147866</td>\n",
       "      <td>800</td>\n",
       "      <td>0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.823077</td>\n",
       "      <td>0.803636</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          model                                    params  num_params  \\\n",
       "0  RandomForest  {\"n_estimators\": 100, \"max_depth\": null}         NaN   \n",
       "1  RandomForest  {\"n_estimators\": 200, \"max_depth\": null}         NaN   \n",
       "2  RandomForest    {\"n_estimators\": 100, \"max_depth\": 10}         NaN   \n",
       "3  RandomForest    {\"n_estimators\": 200, \"max_depth\": 20}         NaN   \n",
       "4     LinearSVM              {\"type\": \"linear\", \"C\": 0.1}       800.0   \n",
       "\n",
       "   train_time_sec  feature_dim  seed  val_accuracy  val_precision  val_recall  \\\n",
       "0        0.118839          800     0      0.944444       0.964286    0.900000   \n",
       "1        0.179069          800     0      0.944444       0.964286    0.900000   \n",
       "2        0.108726          800     0      0.944444       0.964286    0.900000   \n",
       "3        0.189322          800     0      0.944444       0.964286    0.900000   \n",
       "4        0.147866          800     0      0.833333       0.791667    0.823077   \n",
       "\n",
       "     val_f1   val_auc  val_inference_time_sec  test_accuracy  test_precision  \\\n",
       "0  0.925926  0.953846                0.014990            0.8        0.809524   \n",
       "1  0.925926  0.953846                0.014936            0.8        0.809524   \n",
       "2  0.925926  0.938462                0.014594            0.8        0.809524   \n",
       "3  0.925926  0.953846                0.026239            0.8        0.809524   \n",
       "4  0.803636  0.923077                0.000249            0.9        0.895833   \n",
       "\n",
       "   test_recall   test_f1  test_auc  test_inference_time_sec  support_ratio  \n",
       "0     0.770833  0.780220  0.911458                 0.014467            0.1  \n",
       "1     0.770833  0.780220  0.921875                 0.014618            0.1  \n",
       "2     0.770833  0.780220  0.911458                 0.014016            0.1  \n",
       "3     0.770833  0.780220  0.921875                 0.014444            0.1  \n",
       "4     0.895833  0.895833  0.937500                 0.000138            0.1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved all classic ML runs -> C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\results\\classic_ml_all_runs.csv\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# I.2:      Integration of Prior Work\n",
    "# ======================================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "RESULTS_ROOT = Path(\"q1_frequent_subgraphs_classic_ml\") / \"results\"\n",
    "\n",
    "all_runs = []\n",
    "\n",
    "for seed_dir in sorted(RESULTS_ROOT.glob(\"seed_*\")):\n",
    "    seed = int(seed_dir.name.split(\"_\")[1])\n",
    "    for csv_path in sorted(seed_dir.glob(\"classic_ml_support_*.csv\")):\n",
    "        support_str = csv_path.stem.split(\"_\")[-1]  # e.g., '0.20'\n",
    "        support_ratio = float(support_str)\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df[\"seed\"] = seed\n",
    "        df[\"support_ratio\"] = support_ratio\n",
    "        all_runs.append(df)\n",
    "\n",
    "classic_results = pd.concat(all_runs, ignore_index=True)\n",
    "display(classic_results.head())\n",
    "\n",
    "classic_all_runs_csv = RESULTS_DIR / \"classic_ml_all_runs.csv\"\n",
    "classic_results.to_csv(classic_all_runs_csv, index=False)\n",
    "print(f\"Saved all classic ML runs -> {classic_all_runs_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e938e0d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>support_ratio</th>\n",
       "      <th>model</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.4</td>\n",
       "      <td>RBFSVM</td>\n",
       "      <td>0.867860</td>\n",
       "      <td>0.043337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.3</td>\n",
       "      <td>LinearSVM</td>\n",
       "      <td>0.864010</td>\n",
       "      <td>0.080057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>LinearSVM</td>\n",
       "      <td>0.863362</td>\n",
       "      <td>0.098904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.3</td>\n",
       "      <td>RBFSVM</td>\n",
       "      <td>0.832282</td>\n",
       "      <td>0.050586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1</td>\n",
       "      <td>RBFSVM</td>\n",
       "      <td>0.809889</td>\n",
       "      <td>0.106812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2</td>\n",
       "      <td>RBFSVM</td>\n",
       "      <td>0.809388</td>\n",
       "      <td>0.092543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2</td>\n",
       "      <td>LinearSVM</td>\n",
       "      <td>0.806404</td>\n",
       "      <td>0.151465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.4</td>\n",
       "      <td>LinearSVM</td>\n",
       "      <td>0.794668</td>\n",
       "      <td>0.103634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.769240</td>\n",
       "      <td>0.065530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.3</td>\n",
       "      <td>RandomForest</td>\n",
       "      <td>0.748568</td>\n",
       "      <td>0.165390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    support_ratio         model  mean_test_f1  std_test_f1\n",
       "10            0.4        RBFSVM      0.867860     0.043337\n",
       "6             0.3     LinearSVM      0.864010     0.080057\n",
       "0             0.1     LinearSVM      0.863362     0.098904\n",
       "7             0.3        RBFSVM      0.832282     0.050586\n",
       "1             0.1        RBFSVM      0.809889     0.106812\n",
       "4             0.2        RBFSVM      0.809388     0.092543\n",
       "3             0.2     LinearSVM      0.806404     0.151465\n",
       "9             0.4     LinearSVM      0.794668     0.103634\n",
       "2             0.1  RandomForest      0.769240     0.065530\n",
       "8             0.3  RandomForest      0.748568     0.165390"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved best-by-support summary -> C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\results\\classic_ml_best_by_support.csv\n"
     ]
    }
   ],
   "source": [
    "# Best test F1 per support_ratio (averaged or max over seeds/models)\n",
    "best_by_support = (\n",
    "    classic_results\n",
    "    .groupby([\"support_ratio\", \"model\"], as_index=False)\n",
    "    .agg(\n",
    "        mean_test_f1=(\"test_f1\", \"mean\"),\n",
    "        std_test_f1=(\"test_f1\", \"std\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "display(best_by_support.sort_values(\"mean_test_f1\", ascending=False).head(10))\n",
    "\n",
    "classic_best_support_csv = RESULTS_DIR / \"classic_ml_best_by_support.csv\"\n",
    "best_by_support.to_csv(classic_best_support_csv, index=False)\n",
    "print(f\"Saved best-by-support summary -> {classic_best_support_csv}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ae4c29e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "for model in best_by_support[\"model\"].unique():\n",
    "    sub = best_by_support[best_by_support[\"model\"] == model]\n",
    "    plt.plot(sub[\"support_ratio\"], sub[\"mean_test_f1\"], marker=\"o\", label=model)\n",
    "\n",
    "plt.xlabel(\"Support ratio\")\n",
    "plt.ylabel(\"Mean test F1\")\n",
    "plt.title(\"Classical models: performance vs support ratio\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "25fc2ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>seed</th>\n",
       "      <th>support_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best classical (LinearSVM)</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model  test_f1  seed  support_ratio\n",
       "0  Best classical (LinearSVM)      1.0     1            0.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved classical vs GNN summary -> C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\results\\classic_vs_gnn_summary.csv\n"
     ]
    }
   ],
   "source": [
    "comparison_rows = []\n",
    "\n",
    "# 1) Best classical run overall\n",
    "idx_max = classic_results[\"test_f1\"].idxmax()\n",
    "best_classic = classic_results.loc[idx_max]\n",
    "comparison_rows.append({\n",
    "    \"model\": f\"Best classical ({best_classic['model']})\",\n",
    "    \"test_f1\": best_classic[\"test_f1\"],\n",
    "    \"seed\": best_classic[\"seed\"],\n",
    "    \"support_ratio\": best_classic[\"support_ratio\"],\n",
    "})\n",
    "\n",
    "# 2) GNNs (values pulled from your GNN evaluation cells)\n",
    "#comparison_rows.append({\"model\": \"GCN\", \"test_f1\": gcn_test_f1})\n",
    "# comparison_rows.append({\"model\": \"GraphSAGE\", \"test_f1\": sage_test_f1})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "display(comparison_df)\n",
    "\n",
    "comparison_csv = RESULTS_DIR / \"classic_vs_gnn_summary.csv\"\n",
    "comparison_df.to_csv(comparison_csv, index=False)\n",
    "print(f\"Saved classical vs GNN summary -> {comparison_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "42c5f0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# FIGURE 2: Min-support vs F1 and #features\n",
    "# -------------------------------\n",
    "# WHAT:\n",
    "#   Visualize the tradeoff:\n",
    "#     - x-axis: gSpan min_support\n",
    "#     - left y-axis: SVM / RF F1 score\n",
    "#     - right y-axis: number of features (subgraphs)\n",
    "#\n",
    "# WHY:\n",
    "#   Shows how support threshold controls feature dimensionality\n",
    "#   and downstream classifier performance.\n",
    "\n",
    "\n",
    "def plot_min_support_ablation(ablation_df):\n",
    "    supports = ablation_df[\"support\"].values\n",
    "    svm_f1 = ablation_df[\"svm_f1\"].values\n",
    "    rf_f1 = ablation_df[\"rf_f1\"].values\n",
    "    n_features = ablation_df[\"n_features\"].values\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "    # Left axis: F1 scores\n",
    "    ax1.set_xlabel(\"gSpan min_support\")\n",
    "    ax1.set_ylabel(\"F1 score (SVM / RF)\")\n",
    "    ax1.plot(supports, svm_f1, marker=\"o\", label=\"SVM F1\")\n",
    "    ax1.plot(supports, rf_f1, marker=\"s\", label=\"RF F1\")\n",
    "    ax1.set_ylim(0, 1.0)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    # Right axis: number of features\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(\"# frequent subgraph features\")\n",
    "    ax2.bar(\n",
    "        supports,\n",
    "        n_features,\n",
    "        alpha=0.2,\n",
    "        width=(supports.max() - supports.min()) / (len(supports) + 1),\n",
    "        label=\"#features\",\n",
    "    )\n",
    "\n",
    "    plt.title(\"Effect of gSpan min_support on F1 and feature count\")\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example call:\n",
    "plot_min_support_ablation(ablation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "86121158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Q-2: GNNs on MUTAG – GCN + GraphSAGE\n",
    "# ============================================================\n",
    "\n",
    "# WHAT:\n",
    "#   We will train two graph neural network models for MUTAG:\n",
    "#     1) A GCN-based graph classifier\n",
    "#     2) A GraphSAGE-based graph classifier\n",
    "#\n",
    "# WHY:\n",
    "#   This satisfies Q-2 of your project:\n",
    "#   \"Train at least two GNN architectures (e.g., GCN, GraphSAGE) for\n",
    "#   graph classification and analyze their performance.\" \n",
    "#\n",
    "# HOW:\n",
    "#   Use PyTorch Geometric (PyG) to:\n",
    "#     - Load MUTAG graphs\n",
    "#     - Build GNN models\n",
    "#     - Train with early stopping\n",
    "#     - Evaluate accuracy & F1 on a held-out test set\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import (\n",
    "    GCNConv,        # GCN layer\n",
    "    SAGEConv,       # GraphSAGE layer\n",
    "    GINConv,        # GIN layer\n",
    "    global_mean_pool  # graph-level pooling (aggregates node embeddings)\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "\n",
    "# ------------------------------\n",
    "# Choose device (CPU or GPU)\n",
    "# ------------------------------\n",
    "# WHAT: Use GPU if available, otherwise CPU.\n",
    "# WHY: GNN training can be accelerated by GPU, but should still run on CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ------------------------------\n",
    "# Set random seeds for reproducibility\n",
    "# ------------------------------\n",
    "# WHAT: Fix seeds for numpy and torch.\n",
    "# WHY: Makes runs more repeatable (same splits, similar metrics).\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c2c9ccd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs in MUTAG: 188\n",
      "Number of node features  : 7\n",
      "Number of classes        : 2\n",
      "\n",
      "Example graph 0:\n",
      "  num_nodes: 17\n",
      "  num_edges: 38\n",
      "  x shape  : torch.Size([17, 7])\n",
      "  y label  : 1\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 1) Load the MUTAG dataset\n",
    "# ============================================================\n",
    "\n",
    "# WHAT:\n",
    "#   Load the MUTAG graph classification dataset from PyG.\n",
    "#\n",
    "# WHY:\n",
    "#   MUTAG is the required dataset in your project – 188 small graphs\n",
    "#   (chemical compounds) with binary labels (mutagenic / non-mutagenic).\n",
    "#\n",
    "# HOW:\n",
    "#   TUDataset(root=\"data\", name=\"MUTAG\") automatically downloads\n",
    "#   and loads the dataset if not already present.\n",
    "\n",
    "dataset = TUDataset(root=\"data\", name=\"MUTAG\")\n",
    "\n",
    "print(\"Number of graphs in MUTAG:\", len(dataset))\n",
    "print(\"Number of node features  :\", dataset.num_features)\n",
    "print(\"Number of classes        :\", dataset.num_classes)\n",
    "\n",
    "# Inspect one example graph to understand structure\n",
    "data0 = dataset[0]\n",
    "print(\"\\nExample graph 0:\")\n",
    "print(\"  num_nodes:\", data0.num_nodes)\n",
    "print(\"  num_edges:\", data0.num_edges)\n",
    "print(\"  x shape  :\", None if data0.x is None else data0.x.shape)\n",
    "print(\"  y label  :\", data0.y.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "252d5468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_gnn(model, loader, device):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "        Evaluate a graph classification GNN on a DataLoader.\n",
    "\n",
    "    HOW:\n",
    "        - Puts model in eval mode.\n",
    "        - Iterates over all batches, collects predictions and labels.\n",
    "        - Computes:\n",
    "            * accuracy\n",
    "            * F1-score (binary)\n",
    "            * ROC-AUC (if possible)\n",
    "\n",
    "    RETURNS:\n",
    "        metrics_dict, y_true, y_prob\n",
    "        where:\n",
    "          - metrics_dict has 'accuracy', 'f1', 'roc_auc'\n",
    "          - y_true is a 1D numpy array of labels\n",
    "          - y_prob is a 1D numpy array of predicted prob for class 1\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.batch)  # [batch_size, num_classes]\n",
    "            all_logits.append(out.cpu())\n",
    "            all_labels.append(data.y.cpu())\n",
    "\n",
    "    logits = torch.cat(all_logits, dim=0)      # [N_graphs, num_classes]\n",
    "    labels = torch.cat(all_labels, dim=0)      # [N_graphs]\n",
    "\n",
    "    # Convert to numpy\n",
    "    y_true = labels.numpy()\n",
    "    # For binary classification, turn logits into probabilities for class 1\n",
    "    probs = torch.softmax(logits, dim=1)[:, 1].numpy()\n",
    "    y_pred = (probs >= 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1  = f1_score(y_true, y_pred)\n",
    "\n",
    "    # ROC-AUC can fail if only one class is present, so be careful\n",
    "    try:\n",
    "        roc = roc_auc_score(y_true, probs)\n",
    "    except ValueError:\n",
    "        roc = np.nan\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc,\n",
    "    }\n",
    "    return metrics, y_true, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7cf539fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train graphs: 112\n",
      "Val graphs:   38\n",
      "Test graphs:  38\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 2) Train / validation / test split\n",
    "# ============================================================\n",
    "\n",
    "# WHAT:\n",
    "#   Split the 188 graphs into:\n",
    "#     - ~60% training\n",
    "#     - ~20% validation\n",
    "#     - ~20% test\n",
    "#\n",
    "# WHY:\n",
    "#   - Train set: update model weights.\n",
    "#   - Validation set: tune hyperparameters / early stopping.\n",
    "#   - Test set: final unbiased performance estimate.\n",
    "#\n",
    "# HOW:\n",
    "#   Use sklearn.train_test_split with stratification on labels\n",
    "#   so class balance is similar across splits.\n",
    "\n",
    "num_graphs = len(dataset)\n",
    "\n",
    "# All graph indices\n",
    "all_indices = np.arange(num_graphs)\n",
    "\n",
    "# Graph labels as a NumPy array for stratification\n",
    "all_labels = np.array([graph.y.item() for graph in dataset])\n",
    "\n",
    "# First split: train (60%) vs temp (40%)\n",
    "train_idx, temp_idx, y_train, y_temp = train_test_split(\n",
    "    all_indices,\n",
    "    all_labels,\n",
    "    test_size=0.4,       # 40% goes to temp\n",
    "    stratify=all_labels,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "# Second split: validation (20%) vs test (20%) from temp\n",
    "val_idx, test_idx, y_val, y_test = train_test_split(\n",
    "    temp_idx,\n",
    "    y_temp,\n",
    "    test_size=0.5,       # split temp 50/50 -> 20% val, 20% test\n",
    "    stratify=y_temp,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Train graphs:\", len(train_idx))\n",
    "print(\"Val graphs:  \", len(val_idx))\n",
    "print(\"Test graphs: \", len(test_idx))\n",
    "\n",
    "# Build subset datasets by indexing into MUTAG\n",
    "# HOW: dataset[...] returns Data objects; wrapping them in a list\n",
    "#      is enough for PyG's DataLoader.\n",
    "train_dataset = [dataset[i] for i in train_idx]\n",
    "val_dataset   = [dataset[i] for i in val_idx]\n",
    "test_dataset  = [dataset[i] for i in test_idx]\n",
    "\n",
    "# Wrap them in DataLoader for batching\n",
    "# WHY: DataLoader handles:\n",
    "#   - batching multiple graphs\n",
    "#   - shuffling during training\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "09b69451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 3) Training & evaluation helpers\n",
    "# ============================================================\n",
    "\n",
    "# WHAT:\n",
    "#   - train_one_epoch: trains the model for a single epoch.\n",
    "#   - eval_model: evaluates the model (no parameter updates).\n",
    "#\n",
    "# WHY:\n",
    "#   Avoid duplicating logic for GCN and GraphSAGE; ensures\n",
    "#   a fair comparison (same training procedure).\n",
    "#\n",
    "# HOW:\n",
    "#   - Loop over batches from DataLoader\n",
    "#   - Move batch to device (CPU/GPU)\n",
    "#   - Forward pass → logits\n",
    "#   - Compute cross-entropy loss\n",
    "#   - Backprop + optimizer.step (train only)\n",
    "#   - Track loss, accuracy, F1 across the whole split\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion, device):\n",
    "    \"\"\"\n",
    "    Train the model for ONE epoch on the given DataLoader.\n",
    "    \"\"\"\n",
    "    model.train()  # enable dropout, batchnorm, etc.\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        # Move batch (graphs) to device\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        # Zero out gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass:\n",
    "        #   model outputs class scores (logits) for each graph in the batch\n",
    "        logits = model(batch)   # shape: [batch_size, num_classes]\n",
    "\n",
    "        # Ground-truth graph labels\n",
    "        y = batch.y.view(-1)    # ensure shape: [batch_size]\n",
    "\n",
    "        # Compute loss for this batch\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        # Backpropagate to compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Update model parameters using optimizer\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate total loss (weighted by batch size)\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "        # Convert logits to predicted class indices\n",
    "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "        labels = y.detach().cpu().numpy()\n",
    "\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # Concatenate predictions/labels from all batches\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # Compute average loss & metrics across the entire training set\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)  # binary F1 by default\n",
    "    \n",
    "    return avg_loss, acc, f1\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on validation or test set (no gradient updates).\n",
    "    \"\"\"\n",
    "    model.eval()  # disable dropout, etc.\n",
    "\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "\n",
    "        logits = model(batch)\n",
    "        y = batch.y.view(-1)\n",
    "\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * batch.num_graphs\n",
    "\n",
    "        preds = logits.argmax(dim=1).cpu().numpy()\n",
    "        labels = y.cpu().numpy()\n",
    "\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "\n",
    "    return avg_loss, acc, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ae137281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_roc_auc(model, loader, device, multi_class=\"ovr\"):\n",
    "    \"\"\"\n",
    "    Compute ROC-AUC (and optionally ROC curve) on a given loader.\n",
    "\n",
    "    For binary classification:\n",
    "        - returns auc, fpr, tpr\n",
    "\n",
    "    For multi-class:\n",
    "        - returns macro/micro AUC depending on `multi_class`\n",
    "        - fpr / tpr are None (you’d need per-class curves if you want them)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            logits = model(batch)  # shape: [batch_size, num_classes]\n",
    "\n",
    "            # Turn logits into probabilities\n",
    "            if logits.shape[1] == 1:\n",
    "                # Rare case: single-logit + sigmoid binary classifier\n",
    "                probs = torch.sigmoid(logits).cpu().numpy()\n",
    "            else:\n",
    "                probs = F.softmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "            labels = batch.y.view(-1).cpu().numpy()\n",
    "\n",
    "            all_probs.append(probs)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    all_probs = np.vstack(all_probs)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "\n",
    "    # ----- Binary vs multi-class handling -----\n",
    "    if all_probs.shape[1] == 2:\n",
    "        # Binary: use probability of the positive class\n",
    "        y_score = all_probs[:, 1]\n",
    "        auc = roc_auc_score(all_labels, y_score)\n",
    "        fpr, tpr, _ = roc_curve(all_labels, y_score)\n",
    "        return auc, fpr, tpr\n",
    "    else:\n",
    "        # Multi-class: macro/micro AUC over all classes\n",
    "        auc = roc_auc_score(all_labels, all_probs, multi_class=multi_class)\n",
    "        return auc, None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eed7e172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_early_stopping(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    device,\n",
    "    max_epochs=200,\n",
    "    lr=0.01,\n",
    "    weight_decay=5e-4,\n",
    "    patience=20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a GNN model with early stopping based on validation accuracy.\n",
    "\n",
    "    WHAT:\n",
    "      - Train the model for up to max_epochs.\n",
    "      - After each epoch, evaluate on validation set.\n",
    "      - If validation accuracy does not improve for 'patience' epochs\n",
    "        in a row, stop training and revert to the best weights.\n",
    "\n",
    "    WHY:\n",
    "      Prevent overfitting and save time; we only keep the model\n",
    "      that performed best on the validation set.\n",
    "\n",
    "    RETURNS:\n",
    "      - best_model: model with best validation accuracy\n",
    "      - history: list of per-epoch metrics for plotting/report\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    best_state_dict = copy.deepcopy(model.state_dict())\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # --- Training on the training set ---\n",
    "        train_loss, train_acc, train_f1 = train_one_epoch(\n",
    "            model, train_loader, optimizer, criterion, device\n",
    "        )\n",
    "\n",
    "        # --- Evaluation on the validation set ---\n",
    "        val_loss, val_acc, val_f1 = eval_model(\n",
    "            model, val_loader, criterion, device\n",
    "        )\n",
    "\n",
    "        # Save metrics for later analysis / plotting\n",
    "        history.append(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"train_f1\": train_f1,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"val_f1\": val_f1,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f}, F1: {train_f1:.4f} | \"\n",
    "            f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, F1: {val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "        # Check if validation accuracy improved\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Stop if no improvement for 'patience' epochs\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"\\nEarly stopping triggered after {epoch} epochs.\")\n",
    "            break\n",
    "\n",
    "    # Restore the best model weights (highest validation accuracy)\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "32369352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 4) GCN-based graph classifier\n",
    "# ============================================================\n",
    "\n",
    "# WHAT:\n",
    "#   A simple 2-layer GCN for graph classification:\n",
    "#     - Two GCNConv layers to update node embeddings\n",
    "#     - Mean pooling over nodes to get graph-level embedding\n",
    "#     - Linear layer to predict class logits per graph\n",
    "#\n",
    "# WHY:\n",
    "#   GCN is a canonical GNN architecture and a strong baseline for\n",
    "#   many graph classification tasks.\n",
    "\n",
    "class GCNGraphClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # First GCN layer:\n",
    "        #   Input: node features of dimension in_channels\n",
    "        #   Output: hidden node embeddings\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "\n",
    "        # Second GCN layer:\n",
    "        #   Input: hidden embeddings\n",
    "        #   Output: hidden embeddings (same dimension)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Final linear layer to map graph embedding -> class logits\n",
    "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "        # Dropout rate for regularization (to reduce overfitting)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data is a PyG Batch object with:\n",
    "        #   data.x          : node features [num_nodes_total, in_channels]\n",
    "        #   data.edge_index : edges [2, num_edges_total]\n",
    "        #   data.batch      : graph IDs for each node [num_nodes_total]\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # 1st GCN layer followed by ReLU non-linearity\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Apply dropout to node embeddings during training\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # 2nd GCN layer + ReLU\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Graph-level pooling:\n",
    "        #   Aggregate node embeddings into a single vector per graph\n",
    "        #   Here we use mean pooling (can also try sum or max).\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Final linear layer to obtain logits for each graph\n",
    "        out = self.lin(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "8e3d84ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5) GraphSAGE-based graph classifier\n",
    "# ============================================================\n",
    "\n",
    "# WHAT:\n",
    "#   A 2-layer GraphSAGE model for graph classification:\n",
    "#     - Two SAGEConv layers to update node embeddings\n",
    "#     - Mean pooling for graph-level embedding\n",
    "#     - Linear layer for class logits\n",
    "#\n",
    "# WHY:\n",
    "#   GraphSAGE uses neighborhood sampling and aggregation, and is\n",
    "#   another standard architecture in graph ML. Comparing GraphSAGE\n",
    "#   and GCN fits the \"two GNN architectures\" requirement.\n",
    "\n",
    "class GraphSAGEGraphClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # First GraphSAGE layer\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "\n",
    "        # Second GraphSAGE layer\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        # Linear classifier from graph embedding to class logits\n",
    "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # 1st SAGE layer + ReLU\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # 2nd SAGE layer + ReLU\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Graph-level mean pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Linear classification layer\n",
    "        out = self.lin(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4de72151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# 5b) GIN-based graph classifier (fixed forward signature)\n",
    "# ============================================================\n",
    "\n",
    "class GINGraphClassifier(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, num_classes, dropout=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # MLP used inside the first GIN layer\n",
    "        mlp1 = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "        )\n",
    "        self.conv1 = GINConv(mlp1)\n",
    "\n",
    "        # MLP used inside the second GIN layer\n",
    "        mlp2 = nn.Sequential(\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels, hidden_channels),\n",
    "        )\n",
    "        self.conv2 = GINConv(mlp2)\n",
    "\n",
    "        # Final classifier on pooled graph embedding\n",
    "        self.lin = nn.Linear(hidden_channels, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # Unpack the batch object like in GCN/GraphSAGE\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GIN layer\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Second GIN layer\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Graph-level mean pooling\n",
    "        x = global_mean_pool(x, batch)\n",
    "\n",
    "        # Dropout before final linear layer\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        # Class logits\n",
    "        return self.lin(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c19eb6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from types import SimpleNamespace\n",
    "\n",
    "class GraphClassifierWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "        Small adapter that lets PyG's Explainer call your model with\n",
    "        (x, edge_index, batch) instead of a single `data` object.\n",
    "\n",
    "    WHY:\n",
    "        Your GCN/GraphSAGE are written as forward(self, data), but\n",
    "        Explainer expects forward(self, x, edge_index, batch).\n",
    "        This wrapper glues the two worlds together.\n",
    "\n",
    "    HOW:\n",
    "        - Receives x, edge_index, batch from Explainer\n",
    "        - Builds a lightweight object with attributes .x, .edge_index, .batch\n",
    "        - Calls the original model on that object\n",
    "    \"\"\"\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Create a simple \"data-like\" object with required attributes\n",
    "        data = SimpleNamespace(x=x, edge_index=edge_index, batch=batch)\n",
    "        return self.base_model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6f687880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== GCN MODEL ====================\n",
      "Epoch 001 | Train Loss: 0.6506, Acc: 0.6250, F1: 0.7308 | Val Loss: 0.6131, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6293, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6042, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6172, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6017, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6161, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5824, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6022, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5743, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5921, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5747, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5791, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5574, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5666, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5502, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5563, Acc: 0.7232, F1: 0.8268 | Val Loss: 0.5430, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5436, Acc: 0.7143, F1: 0.8202 | Val Loss: 0.5344, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 011 | Train Loss: 0.5146, Acc: 0.7500, F1: 0.8372 | Val Loss: 0.5326, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 012 | Train Loss: 0.5115, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5314, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.4983, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5320, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.5085, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5399, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.5386, Acc: 0.7321, F1: 0.7887 | Val Loss: 0.5691, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 016 | Train Loss: 0.4766, Acc: 0.7143, F1: 0.7895 | Val Loss: 0.5868, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 017 | Train Loss: 0.5077, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5437, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.5286, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5557, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 019 | Train Loss: 0.5383, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5494, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 020 | Train Loss: 0.4996, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5456, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.5021, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5353, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.4886, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5422, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 023 | Train Loss: 0.4879, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5361, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4789, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5364, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.5000, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5354, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4837, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5357, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4608, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5403, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4733, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5372, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      "GCN Test Results:\n",
      "  Loss: 0.5925\n",
      "  Acc:  0.6842\n",
      "  F1:   0.8065\n",
      "  ROC-AUC: 0.7015\n",
      "\n",
      "================= GraphSAGE MODEL =================\n",
      "Epoch 001 | Train Loss: 0.6728, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.6109, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6327, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5998, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6180, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6034, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6111, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5901, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6096, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5727, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5831, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5755, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5772, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5518, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5626, Acc: 0.6696, F1: 0.7978 | Val Loss: 0.5440, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5467, Acc: 0.6964, F1: 0.8068 | Val Loss: 0.5322, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 010 | Train Loss: 0.5420, Acc: 0.7054, F1: 0.7950 | Val Loss: 0.5380, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 011 | Train Loss: 0.5199, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5273, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 012 | Train Loss: 0.5336, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5322, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 013 | Train Loss: 0.5261, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5325, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.5107, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5521, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.5023, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5309, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 016 | Train Loss: 0.4987, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5365, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.5069, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5224, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 018 | Train Loss: 0.4856, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5256, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4903, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5211, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.5497, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5255, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 021 | Train Loss: 0.4643, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5687, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 022 | Train Loss: 0.5156, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5166, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 023 | Train Loss: 0.5012, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5199, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 024 | Train Loss: 0.4788, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5187, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4635, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5354, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4555, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5188, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 027 | Train Loss: 0.4795, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5232, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4699, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5479, Acc: 0.7368, F1: 0.7917\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      "GraphSAGE Test Results:\n",
      "  Loss: 0.5870\n",
      "  Acc:  0.6842\n",
      "  F1:   0.7931\n",
      "  ROC-AUC: 0.6985\n",
      "\n",
      "==================== GIN MODEL ====================\n",
      "Epoch 001 | Train Loss: 0.6550, Acc: 0.6339, F1: 0.7657 | Val Loss: 0.6103, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6329, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5989, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6101, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5837, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6315, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5970, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5829, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5673, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5716, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5368, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 007 | Train Loss: 0.5630, Acc: 0.6964, F1: 0.8090 | Val Loss: 0.5190, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 008 | Train Loss: 0.5076, Acc: 0.6964, F1: 0.7875 | Val Loss: 0.5274, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 009 | Train Loss: 0.5404, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5189, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 010 | Train Loss: 0.4888, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5145, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 011 | Train Loss: 0.4951, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.6481, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 012 | Train Loss: 0.5316, Acc: 0.7768, F1: 0.8120 | Val Loss: 0.5325, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 013 | Train Loss: 0.5744, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.4894, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 014 | Train Loss: 0.5061, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5054, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4837, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4878, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 016 | Train Loss: 0.4461, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5072, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 017 | Train Loss: 0.4621, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4806, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4970, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.4889, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 019 | Train Loss: 0.4683, Acc: 0.7857, F1: 0.8209 | Val Loss: 0.4799, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 020 | Train Loss: 0.4339, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5456, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 021 | Train Loss: 0.4816, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.4939, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 022 | Train Loss: 0.4734, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4698, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 023 | Train Loss: 0.4714, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4639, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 024 | Train Loss: 0.4509, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4616, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 025 | Train Loss: 0.5449, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4766, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 026 | Train Loss: 0.4642, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4905, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 027 | Train Loss: 0.4534, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4691, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 028 | Train Loss: 0.4516, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4569, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 029 | Train Loss: 0.4364, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4500, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 030 | Train Loss: 0.4271, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4446, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 031 | Train Loss: 0.4483, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4372, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 032 | Train Loss: 0.4534, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.4380, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 033 | Train Loss: 0.4737, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.4296, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 034 | Train Loss: 0.4049, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.4351, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 035 | Train Loss: 0.4409, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4383, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.4590, Acc: 0.8125, F1: 0.8467 | Val Loss: 0.4340, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 037 | Train Loss: 0.4600, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4320, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 038 | Train Loss: 0.4514, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4640, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 039 | Train Loss: 0.4070, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4445, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 040 | Train Loss: 0.4053, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4888, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 041 | Train Loss: 0.4306, Acc: 0.8393, F1: 0.8636 | Val Loss: 0.4315, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 042 | Train Loss: 0.4124, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4288, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 043 | Train Loss: 0.3858, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4839, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 044 | Train Loss: 0.3944, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4419, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 045 | Train Loss: 0.4339, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4763, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 046 | Train Loss: 0.4229, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.4260, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 047 | Train Loss: 0.3816, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4282, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 048 | Train Loss: 0.3866, Acc: 0.8125, F1: 0.8531 | Val Loss: 0.4180, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 049 | Train Loss: 0.4218, Acc: 0.7946, F1: 0.8369 | Val Loss: 0.4449, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 050 | Train Loss: 0.4237, Acc: 0.8125, F1: 0.8531 | Val Loss: 0.4184, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 051 | Train Loss: 0.3833, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4377, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 052 | Train Loss: 0.4033, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4330, Acc: 0.7368, F1: 0.7727\n",
      "\n",
      "Early stopping triggered after 52 epochs.\n",
      "\n",
      "GIN Test Results:\n",
      "  Loss: 0.5169\n",
      "  Acc:  0.7895\n",
      "  F1:   0.8462\n",
      "  ROC-AUC: 0.8338\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_type</th>\n",
       "      <th>hidden_channels</th>\n",
       "      <th>dropout</th>\n",
       "      <th>lr</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCN</td>\n",
       "      <td>64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>0.701538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GraphSAGE</td>\n",
       "      <td>64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.698462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GIN</td>\n",
       "      <td>64</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.833846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_type  hidden_channels  dropout    lr  accuracy        f1   roc_auc\n",
       "0        GCN               64      0.5  0.01  0.684211  0.806452  0.701538\n",
       "1  GraphSAGE               64      0.5  0.01  0.684211  0.793103  0.698462\n",
       "2        GIN               64      0.5  0.01  0.789474  0.846154  0.833846"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved main GNN results -> C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\results\\gnn_main_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# 6) Train & evaluate GCN, GraphSAGE and GIN\n",
    "# ============================================================\n",
    "\n",
    "num_node_features = dataset.num_features\n",
    "num_classes = dataset.num_classes\n",
    "BASE_GNN_LR = 0.01  # default learning rate for the main runs\n",
    "\n",
    "# --------------------- Train GCN ---------------------\n",
    "print(\"\\n==================== GCN MODEL ====================\")\n",
    "\n",
    "gcn_model = GCNGraphClassifier(\n",
    "    in_channels=num_node_features,\n",
    "    hidden_channels=64,   # you can ablate 32 vs 64 vs 128 later\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "gcn_model, gcn_history = train_with_early_stopping(\n",
    "    model=gcn_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    max_epochs=200,\n",
    "    lr=BASE_GNN_LR,\n",
    "    weight_decay=5e-4,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "# Evaluate on the test set\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "gcn_test_loss, gcn_test_acc, gcn_test_f1 = eval_model(\n",
    "    gcn_model, test_loader, criterion, device\n",
    ")\n",
    "\n",
    "gcn_auc, gcn_fpr, gcn_tpr = compute_roc_auc(gcn_model, test_loader, device)\n",
    "\n",
    "print(\"\\nGCN Test Results:\")\n",
    "print(f\"  Loss: {gcn_test_loss:.4f}\")\n",
    "print(f\"  Acc:  {gcn_test_acc:.4f}\")\n",
    "print(f\"  F1:   {gcn_test_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {gcn_auc:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gcn_metrics = {\n",
    "    \"accuracy\": float(gcn_test_acc),\n",
    "    \"f1\":       float(gcn_test_f1),\n",
    "    # If you don't compute ROC-AUC for GNNs, just mark as NaN.\n",
    "    # The plotting code will ignore/handle NaNs gracefully.\n",
    "    \"roc_auc\": float(gcn_auc),\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------- Train GraphSAGE -------------------\n",
    "print(\"\\n================= GraphSAGE MODEL =================\")\n",
    "\n",
    "sage_model = GraphSAGEGraphClassifier(\n",
    "    in_channels=num_node_features,\n",
    "    hidden_channels=64,\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "sage_model, sage_history = train_with_early_stopping(\n",
    "    model=sage_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    max_epochs=200,\n",
    "    lr=BASE_GNN_LR,\n",
    "    weight_decay=5e-4,\n",
    "    patience=20\n",
    ")\n",
    "\n",
    "sage_test_loss, sage_test_acc, sage_test_f1 = eval_model(\n",
    "    sage_model, test_loader, criterion, device\n",
    ")\n",
    "sage_auc, sage_fpr, sage_tpr = compute_roc_auc(sage_model, test_loader, device)\n",
    "\n",
    "print(\"\\nGraphSAGE Test Results:\")\n",
    "print(f\"  Loss: {sage_test_loss:.4f}\")\n",
    "print(f\"  Acc:  {sage_test_acc:.4f}\")\n",
    "print(f\"  F1:   {sage_test_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {sage_auc:.4f}\")\n",
    "\n",
    "sage_metrics = {\n",
    "    \"accuracy\": float(sage_test_acc),\n",
    "    \"f1\":       float(sage_test_f1),\n",
    "    \"roc_auc\":  float(sage_auc),\n",
    "}\n",
    "\n",
    "# --------------------- Train GIN ---------------------\n",
    "print(\"\\n==================== GIN MODEL ====================\")\n",
    "\n",
    "gin_model = GINGraphClassifier(\n",
    "    in_channels=num_node_features,\n",
    "    hidden_channels=64,   # keep same as GCN/SAGE for fair comparison\n",
    "    num_classes=num_classes,\n",
    "    dropout=0.5\n",
    ").to(device)\n",
    "\n",
    "gin_model, gin_history = train_with_early_stopping(\n",
    "    model=gin_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    device=device,\n",
    "    max_epochs=200,\n",
    "    lr=BASE_GNN_LR,\n",
    "    weight_decay=5e-4,\n",
    "    patience=20,\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "gin_test_loss, gin_test_acc, gin_test_f1 = eval_model(\n",
    "    gin_model, test_loader, criterion, device\n",
    ")\n",
    "gin_auc, gin_fpr, gin_tpr = compute_roc_auc(gin_model, test_loader, device)\n",
    "\n",
    "print(\"\\nGIN Test Results:\")\n",
    "print(f\"  Loss: {gin_test_loss:.4f}\")\n",
    "print(f\"  Acc:  {gin_test_acc:.4f}\")\n",
    "print(f\"  F1:   {gin_test_f1:.4f}\")\n",
    "print(f\"  ROC-AUC: {gin_auc:.4f}\")\n",
    "\n",
    "gin_metrics = {\n",
    "    \"accuracy\": float(gin_test_acc),\n",
    "    \"f1\":       float(gin_test_f1),\n",
    "    \"roc_auc\":  float(gin_auc),  # fill in later if you add ROC-AUC\n",
    "}\n",
    "\n",
    "# Collect main GNN results into a small table\n",
    "gnn_main_results = [\n",
    "    {\n",
    "        \"model_type\": \"GCN\",\n",
    "        \"hidden_channels\": 64,\n",
    "        \"dropout\": 0.5,\n",
    "        \"lr\": BASE_GNN_LR,\n",
    "        **gcn_metrics,\n",
    "    },\n",
    "    {\n",
    "        \"model_type\": \"GraphSAGE\",\n",
    "        \"hidden_channels\": 64,\n",
    "        \"dropout\": 0.5,\n",
    "        \"lr\": BASE_GNN_LR,\n",
    "        **sage_metrics,\n",
    "    },\n",
    "  {\n",
    "        \"model_type\": \"GIN\",\n",
    "        \"hidden_channels\": 64,\n",
    "        \"dropout\": 0.5,\n",
    "        \"lr\": BASE_GNN_LR,\n",
    "        **gin_metrics,\n",
    "     },\n",
    "]\n",
    "\n",
    "gnn_main_df = pd.DataFrame(gnn_main_results)\n",
    "display(gnn_main_df)\n",
    "\n",
    "gnn_main_csv = RESULTS_DIR / \"gnn_main_results.csv\"\n",
    "gnn_main_df.to_csv(gnn_main_csv, index=False)\n",
    "print(f\"Saved main GNN results -> {gnn_main_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "ae8bf176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# ============================================================\n",
    "# A.1) Helper: run a single GNN experiment (GCN or GraphSAGE)\n",
    "# ============================================================\n",
    "\n",
    "def run_gnn_experiment(\n",
    "    model_type,\n",
    "    hidden_channels=64,\n",
    "    dropout=0.5,\n",
    "    max_epochs=200,\n",
    "    lr=0.01,\n",
    "    weight_decay=5e-4,\n",
    "    patience=20,\n",
    "):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "        Train *one* GNN configuration and evaluate on the test set.\n",
    "\n",
    "    WHY:\n",
    "        Ablation = systematically testing different configurations.\n",
    "        This function encapsulates:\n",
    "            - model construction\n",
    "            - training w/ early stopping\n",
    "            - final test evaluation\n",
    "            - runtime measurement\n",
    "\n",
    "    INPUT:\n",
    "        model_type: \"gcn\" or \"sage\"\n",
    "        hidden_channels: size of hidden layer\n",
    "        dropout: dropout probability\n",
    "        max_epochs, lr, weight_decay, patience: training hyperparameters\n",
    "\n",
    "    RETURNS:\n",
    "        dict with:\n",
    "            - model_type\n",
    "            - hidden_channels\n",
    "            - dropout\n",
    "            - best_val_acc (from training)\n",
    "            - test_loss, test_acc, test_f1\n",
    "            - train_time_seconds\n",
    "    \"\"\"\n",
    "\n",
    "    num_node_features = dataset.num_features\n",
    "    num_classes = dataset.num_classes\n",
    "\n",
    "    # ----- 1) Instantiate the model based on type -----\n",
    "    mtype = model_type.lower()\n",
    "\n",
    "    if mtype == \"gcn\":\n",
    "        model = GCNGraphClassifier(\n",
    "            in_channels=num_node_features,\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "    elif mtype == \"sage\":\n",
    "        model = GraphSAGEGraphClassifier(\n",
    "            in_channels=num_node_features,\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "    elif mtype == \"gin\":\n",
    "        model = GINGraphClassifier(\n",
    "            in_channels=num_node_features,\n",
    "            hidden_channels=hidden_channels,\n",
    "            num_classes=num_classes,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(\"model_type must be 'gcn', 'sage', or 'gin'.\")\n",
    "\n",
    "\n",
    "    # ----- 2) Train with early stopping -----\n",
    "    start_time = time.time()\n",
    "\n",
    "    model, history = train_with_early_stopping(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        device=device,\n",
    "        max_epochs=max_epochs,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        patience=patience,\n",
    "    )\n",
    "\n",
    "    end_time = time.time() - start_time\n",
    "    train_time_sec = end_time - start_time\n",
    "    # Best validation accuracy from history\n",
    "    best_val_acc = max(h[\"val_acc\"] for h in history)\n",
    "\n",
    "    # ----- 3) Final test evaluation -----\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    test_loss, test_acc, test_f1 = eval_model(model, test_loader, criterion, device)\n",
    "    \n",
    "    try:\n",
    "        test_auc, _, _ = compute_roc_auc(model, test_loader, device)\n",
    "    except ValueError:\n",
    "        # e.g., if labels are single-class in some weird split\n",
    "        test_auc = np.nan\n",
    "\n",
    "    # Wrap everything in a dict for easy tabulation\n",
    "    result = {\n",
    "        \"model_type\": model_type,\n",
    "        \"hidden_channels\": hidden_channels,\n",
    "        \"dropout\": dropout,\n",
    "        \"lr\": lr,\n",
    "        \"seed\": seed,\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"test_roc_auc\": float(test_auc), \n",
    "        \"train_time_sec\": train_time_sec,   \n",
    "        \n",
    "    }\n",
    "\n",
    "    return result, model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "aefcc9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Running gcn hidden=16, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7434, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7430, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7365, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7342, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.7282, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7255, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 004 | Train Loss: 0.7201, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7170, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 005 | Train Loss: 0.7127, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7087, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 006 | Train Loss: 0.7052, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7002, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 007 | Train Loss: 0.6971, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6919, Acc: 0.4474, F1: 0.3226\n",
      "Epoch 008 | Train Loss: 0.6898, Acc: 0.6250, F1: 0.6719 | Val Loss: 0.6839, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 009 | Train Loss: 0.6834, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.6767, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.6770, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6699, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6712, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6631, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6649, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6569, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6603, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6509, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6559, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6454, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6518, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6405, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6473, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6365, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6442, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6328, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6419, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6295, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6395, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6266, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6371, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6244, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6359, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6223, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6343, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6207, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6339, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6194, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 024 | Train Loss: 0.6332, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6182, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 025 | Train Loss: 0.6323, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6175, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 026 | Train Loss: 0.6318, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6168, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 027 | Train Loss: 0.6316, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6160, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 028 | Train Loss: 0.6310, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6155, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6876, Acc: 0.5179, F1: 0.6494 | Val Loss: 0.6542, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6469, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6228, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6275, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6100, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6280, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6071, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6289, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6032, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6220, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5994, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6210, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5968, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6108, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5924, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6064, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5882, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6015, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5846, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5960, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5800, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5896, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5750, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5822, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5694, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5743, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5648, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5667, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5559, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5540, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5485, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5465, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5412, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 018 | Train Loss: 0.5336, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5378, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 019 | Train Loss: 0.5230, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5429, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.5201, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5302, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.5095, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5298, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.5006, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5426, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.5027, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5403, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.5006, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5381, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.4948, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5385, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.4969, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5397, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4901, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5535, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 028 | Train Loss: 0.5006, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5570, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 029 | Train Loss: 0.4978, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5493, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4896, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5431, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 031 | Train Loss: 0.4995, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5421, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 032 | Train Loss: 0.4928, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5432, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 033 | Train Loss: 0.4883, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5428, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 034 | Train Loss: 0.4884, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5426, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 035 | Train Loss: 0.4858, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5449, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 036 | Train Loss: 0.4867, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5420, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 037 | Train Loss: 0.4864, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5407, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6591, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6108, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6324, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6041, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6249, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5971, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6188, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5817, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6156, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5981, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5825, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5681, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5777, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5449, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 008 | Train Loss: 0.5373, Acc: 0.7500, F1: 0.8333 | Val Loss: 0.5354, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 009 | Train Loss: 0.5137, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5405, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 010 | Train Loss: 0.5263, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5399, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 011 | Train Loss: 0.4954, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5603, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 012 | Train Loss: 0.5101, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5484, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.5064, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5533, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4791, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5574, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 015 | Train Loss: 0.5198, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5478, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.5090, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5375, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.4874, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5611, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 018 | Train Loss: 0.4966, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5367, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 019 | Train Loss: 0.4880, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5400, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4922, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5396, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.5051, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5361, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.5090, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5578, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 023 | Train Loss: 0.4959, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5435, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 024 | Train Loss: 0.4815, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5486, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 025 | Train Loss: 0.5400, Acc: 0.7321, F1: 0.7857 | Val Loss: 0.5366, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.5332, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5529, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 027 | Train Loss: 0.5264, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5940, Acc: 0.6842, F1: 0.7273\n",
      "\n",
      "Early stopping triggered after 27 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6703, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6619, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6647, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6557, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6602, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6499, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6558, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6448, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6519, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6404, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6481, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6364, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6450, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6323, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6424, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6282, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6388, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6246, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6361, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6214, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6343, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6185, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6326, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6159, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6304, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6139, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6295, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6121, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6280, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6108, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6270, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6095, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6271, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6083, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6259, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6079, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6251, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6072, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6246, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6064, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6243, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6057, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7093, Acc: 0.3839, F1: 0.4000 | Val Loss: 0.6583, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6546, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6247, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6319, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6161, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6325, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6118, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6310, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6058, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6233, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6022, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6174, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5973, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6124, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5963, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6063, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5875, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5981, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5766, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5868, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5690, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5834, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5614, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5715, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5538, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5627, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5504, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 015 | Train Loss: 0.5532, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5443, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 016 | Train Loss: 0.5400, Acc: 0.7321, F1: 0.8295 | Val Loss: 0.5342, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 017 | Train Loss: 0.5382, Acc: 0.7232, F1: 0.8268 | Val Loss: 0.5331, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 018 | Train Loss: 0.5309, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5278, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 019 | Train Loss: 0.5237, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5497, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.5184, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5243, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 021 | Train Loss: 0.5094, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.5267, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 022 | Train Loss: 0.5120, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.5238, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 023 | Train Loss: 0.5133, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5520, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 024 | Train Loss: 0.5071, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5250, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.5286, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5327, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 026 | Train Loss: 0.4945, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5376, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.5014, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5447, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.5025, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5264, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 029 | Train Loss: 0.4916, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5280, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.4870, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5325, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4867, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5356, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.4881, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5325, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.4844, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5324, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 034 | Train Loss: 0.4871, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5366, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 34 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6468, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6062, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6295, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6006, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6079, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5836, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5901, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5576, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6017, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5452, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 006 | Train Loss: 0.5986, Acc: 0.7054, F1: 0.7755 | Val Loss: 0.5634, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 007 | Train Loss: 0.5734, Acc: 0.7143, F1: 0.8140 | Val Loss: 0.5663, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5593, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5384, Acc: 0.6316, F1: 0.7586\n",
      "Epoch 009 | Train Loss: 0.5360, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5470, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 010 | Train Loss: 0.5286, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5250, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 011 | Train Loss: 0.5037, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5268, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 012 | Train Loss: 0.4917, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5388, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.4847, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5567, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.4867, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5713, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.5128, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5595, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.4923, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5780, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 017 | Train Loss: 0.5385, Acc: 0.7054, F1: 0.7519 | Val Loss: 0.5460, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 018 | Train Loss: 0.5415, Acc: 0.7857, F1: 0.8554 | Val Loss: 0.5328, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 019 | Train Loss: 0.4809, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5735, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 020 | Train Loss: 0.5056, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5332, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 021 | Train Loss: 0.5545, Acc: 0.7857, F1: 0.8571 | Val Loss: 0.5404, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 022 | Train Loss: 0.4814, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5976, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 023 | Train Loss: 0.5258, Acc: 0.7411, F1: 0.7943 | Val Loss: 0.5267, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 024 | Train Loss: 0.4888, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.5552, Acc: 0.6316, F1: 0.7586\n",
      "Epoch 025 | Train Loss: 0.4972, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.5496, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 25 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7096, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7070, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7014, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6973, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.6929, Acc: 0.4554, F1: 0.3711 | Val Loss: 0.6879, Acc: 0.7632, F1: 0.8475\n",
      "Epoch 004 | Train Loss: 0.6847, Acc: 0.7054, F1: 0.8156 | Val Loss: 0.6790, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6774, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6704, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6687, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6628, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6643, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6555, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6572, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6489, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6514, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6427, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6479, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6368, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6438, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6321, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6395, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6280, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6355, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6242, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6332, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6209, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6309, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6180, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6294, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6158, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6280, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6139, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6262, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6121, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6244, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6107, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6237, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6099, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6230, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6091, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6223, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6085, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6242, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6077, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 23 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6855, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6558, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6537, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6304, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6353, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6140, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6381, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6111, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6318, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6097, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6289, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6088, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6262, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6068, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6238, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6053, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6205, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6040, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6173, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5978, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6101, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5897, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6039, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5820, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5930, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5735, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5797, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5671, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5703, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5507, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 016 | Train Loss: 0.5562, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5404, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 017 | Train Loss: 0.5381, Acc: 0.7232, F1: 0.8208 | Val Loss: 0.5400, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 018 | Train Loss: 0.5264, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5284, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 019 | Train Loss: 0.5152, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5280, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.5049, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5297, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.5025, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5330, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.5044, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5310, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.4892, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5426, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 024 | Train Loss: 0.4866, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5395, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4947, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5367, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4901, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5456, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4957, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5369, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.5045, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5369, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 029 | Train Loss: 0.4846, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5443, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4902, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5449, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.4830, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5383, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 032 | Train Loss: 0.5021, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5381, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 033 | Train Loss: 0.4914, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5475, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 034 | Train Loss: 0.4827, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5368, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.4779, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5389, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 036 | Train Loss: 0.4982, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5367, Acc: 0.7105, F1: 0.7843\n",
      "\n",
      "Early stopping triggered after 36 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6451, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6160, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6525, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6222, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6284, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6070, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6236, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6030, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6425, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5932, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5990, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6094, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6219, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6105, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6039, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5743, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6000, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5734, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5782, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5642, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5801, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5730, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5651, Acc: 0.7411, F1: 0.8362 | Val Loss: 0.5446, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 013 | Train Loss: 0.5502, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5453, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5346, Acc: 0.7232, F1: 0.8268 | Val Loss: 0.5390, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 015 | Train Loss: 0.5311, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5291, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 016 | Train Loss: 0.5534, Acc: 0.7232, F1: 0.8249 | Val Loss: 0.5277, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 017 | Train Loss: 0.5245, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5550, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4979, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5514, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 019 | Train Loss: 0.5194, Acc: 0.7589, F1: 0.8402 | Val Loss: 0.5267, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 020 | Train Loss: 0.4957, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5289, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.4973, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5284, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.4968, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5310, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 023 | Train Loss: 0.4974, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5297, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.4898, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5350, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4799, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5333, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 026 | Train Loss: 0.4811, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5333, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 027 | Train Loss: 0.4866, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5368, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4954, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5317, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4730, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5337, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.4936, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5352, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5253, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5260, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 31 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7061, Acc: 0.3482, F1: 0.1205 | Val Loss: 0.6980, Acc: 0.3947, F1: 0.4103\n",
      "Epoch 002 | Train Loss: 0.6970, Acc: 0.3750, F1: 0.5070 | Val Loss: 0.6882, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6885, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6799, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6811, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6729, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6762, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6661, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6698, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6602, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6648, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6548, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6618, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6498, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6581, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6450, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6536, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6413, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6507, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6382, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6497, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6355, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6461, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6331, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6464, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6308, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6441, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6287, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6428, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6267, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6382, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6250, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6382, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6233, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6388, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6219, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6384, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6204, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6367, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6190, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6366, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6178, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 22 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6836, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6456, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6418, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6212, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6380, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6148, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6340, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6121, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6343, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6093, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6291, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6063, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6270, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6066, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6221, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6054, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6196, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6004, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6171, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5921, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6095, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5860, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6054, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5787, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5936, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5775, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5911, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5773, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5832, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5586, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 016 | Train Loss: 0.5661, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5493, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 017 | Train Loss: 0.5540, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5423, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 018 | Train Loss: 0.5462, Acc: 0.7946, F1: 0.8623 | Val Loss: 0.5398, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 019 | Train Loss: 0.5454, Acc: 0.6786, F1: 0.7931 | Val Loss: 0.5321, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 020 | Train Loss: 0.5211, Acc: 0.7500, F1: 0.8372 | Val Loss: 0.5313, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.5293, Acc: 0.7054, F1: 0.7871 | Val Loss: 0.5417, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 022 | Train Loss: 0.5232, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5234, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.5151, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5253, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 024 | Train Loss: 0.5186, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5274, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.5012, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5278, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.5079, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5285, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.5060, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5308, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.5014, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5608, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 029 | Train Loss: 0.5021, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5327, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.5069, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5324, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 031 | Train Loss: 0.5052, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5318, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.4902, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5344, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.4975, Acc: 0.7232, F1: 0.8000 | Val Loss: 0.5314, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.5004, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5317, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.4937, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5380, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 036 | Train Loss: 0.4926, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5324, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 037 | Train Loss: 0.4900, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5320, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6588, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6163, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6364, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6149, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6367, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6147, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6453, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6007, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6289, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6077, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6274, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6062, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6121, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5853, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5929, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5624, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5579, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5520, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 010 | Train Loss: 0.5460, Acc: 0.7500, F1: 0.8353 | Val Loss: 0.5224, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 011 | Train Loss: 0.5086, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5471, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 012 | Train Loss: 0.5214, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5805, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 013 | Train Loss: 0.5459, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5416, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.5197, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5388, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.4994, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5509, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 016 | Train Loss: 0.4798, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5302, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 017 | Train Loss: 0.5014, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5276, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 018 | Train Loss: 0.4926, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5395, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.5141, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5305, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 020 | Train Loss: 0.5216, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5367, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4836, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5328, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 022 | Train Loss: 0.4999, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5271, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 023 | Train Loss: 0.4828, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5382, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4850, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5301, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 025 | Train Loss: 0.4800, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5330, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 026 | Train Loss: 0.4914, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5433, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4988, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5387, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4826, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5291, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4769, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5485, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4892, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5307, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.4909, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5265, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 032 | Train Loss: 0.4769, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5417, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.4784, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5299, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 034 | Train Loss: 0.4942, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5385, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4955, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5556, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 036 | Train Loss: 0.4769, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5379, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.5138, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5444, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 038 | Train Loss: 0.4897, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5549, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 039 | Train Loss: 0.4665, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5400, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 39 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7463, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7424, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7310, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7290, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.7234, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7170, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 004 | Train Loss: 0.7117, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7061, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 005 | Train Loss: 0.7033, Acc: 0.3482, F1: 0.0267 | Val Loss: 0.6955, Acc: 0.4211, F1: 0.2667\n",
      "Epoch 006 | Train Loss: 0.6926, Acc: 0.5179, F1: 0.4490 | Val Loss: 0.6862, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 007 | Train Loss: 0.6856, Acc: 0.6696, F1: 0.7613 | Val Loss: 0.6769, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 008 | Train Loss: 0.6777, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.6682, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6703, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6597, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6634, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6520, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6582, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6449, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6508, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6382, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6436, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6325, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6401, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6269, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6364, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6222, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6352, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6178, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6298, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6142, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6266, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6120, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6278, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6101, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6295, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6084, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6287, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6075, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6260, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6068, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6258, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6060, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 024 | Train Loss: 0.6248, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6053, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 025 | Train Loss: 0.6270, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6047, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 026 | Train Loss: 0.6210, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6041, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 26 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6491, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6233, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6382, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6163, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6364, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6138, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6320, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6128, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6297, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6180, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6305, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6163, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6286, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6135, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6271, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6058, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6186, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6004, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6140, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5945, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6109, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5911, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6113, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5806, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5970, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5792, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5855, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5674, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5779, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5585, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5711, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5547, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 017 | Train Loss: 0.5508, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5496, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 018 | Train Loss: 0.5580, Acc: 0.7054, F1: 0.8114 | Val Loss: 0.5379, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 019 | Train Loss: 0.5371, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.5363, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 020 | Train Loss: 0.5398, Acc: 0.7411, F1: 0.8242 | Val Loss: 0.5291, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.5205, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5305, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.5263, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5264, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 023 | Train Loss: 0.5118, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5301, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.5004, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5298, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4858, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5296, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 026 | Train Loss: 0.5073, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5374, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.5106, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5366, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4952, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5325, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4789, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5375, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.5031, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5382, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5028, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5350, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.5143, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5361, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.5172, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5506, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 034 | Train Loss: 0.4858, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5499, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 035 | Train Loss: 0.4882, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5407, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 036 | Train Loss: 0.5041, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5351, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 36 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6782, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6173, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6325, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6109, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6233, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5946, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6075, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5762, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5834, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5746, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 006 | Train Loss: 0.5865, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5427, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 007 | Train Loss: 0.5653, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5307, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 008 | Train Loss: 0.5329, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5372, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 009 | Train Loss: 0.5307, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5316, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 010 | Train Loss: 0.5477, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5595, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 011 | Train Loss: 0.4963, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5645, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 012 | Train Loss: 0.5143, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5664, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 013 | Train Loss: 0.5477, Acc: 0.7411, F1: 0.7943 | Val Loss: 0.5302, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.5353, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5334, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.5390, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5632, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 016 | Train Loss: 0.5326, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5492, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 017 | Train Loss: 0.5253, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5406, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.5096, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5426, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4913, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5427, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.5317, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5403, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4953, Acc: 0.7232, F1: 0.7862 | Val Loss: 0.5545, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 022 | Train Loss: 0.4901, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5457, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 023 | Train Loss: 0.5026, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5355, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.4896, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5511, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 025 | Train Loss: 0.5108, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5376, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4802, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5404, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 26 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6979, Acc: 0.4018, F1: 0.1928 | Val Loss: 0.6988, Acc: 0.2895, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.6942, Acc: 0.4643, F1: 0.5161 | Val Loss: 0.6904, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6848, Acc: 0.6607, F1: 0.7865 | Val Loss: 0.6826, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6754, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.6752, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6733, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6685, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6659, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6617, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6593, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6554, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6578, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6494, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6490, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6437, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6423, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6380, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6411, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6330, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6356, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6289, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6372, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6254, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6300, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6223, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6319, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6196, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6330, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6173, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6295, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6154, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6227, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6140, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6285, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6125, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6276, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6117, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6269, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6110, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6244, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6099, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 22 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6596, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6291, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6347, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6121, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6332, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6093, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6367, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6059, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6212, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6036, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6147, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6005, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6222, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6017, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6069, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5954, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6027, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5889, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5973, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5841, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5902, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5806, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5968, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5779, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5821, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5722, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5686, Acc: 0.7232, F1: 0.8268 | Val Loss: 0.5640, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5709, Acc: 0.6607, F1: 0.7912 | Val Loss: 0.5603, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 016 | Train Loss: 0.5657, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.5546, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 017 | Train Loss: 0.5616, Acc: 0.7143, F1: 0.8161 | Val Loss: 0.5471, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 018 | Train Loss: 0.5300, Acc: 0.7411, F1: 0.8362 | Val Loss: 0.5417, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 019 | Train Loss: 0.5228, Acc: 0.7589, F1: 0.8402 | Val Loss: 0.5411, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 020 | Train Loss: 0.5497, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5382, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 021 | Train Loss: 0.5331, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5324, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 022 | Train Loss: 0.5439, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5303, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 023 | Train Loss: 0.5116, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5479, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 024 | Train Loss: 0.5286, Acc: 0.7143, F1: 0.7838 | Val Loss: 0.5312, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 025 | Train Loss: 0.5225, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5294, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 026 | Train Loss: 0.5525, Acc: 0.7232, F1: 0.8144 | Val Loss: 0.5297, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 027 | Train Loss: 0.5178, Acc: 0.7232, F1: 0.8121 | Val Loss: 0.5284, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 028 | Train Loss: 0.5344, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5370, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.5267, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5333, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4861, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5344, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 031 | Train Loss: 0.4993, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5293, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 032 | Train Loss: 0.5062, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5386, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.5216, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5354, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.4963, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5317, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.4694, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5320, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 036 | Train Loss: 0.5215, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5316, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 037 | Train Loss: 0.5051, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5335, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 038 | Train Loss: 0.5102, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5406, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 039 | Train Loss: 0.5243, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5385, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 040 | Train Loss: 0.5065, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5342, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 041 | Train Loss: 0.4778, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5362, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 042 | Train Loss: 0.4995, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5315, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 043 | Train Loss: 0.5141, Acc: 0.7143, F1: 0.7922 | Val Loss: 0.5397, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 044 | Train Loss: 0.5235, Acc: 0.7143, F1: 0.7867 | Val Loss: 0.5480, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 045 | Train Loss: 0.4901, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5322, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 046 | Train Loss: 0.5031, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5303, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 047 | Train Loss: 0.5138, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5288, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 048 | Train Loss: 0.5306, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5327, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 049 | Train Loss: 0.5176, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5335, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 050 | Train Loss: 0.4983, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5245, Acc: 0.6842, F1: 0.7692\n",
      "\n",
      "Early stopping triggered after 50 epochs.\n",
      "\n",
      ">>> Running gcn hidden=16, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6387, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6233, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6406, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6129, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6190, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5919, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5941, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5535, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 005 | Train Loss: 0.5571, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.6293, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6427, Acc: 0.7143, F1: 0.8095 | Val Loss: 0.5680, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 007 | Train Loss: 0.5442, Acc: 0.7054, F1: 0.7815 | Val Loss: 0.5294, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 008 | Train Loss: 0.5067, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5496, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 009 | Train Loss: 0.5166, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5320, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 010 | Train Loss: 0.5148, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5547, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 011 | Train Loss: 0.5336, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5442, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.5092, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5376, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.5389, Acc: 0.7232, F1: 0.8050 | Val Loss: 0.5368, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.5003, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5431, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.4917, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5491, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 016 | Train Loss: 0.5239, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5619, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 017 | Train Loss: 0.5307, Acc: 0.7321, F1: 0.7887 | Val Loss: 0.5337, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5243, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5342, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.4799, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5585, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 020 | Train Loss: 0.5218, Acc: 0.7143, F1: 0.7808 | Val Loss: 0.5341, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.5143, Acc: 0.7232, F1: 0.8050 | Val Loss: 0.5337, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.5115, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5543, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 023 | Train Loss: 0.5452, Acc: 0.6964, F1: 0.7763 | Val Loss: 0.5262, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 024 | Train Loss: 0.5001, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5305, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 24 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7472, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7459, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7367, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7350, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.7267, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7260, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 004 | Train Loss: 0.7190, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7169, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 005 | Train Loss: 0.7108, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7079, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 006 | Train Loss: 0.7022, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6990, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 007 | Train Loss: 0.6945, Acc: 0.3839, F1: 0.1266 | Val Loss: 0.6899, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 008 | Train Loss: 0.6865, Acc: 0.6875, F1: 0.7712 | Val Loss: 0.6804, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6778, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6708, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6698, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6609, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6602, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6513, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6533, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6418, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6453, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6332, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6383, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6255, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6330, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6190, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6284, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6145, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6257, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6113, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6247, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6094, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6241, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6080, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6231, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6071, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6230, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6062, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6218, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6056, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6205, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6052, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 024 | Train Loss: 0.6198, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6054, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 025 | Train Loss: 0.6194, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6055, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 026 | Train Loss: 0.6181, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6049, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 027 | Train Loss: 0.6173, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6038, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 27 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6764, Acc: 0.5982, F1: 0.6763 | Val Loss: 0.6182, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6423, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6116, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6449, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6030, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6232, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6006, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6184, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5968, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6124, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5902, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6038, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5774, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5944, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5671, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5767, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5545, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5718, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5433, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5491, Acc: 0.7321, F1: 0.8315 | Val Loss: 0.5515, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 012 | Train Loss: 0.5421, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5301, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 013 | Train Loss: 0.5245, Acc: 0.7232, F1: 0.8144 | Val Loss: 0.5247, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 014 | Train Loss: 0.5381, Acc: 0.7232, F1: 0.8050 | Val Loss: 0.5343, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.5239, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5297, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 016 | Train Loss: 0.5103, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5284, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.5013, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5504, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 018 | Train Loss: 0.5049, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5281, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 019 | Train Loss: 0.5131, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5306, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.5008, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5707, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 021 | Train Loss: 0.4948, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5320, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 022 | Train Loss: 0.5026, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5412, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 023 | Train Loss: 0.5067, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5400, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4952, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5482, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 025 | Train Loss: 0.4890, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5329, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4927, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5332, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4855, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5427, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4857, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5366, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4816, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5349, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4867, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5343, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6402, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6802, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6641, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5942, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6251, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5845, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6039, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5919, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5960, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5618, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5738, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5412, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 007 | Train Loss: 0.5371, Acc: 0.7232, F1: 0.8268 | Val Loss: 0.5313, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 008 | Train Loss: 0.5549, Acc: 0.7321, F1: 0.7887 | Val Loss: 0.5814, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.6475, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5493, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 010 | Train Loss: 0.6728, Acc: 0.6161, F1: 0.6504 | Val Loss: 0.5653, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 011 | Train Loss: 0.5769, Acc: 0.6339, F1: 0.7515 | Val Loss: 0.5600, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5576, Acc: 0.6696, F1: 0.7978 | Val Loss: 0.5408, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 013 | Train Loss: 0.5424, Acc: 0.7768, F1: 0.8538 | Val Loss: 0.5518, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 014 | Train Loss: 0.5425, Acc: 0.7679, F1: 0.8488 | Val Loss: 0.5361, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 015 | Train Loss: 0.5394, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5340, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 016 | Train Loss: 0.5243, Acc: 0.7589, F1: 0.8439 | Val Loss: 0.5275, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 017 | Train Loss: 0.5080, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5323, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.5107, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5412, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.5317, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5464, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4885, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5695, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 021 | Train Loss: 0.5012, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5639, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 022 | Train Loss: 0.5411, Acc: 0.7143, F1: 0.7612 | Val Loss: 0.5415, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4829, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5607, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 024 | Train Loss: 0.5108, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5351, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.5168, Acc: 0.7232, F1: 0.7862 | Val Loss: 0.5352, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4811, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5354, Acc: 0.6842, F1: 0.7692\n",
      "\n",
      "Early stopping triggered after 26 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6535, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6414, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6483, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6341, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6420, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6292, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6392, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6242, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6355, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6198, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6323, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6163, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6311, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6131, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6295, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6110, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6273, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6096, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6271, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6082, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6262, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6068, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6258, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6056, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6248, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6046, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6254, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6037, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6238, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6032, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6244, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6030, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6222, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6024, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6219, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6208, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6011, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6205, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5999, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6187, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5990, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6975, Acc: 0.4107, F1: 0.4590 | Val Loss: 0.6369, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6393, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6083, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6274, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6034, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6195, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6000, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6163, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5950, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6185, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5982, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6068, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5875, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5965, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5809, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5893, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5739, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5841, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5665, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5756, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5596, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5636, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5569, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 013 | Train Loss: 0.5651, Acc: 0.7321, F1: 0.8295 | Val Loss: 0.5560, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 014 | Train Loss: 0.5498, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5431, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 015 | Train Loss: 0.5425, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5365, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 016 | Train Loss: 0.5292, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.5344, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 017 | Train Loss: 0.5195, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5318, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 018 | Train Loss: 0.5183, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5305, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.5122, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5257, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 020 | Train Loss: 0.5013, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5345, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.4976, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5290, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 022 | Train Loss: 0.4997, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5322, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.4921, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5407, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4917, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5369, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.4990, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5460, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4889, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5374, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4996, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5433, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4928, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5349, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4878, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5414, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.5302, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5768, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 031 | Train Loss: 0.5124, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5393, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 032 | Train Loss: 0.4969, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5320, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 32 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6517, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6460, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6443, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6262, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5949, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6035, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5767, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5731, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5817, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 006 | Train Loss: 0.5598, Acc: 0.7143, F1: 0.8025 | Val Loss: 0.5405, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 007 | Train Loss: 0.5196, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5655, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 008 | Train Loss: 0.5361, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.6971, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 009 | Train Loss: 0.5735, Acc: 0.7232, F1: 0.7634 | Val Loss: 0.5610, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 010 | Train Loss: 0.5537, Acc: 0.8036, F1: 0.8690 | Val Loss: 0.5394, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 011 | Train Loss: 0.5218, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5608, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 012 | Train Loss: 0.5120, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5318, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 013 | Train Loss: 0.5222, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5284, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.5034, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5368, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.4993, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5366, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.5121, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5345, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.5334, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5322, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.5499, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5298, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.4978, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5444, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 020 | Train Loss: 0.5032, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5494, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 021 | Train Loss: 0.4982, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5418, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7424, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7387, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7296, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7261, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.7186, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7145, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 004 | Train Loss: 0.7082, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7035, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 005 | Train Loss: 0.6980, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6924, Acc: 0.6316, F1: 0.6500\n",
      "Epoch 006 | Train Loss: 0.6904, Acc: 0.6071, F1: 0.6986 | Val Loss: 0.6814, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6794, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6713, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6709, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6616, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6630, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6527, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6546, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6447, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6480, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6369, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6436, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6300, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6367, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6245, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6338, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6194, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6299, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6153, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6273, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6121, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6263, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6093, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6244, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6074, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6064, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6228, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6055, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6225, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6044, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6195, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6038, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6185, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6030, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 024 | Train Loss: 0.6180, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6021, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 025 | Train Loss: 0.6167, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6013, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 026 | Train Loss: 0.6154, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6000, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 26 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6490, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6124, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6298, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6076, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6258, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6032, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6213, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5975, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6154, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5978, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6057, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5865, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5954, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5782, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5880, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5703, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5812, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5616, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5615, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5714, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 011 | Train Loss: 0.5665, Acc: 0.7500, F1: 0.8372 | Val Loss: 0.5468, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5421, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5364, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 013 | Train Loss: 0.5299, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.5396, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.5176, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5280, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 015 | Train Loss: 0.5289, Acc: 0.7768, F1: 0.8521 | Val Loss: 0.5374, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 016 | Train Loss: 0.5288, Acc: 0.7054, F1: 0.7871 | Val Loss: 0.5503, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 017 | Train Loss: 0.5103, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5299, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.5155, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5457, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 019 | Train Loss: 0.5098, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5344, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4948, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5480, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 021 | Train Loss: 0.4968, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5366, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4832, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5338, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4934, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5346, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4934, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5480, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4835, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5358, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 026 | Train Loss: 0.5016, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5337, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4818, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5526, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 028 | Train Loss: 0.4873, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5315, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4808, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5369, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4982, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5282, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 031 | Train Loss: 0.4814, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5385, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 31 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6308, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5907, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6295, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5953, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.5943, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5698, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5930, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.6082, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 005 | Train Loss: 0.5653, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.5481, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 006 | Train Loss: 0.5881, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5273, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 007 | Train Loss: 0.5255, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5495, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 008 | Train Loss: 0.5083, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5375, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 009 | Train Loss: 0.5094, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.6013, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 010 | Train Loss: 0.5440, Acc: 0.7321, F1: 0.7826 | Val Loss: 0.5818, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 011 | Train Loss: 0.5695, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.5302, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 012 | Train Loss: 0.5053, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5395, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 013 | Train Loss: 0.4989, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5267, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 014 | Train Loss: 0.4923, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5312, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.4904, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5377, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4857, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5632, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 017 | Train Loss: 0.4938, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5611, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 018 | Train Loss: 0.4929, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5399, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4837, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5342, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4964, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5373, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4983, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5599, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.5331, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5475, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 023 | Train Loss: 0.4999, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5259, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 024 | Train Loss: 0.4794, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5375, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4912, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5293, Acc: 0.7105, F1: 0.7925\n",
      "\n",
      "Early stopping triggered after 25 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7046, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6985, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.6914, Acc: 0.5179, F1: 0.4375 | Val Loss: 0.6859, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 003 | Train Loss: 0.6811, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.6731, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6719, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6601, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6608, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6488, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6511, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6381, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6434, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6282, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6372, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6197, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6299, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6142, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6266, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6089, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6218, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6051, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6207, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6022, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6195, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6000, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6187, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5984, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6194, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5974, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6142, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5966, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6121, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5955, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6131, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5949, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6117, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5942, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6108, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5934, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6104, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5926, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6102, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5920, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 22 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6421, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6101, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6279, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6057, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6251, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6025, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6159, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6031, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6076, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5886, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5930, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5801, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5855, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5739, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5713, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5596, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5540, Acc: 0.7143, F1: 0.8202 | Val Loss: 0.5537, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.5408, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5461, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 011 | Train Loss: 0.5235, Acc: 0.7143, F1: 0.8000 | Val Loss: 0.5397, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 012 | Train Loss: 0.5303, Acc: 0.6964, F1: 0.7763 | Val Loss: 0.5403, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 013 | Train Loss: 0.5151, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5468, Acc: 0.6316, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.5014, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5530, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 015 | Train Loss: 0.5112, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5471, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4975, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5400, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 017 | Train Loss: 0.4886, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5427, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4982, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5384, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.5009, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5390, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 020 | Train Loss: 0.4861, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5573, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 021 | Train Loss: 0.4954, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5394, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4976, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5371, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 023 | Train Loss: 0.4889, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5362, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.5052, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5643, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 025 | Train Loss: 0.4883, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5357, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 026 | Train Loss: 0.5124, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5333, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4766, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5513, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 27 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7352, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6090, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6538, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6316, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6266, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6056, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6202, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5964, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6075, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5856, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5927, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5712, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5928, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5634, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6086, Acc: 0.6875, F1: 0.8000 | Val Loss: 0.5607, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5532, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5563, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5494, Acc: 0.7054, F1: 0.8156 | Val Loss: 0.5488, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 011 | Train Loss: 0.5127, Acc: 0.7857, F1: 0.8554 | Val Loss: 0.5592, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5340, Acc: 0.7143, F1: 0.8202 | Val Loss: 0.6476, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 013 | Train Loss: 0.5623, Acc: 0.7411, F1: 0.7883 | Val Loss: 0.5676, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 014 | Train Loss: 0.5927, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.5333, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.4973, Acc: 0.7232, F1: 0.7862 | Val Loss: 0.5330, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.4757, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5544, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 017 | Train Loss: 0.5270, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5391, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.5252, Acc: 0.7321, F1: 0.7857 | Val Loss: 0.5421, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.5338, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5573, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 020 | Train Loss: 0.4820, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5775, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 021 | Train Loss: 0.5367, Acc: 0.7321, F1: 0.7887 | Val Loss: 0.5284, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.4903, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5325, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 023 | Train Loss: 0.4974, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5323, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.4932, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5338, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 025 | Train Loss: 0.4957, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5307, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.4868, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5342, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.5035, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5357, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 028 | Train Loss: 0.4787, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5416, Acc: 0.7368, F1: 0.7917\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7264, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7157, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7121, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7021, Acc: 0.3158, F1: 0.1333\n",
      "Epoch 003 | Train Loss: 0.6999, Acc: 0.3571, F1: 0.3684 | Val Loss: 0.6906, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6898, Acc: 0.6161, F1: 0.7624 | Val Loss: 0.6810, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6829, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6722, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6739, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6647, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6673, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6580, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6617, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6518, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6565, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6463, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6534, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6414, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6497, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6369, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6475, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6331, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6426, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6300, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6396, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6270, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6360, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6251, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6368, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6229, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6351, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6210, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6325, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6195, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6320, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6181, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6338, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6170, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6315, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6160, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6305, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6155, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6315, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6151, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 23 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6967, Acc: 0.4643, F1: 0.6104 | Val Loss: 0.6338, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6378, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6122, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6312, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6047, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6261, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6001, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6133, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6158, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5993, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6032, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5843, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5916, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5749, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5877, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5667, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5652, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5589, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.5563, Acc: 0.7411, F1: 0.8362 | Val Loss: 0.5535, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 012 | Train Loss: 0.5423, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5361, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 013 | Train Loss: 0.5259, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.5319, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 014 | Train Loss: 0.5259, Acc: 0.6964, F1: 0.7763 | Val Loss: 0.5335, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.5074, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5298, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 016 | Train Loss: 0.5058, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5401, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.5087, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5386, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.5043, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5372, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 019 | Train Loss: 0.5062, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5355, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.5038, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5431, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4959, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5352, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.5096, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5332, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 023 | Train Loss: 0.5008, Acc: 0.7232, F1: 0.8000 | Val Loss: 0.5376, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4951, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5307, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4819, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5369, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4871, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5300, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4854, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5315, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4943, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5308, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 029 | Train Loss: 0.4890, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5317, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4899, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5366, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.4874, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5344, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.4895, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5323, Acc: 0.7105, F1: 0.7843\n",
      "\n",
      "Early stopping triggered after 32 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6612, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6330, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6429, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6076, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6192, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5932, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6285, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5913, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5958, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5668, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5571, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5496, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.5370, Acc: 0.7143, F1: 0.8140 | Val Loss: 0.5489, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 008 | Train Loss: 0.5098, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5557, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 009 | Train Loss: 0.5470, Acc: 0.6875, F1: 0.7445 | Val Loss: 0.5760, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 010 | Train Loss: 0.5292, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5562, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 011 | Train Loss: 0.5118, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5420, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 012 | Train Loss: 0.5329, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5345, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.4698, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5758, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 014 | Train Loss: 0.5107, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5333, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.4918, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5314, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.5060, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5303, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.4991, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5350, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 018 | Train Loss: 0.5160, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5725, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 019 | Train Loss: 0.5144, Acc: 0.7143, F1: 0.7778 | Val Loss: 0.5477, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 020 | Train Loss: 0.5106, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5328, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.4709, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5547, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 022 | Train Loss: 0.5058, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5361, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.5076, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5376, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4912, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5454, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 025 | Train Loss: 0.5008, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5544, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 026 | Train Loss: 0.5147, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5303, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4979, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5303, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 028 | Train Loss: 0.5123, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5322, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4979, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5486, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 030 | Train Loss: 0.4833, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5480, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 031 | Train Loss: 0.4816, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5343, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.4725, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5345, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 033 | Train Loss: 0.4762, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5347, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.4785, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5354, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.4820, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5289, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 036 | Train Loss: 0.4779, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5289, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 037 | Train Loss: 0.4949, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5847, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 038 | Train Loss: 0.5152, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.5383, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 039 | Train Loss: 0.5228, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5602, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 040 | Train Loss: 0.4883, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5626, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 041 | Train Loss: 0.5007, Acc: 0.8036, F1: 0.8642 | Val Loss: 0.5235, Acc: 0.7368, F1: 0.8077\n",
      "\n",
      "Early stopping triggered after 41 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7021, Acc: 0.3661, F1: 0.1013 | Val Loss: 0.6877, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 002 | Train Loss: 0.6834, Acc: 0.6607, F1: 0.7500 | Val Loss: 0.6694, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6650, Acc: 0.6696, F1: 0.7978 | Val Loss: 0.6531, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6533, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6385, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6450, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6264, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6362, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6178, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6326, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6120, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6281, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6088, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6262, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6058, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6161, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6039, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6198, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6025, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6179, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6016, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6187, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6011, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6205, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6006, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6170, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6001, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6210, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5995, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6213, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5985, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6157, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5978, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6161, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5970, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6087, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5962, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6068, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5956, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6787, Acc: 0.5357, F1: 0.6623 | Val Loss: 0.6174, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6264, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6164, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6396, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6056, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6260, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6031, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6152, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5999, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6088, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5903, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6006, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5830, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5950, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5753, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5893, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5755, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5780, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5605, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5658, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5555, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5617, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5503, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 013 | Train Loss: 0.5457, Acc: 0.7143, F1: 0.8118 | Val Loss: 0.5418, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 014 | Train Loss: 0.5625, Acc: 0.7054, F1: 0.8092 | Val Loss: 0.5369, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 015 | Train Loss: 0.5283, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.5362, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.5228, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5346, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.5133, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5302, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5023, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5358, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.4920, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5337, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.5094, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5384, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.5047, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5401, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.5143, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5395, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.5090, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5910, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 024 | Train Loss: 0.4965, Acc: 0.7500, F1: 0.8000 | Val Loss: 0.5385, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.5240, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5479, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 026 | Train Loss: 0.5476, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5423, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.5109, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5476, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 028 | Train Loss: 0.4777, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5367, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 029 | Train Loss: 0.5024, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5332, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.4817, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5330, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 031 | Train Loss: 0.4880, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5413, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.5159, Acc: 0.7143, F1: 0.7867 | Val Loss: 0.5362, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.4783, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5374, Acc: 0.6842, F1: 0.7692\n",
      "\n",
      "Early stopping triggered after 33 epochs.\n",
      "\n",
      ">>> Running gcn hidden=32, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6872, Acc: 0.5357, F1: 0.6623 | Val Loss: 0.6131, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6314, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6111, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6306, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6004, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6286, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6003, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6003, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5838, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6106, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5670, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5699, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5543, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 008 | Train Loss: 0.5350, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5407, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 009 | Train Loss: 0.5545, Acc: 0.7321, F1: 0.8235 | Val Loss: 0.5841, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 010 | Train Loss: 0.5283, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5700, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 011 | Train Loss: 0.5499, Acc: 0.7857, F1: 0.8554 | Val Loss: 0.5488, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 012 | Train Loss: 0.4982, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5393, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.5235, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5500, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.4931, Acc: 0.7143, F1: 0.7838 | Val Loss: 0.5407, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.5044, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5433, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4950, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5492, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 017 | Train Loss: 0.5114, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5384, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5153, Acc: 0.7232, F1: 0.8000 | Val Loss: 0.5317, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4687, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5301, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4962, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5284, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.5015, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5318, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.5283, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5431, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 023 | Train Loss: 0.5616, Acc: 0.7054, F1: 0.7660 | Val Loss: 0.5403, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 024 | Train Loss: 0.5544, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5371, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.5295, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5619, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 026 | Train Loss: 0.5303, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5261, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4980, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5278, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 028 | Train Loss: 0.4951, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5307, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.5064, Acc: 0.8036, F1: 0.8642 | Val Loss: 0.5307, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.5145, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5430, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7015, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6908, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6869, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6776, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6759, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6667, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6676, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6559, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6582, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6459, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6495, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6366, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6443, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6281, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6361, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6215, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6323, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6161, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6285, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6116, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6258, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6082, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6249, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6056, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6228, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6039, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6222, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6022, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6206, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6010, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6190, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5995, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6171, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5984, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6156, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5967, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6138, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5961, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6115, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5948, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6097, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5925, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6632, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6102, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6292, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6054, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6246, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5959, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6145, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5895, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6082, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5885, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5929, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5713, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5896, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5629, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5711, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5624, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5581, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5451, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5595, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5426, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.5541, Acc: 0.6964, F1: 0.8046 | Val Loss: 0.5589, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 012 | Train Loss: 0.5368, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5338, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 013 | Train Loss: 0.5179, Acc: 0.7679, F1: 0.8471 | Val Loss: 0.5322, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 014 | Train Loss: 0.5119, Acc: 0.7768, F1: 0.8521 | Val Loss: 0.5284, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 015 | Train Loss: 0.5038, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5273, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 016 | Train Loss: 0.4954, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5277, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 017 | Train Loss: 0.4921, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5275, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.4883, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5369, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4831, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5402, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4861, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5402, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.4819, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5418, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.5114, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5403, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.4840, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5589, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 024 | Train Loss: 0.5051, Acc: 0.7232, F1: 0.7862 | Val Loss: 0.5399, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.4812, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5390, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4993, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5470, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 027 | Train Loss: 0.4836, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5398, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 028 | Train Loss: 0.4871, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5367, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4786, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5406, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4888, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5347, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 031 | Train Loss: 0.4803, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5363, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.4808, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5374, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 033 | Train Loss: 0.4837, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5359, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 034 | Train Loss: 0.4797, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5359, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.4973, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5381, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 036 | Train Loss: 0.4797, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5509, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 037 | Train Loss: 0.4892, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5343, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 038 | Train Loss: 0.4754, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5343, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 039 | Train Loss: 0.4727, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5334, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 040 | Train Loss: 0.4771, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5330, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 041 | Train Loss: 0.4728, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5352, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 042 | Train Loss: 0.4866, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5365, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 043 | Train Loss: 0.4714, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5416, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 43 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6260, Acc: 0.6429, F1: 0.7403 | Val Loss: 0.5940, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6158, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6335, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 003 | Train Loss: 0.5841, Acc: 0.7232, F1: 0.8249 | Val Loss: 0.6776, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6154, Acc: 0.6696, F1: 0.7933 | Val Loss: 0.6716, Acc: 0.6579, F1: 0.6977\n",
      "Epoch 005 | Train Loss: 0.6518, Acc: 0.7143, F1: 0.7647 | Val Loss: 0.5678, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 006 | Train Loss: 0.5601, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5681, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5359, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.6032, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 008 | Train Loss: 0.5866, Acc: 0.7411, F1: 0.7883 | Val Loss: 0.5746, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5766, Acc: 0.6875, F1: 0.8045 | Val Loss: 0.5274, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 010 | Train Loss: 0.5059, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5264, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 011 | Train Loss: 0.5092, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5676, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 012 | Train Loss: 0.5509, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5513, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 013 | Train Loss: 0.5003, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5339, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.4834, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5365, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.4898, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5422, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.5097, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5388, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.5874, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5282, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5772, Acc: 0.6518, F1: 0.6977 | Val Loss: 0.5863, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 019 | Train Loss: 0.5549, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5492, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.5150, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5386, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 021 | Train Loss: 0.5119, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5296, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 022 | Train Loss: 0.5033, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5262, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 023 | Train Loss: 0.4915, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5289, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.4842, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5285, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.5242, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5403, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 25 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6853, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6743, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6745, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6645, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6661, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6554, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6577, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6457, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6501, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6364, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6424, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6281, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6365, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6204, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6308, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6146, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6287, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6102, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6251, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6077, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6249, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6058, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6241, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6044, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6233, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6033, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6211, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6022, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6199, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6012, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6176, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5999, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6155, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5989, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6139, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5979, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6120, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5961, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6093, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5940, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6071, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5916, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6695, Acc: 0.6429, F1: 0.7403 | Val Loss: 0.6211, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6332, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6091, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6307, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6047, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6218, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5986, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6135, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5920, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6106, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5848, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6009, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5770, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5928, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5722, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5748, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5603, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5707, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5504, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5503, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5457, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 012 | Train Loss: 0.5380, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.5362, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 013 | Train Loss: 0.5211, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5267, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 014 | Train Loss: 0.5242, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5249, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 015 | Train Loss: 0.5330, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.5268, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 016 | Train Loss: 0.5198, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5598, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 017 | Train Loss: 0.4866, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5496, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 018 | Train Loss: 0.5151, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5345, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.5071, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5697, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 020 | Train Loss: 0.4922, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5412, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 021 | Train Loss: 0.5041, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5330, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.5194, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5580, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 023 | Train Loss: 0.5198, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5373, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 024 | Train Loss: 0.4904, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5362, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4870, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5482, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 026 | Train Loss: 0.4895, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5335, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4852, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5361, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 028 | Train Loss: 0.4879, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5387, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4888, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5444, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4935, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5349, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.4812, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5396, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.5011, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5361, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 32 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7046, Acc: 0.5357, F1: 0.6623 | Val Loss: 0.6306, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6359, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6017, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6202, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6021, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6131, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5779, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6297, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5804, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6321, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5665, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5861, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6308, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.6291, Acc: 0.6964, F1: 0.8111 | Val Loss: 0.5878, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6085, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5628, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5684, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5569, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5751, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5593, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 012 | Train Loss: 0.5562, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5517, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 013 | Train Loss: 0.5457, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5494, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 014 | Train Loss: 0.5360, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5303, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 015 | Train Loss: 0.5122, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5240, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.5406, Acc: 0.7232, F1: 0.8166 | Val Loss: 0.5439, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 017 | Train Loss: 0.5253, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5267, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 018 | Train Loss: 0.4906, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5297, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.5059, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5436, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 020 | Train Loss: 0.5321, Acc: 0.7679, F1: 0.8116 | Val Loss: 0.5745, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.5581, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5333, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.5198, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5331, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.4732, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5620, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 024 | Train Loss: 0.5106, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5418, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4965, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5407, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4986, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5324, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.5027, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5319, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 028 | Train Loss: 0.4835, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5312, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4801, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5333, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.4929, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5346, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 031 | Train Loss: 0.4709, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5440, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4951, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5323, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 033 | Train Loss: 0.4742, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5318, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 034 | Train Loss: 0.4764, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5354, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.4821, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5324, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 036 | Train Loss: 0.5285, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5345, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 36 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6986, Acc: 0.3482, F1: 0.0267 | Val Loss: 0.6889, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 002 | Train Loss: 0.6851, Acc: 0.6964, F1: 0.7927 | Val Loss: 0.6747, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6748, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6612, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6626, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6491, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6505, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6382, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6450, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6279, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6344, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6205, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6299, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6141, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6254, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6094, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6224, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6061, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6224, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6039, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6209, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6024, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6195, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6015, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6178, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6001, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6169, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5995, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6133, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5983, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6125, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5963, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6100, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5941, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6089, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5927, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6064, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5900, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6025, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5879, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6918, Acc: 0.5357, F1: 0.6623 | Val Loss: 0.6145, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6266, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6044, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6185, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5957, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6084, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5879, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5981, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5828, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5770, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5675, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5727, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5533, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5478, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5499, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 009 | Train Loss: 0.5354, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5335, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 010 | Train Loss: 0.5365, Acc: 0.7232, F1: 0.8229 | Val Loss: 0.5310, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.5087, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5423, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 012 | Train Loss: 0.4938, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5394, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 013 | Train Loss: 0.5086, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5423, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.5144, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5426, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.4923, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5400, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.5000, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5470, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.4984, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5373, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.4902, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5361, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4891, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5426, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4896, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5339, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4924, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5328, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 022 | Train Loss: 0.5119, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5398, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4913, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5320, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 024 | Train Loss: 0.4847, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5417, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4958, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5350, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4905, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5340, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 027 | Train Loss: 0.4798, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5515, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 028 | Train Loss: 0.4791, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5375, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6751, Acc: 0.6071, F1: 0.7143 | Val Loss: 0.6039, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6571, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6070, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6240, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5997, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5932, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5799, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6210, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5640, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5815, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5688, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5621, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5507, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5281, Acc: 0.6786, F1: 0.7955 | Val Loss: 0.5802, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 009 | Train Loss: 0.5528, Acc: 0.7054, F1: 0.8024 | Val Loss: 0.5278, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 010 | Train Loss: 0.4856, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5771, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 011 | Train Loss: 0.5116, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5760, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 012 | Train Loss: 0.5058, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5576, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 013 | Train Loss: 0.4848, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5737, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 014 | Train Loss: 0.5710, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5546, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.5009, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5501, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.4899, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5533, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 017 | Train Loss: 0.4887, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5409, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4922, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5455, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4965, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5385, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4882, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5341, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.5007, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5309, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4979, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5416, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 023 | Train Loss: 0.4890, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5389, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4798, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5304, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.4754, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5334, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 026 | Train Loss: 0.4924, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5541, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 027 | Train Loss: 0.5472, Acc: 0.7321, F1: 0.7761 | Val Loss: 0.5338, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 028 | Train Loss: 0.5510, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5466, Acc: 0.7105, F1: 0.8070\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6983, Acc: 0.4018, F1: 0.2299 | Val Loss: 0.6828, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 002 | Train Loss: 0.6784, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.6632, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6632, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6460, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6490, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6318, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6386, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6209, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6319, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6133, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6276, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6089, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6230, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6067, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6208, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6040, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6225, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6022, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6201, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6015, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6191, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6004, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6181, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5994, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6185, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5983, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6163, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5974, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6152, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5967, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6139, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5963, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6136, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5957, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6110, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5939, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6108, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5920, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6084, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5913, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6583, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6153, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6364, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6084, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6288, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6034, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6143, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5932, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6101, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5857, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5873, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5710, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5911, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5607, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5587, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5640, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 009 | Train Loss: 0.5781, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5429, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.5490, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.5649, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.5447, Acc: 0.6875, F1: 0.8045 | Val Loss: 0.5445, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 012 | Train Loss: 0.5242, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5391, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.5088, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5348, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 014 | Train Loss: 0.5161, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5302, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.4973, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5351, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4933, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5367, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.5096, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5391, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.4970, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5499, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4899, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5412, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4922, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5396, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4918, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5594, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 022 | Train Loss: 0.4955, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5310, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 023 | Train Loss: 0.4908, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5304, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 024 | Train Loss: 0.4854, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5409, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4838, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5308, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4779, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5335, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4784, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5356, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.5015, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5379, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4853, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5481, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4965, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5358, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6733, Acc: 0.5179, F1: 0.6494 | Val Loss: 0.6539, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6648, Acc: 0.6607, F1: 0.7816 | Val Loss: 0.6067, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6367, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5927, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6129, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5993, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5932, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5868, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6031, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5657, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5597, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5388, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 008 | Train Loss: 0.5855, Acc: 0.6964, F1: 0.7821 | Val Loss: 0.5708, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.6096, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5592, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 010 | Train Loss: 0.5697, Acc: 0.6964, F1: 0.7463 | Val Loss: 0.5278, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 011 | Train Loss: 0.5301, Acc: 0.7054, F1: 0.8070 | Val Loss: 0.5613, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5318, Acc: 0.7500, F1: 0.8353 | Val Loss: 0.5403, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.5113, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5318, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 014 | Train Loss: 0.5022, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5345, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.5055, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5364, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.4980, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5351, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.4958, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5395, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.5039, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5283, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.4993, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5373, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.5141, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5419, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 021 | Train Loss: 0.5211, Acc: 0.7411, F1: 0.7914 | Val Loss: 0.5304, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.5221, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5400, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4851, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5576, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 024 | Train Loss: 0.4859, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5523, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.5402, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5343, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.5317, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5405, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.5275, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5359, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 27 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7113, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7038, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.6994, Acc: 0.3393, F1: 0.2292 | Val Loss: 0.6885, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6836, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6754, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6739, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6619, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6640, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6494, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6531, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6387, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6437, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6291, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6391, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6213, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6326, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6168, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6304, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6138, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6272, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6116, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6290, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6099, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6274, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6087, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6244, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6078, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6235, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6067, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6251, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6062, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6232, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6051, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6198, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6047, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6204, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6044, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6199, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6035, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6183, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6158, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6003, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 22 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6838, Acc: 0.5179, F1: 0.5909 | Val Loss: 0.6130, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6647, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6116, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6266, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6199, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6305, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6186, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6230, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6014, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6118, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5886, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6041, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5818, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5919, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5766, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5837, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5675, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5646, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5557, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5540, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5467, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5442, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.5372, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 013 | Train Loss: 0.5273, Acc: 0.7500, F1: 0.8372 | Val Loss: 0.5317, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 014 | Train Loss: 0.5116, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5293, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 015 | Train Loss: 0.5087, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5423, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4980, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5438, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4851, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5401, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.4854, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5449, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.4911, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5455, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.5147, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5421, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.5201, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5516, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 022 | Train Loss: 0.5391, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5924, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 023 | Train Loss: 0.5090, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5424, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 024 | Train Loss: 0.5272, Acc: 0.7857, F1: 0.8571 | Val Loss: 0.5484, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 025 | Train Loss: 0.5089, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5789, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 026 | Train Loss: 0.5212, Acc: 0.7411, F1: 0.7914 | Val Loss: 0.5323, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4954, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5521, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 028 | Train Loss: 0.4971, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5305, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.5011, Acc: 0.7143, F1: 0.7838 | Val Loss: 0.5427, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4923, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5307, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 031 | Train Loss: 0.4762, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5317, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 31 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6883, Acc: 0.5893, F1: 0.7356 | Val Loss: 0.6208, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6318, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6007, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6220, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6075, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6670, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6553, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6520, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6172, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6465, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6082, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6252, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6112, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6155, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5834, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6133, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5655, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5624, Acc: 0.6786, F1: 0.8022 | Val Loss: 0.5784, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 011 | Train Loss: 0.5994, Acc: 0.6875, F1: 0.7977 | Val Loss: 0.5309, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 012 | Train Loss: 0.5400, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5254, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 013 | Train Loss: 0.5492, Acc: 0.7143, F1: 0.8095 | Val Loss: 0.5275, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 014 | Train Loss: 0.5254, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5616, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.5216, Acc: 0.7411, F1: 0.7943 | Val Loss: 0.5428, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4968, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5426, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 017 | Train Loss: 0.5017, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5456, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 018 | Train Loss: 0.5330, Acc: 0.7232, F1: 0.7832 | Val Loss: 0.5345, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.5538, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5280, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.5465, Acc: 0.7143, F1: 0.7746 | Val Loss: 0.5877, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 021 | Train Loss: 0.5332, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5254, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.5038, Acc: 0.7946, F1: 0.8623 | Val Loss: 0.5241, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 023 | Train Loss: 0.4810, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5497, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 024 | Train Loss: 0.5012, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.5280, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4921, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5418, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4962, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5348, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.5229, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5327, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.5297, Acc: 0.7411, F1: 0.7943 | Val Loss: 0.5677, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 029 | Train Loss: 0.5356, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5378, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 030 | Train Loss: 0.4983, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5274, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5091, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5326, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.4918, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5268, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 033 | Train Loss: 0.4876, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5298, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 034 | Train Loss: 0.4782, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5363, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 035 | Train Loss: 0.4827, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5773, Acc: 0.7105, F1: 0.7660\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6846, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6721, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6730, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6606, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6621, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6516, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6555, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6423, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6471, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6341, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6399, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6270, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6368, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6213, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6326, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6166, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6299, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6139, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6301, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6113, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6289, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6095, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6252, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6082, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6263, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6070, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6223, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6061, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6234, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6052, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6256, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6042, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6209, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6037, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6197, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6031, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6181, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6020, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6139, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6003, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6172, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5992, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6693, Acc: 0.6071, F1: 0.7143 | Val Loss: 0.6077, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6265, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6028, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6208, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5956, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6172, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5902, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6080, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5847, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5940, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5772, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5901, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5690, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5936, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5609, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5697, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5733, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5705, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5526, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.5563, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5474, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 012 | Train Loss: 0.5539, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5394, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 013 | Train Loss: 0.5361, Acc: 0.7232, F1: 0.8249 | Val Loss: 0.5453, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 014 | Train Loss: 0.5405, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5324, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 015 | Train Loss: 0.5150, Acc: 0.7589, F1: 0.8421 | Val Loss: 0.5280, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 016 | Train Loss: 0.5191, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5265, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 017 | Train Loss: 0.5076, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5312, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5095, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5350, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 019 | Train Loss: 0.4827, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5568, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 020 | Train Loss: 0.5752, Acc: 0.6875, F1: 0.7445 | Val Loss: 0.5403, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.5180, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5740, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 022 | Train Loss: 0.5350, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5565, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 023 | Train Loss: 0.5097, Acc: 0.7232, F1: 0.7832 | Val Loss: 0.5401, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4912, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5444, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 025 | Train Loss: 0.4895, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5368, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.5084, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5388, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.5056, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5390, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 028 | Train Loss: 0.4863, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5452, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.5248, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5506, Acc: 0.6842, F1: 0.7500\n",
      "\n",
      "Early stopping triggered after 29 epochs.\n",
      "\n",
      ">>> Running gcn hidden=64, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7123, Acc: 0.5357, F1: 0.6579 | Val Loss: 0.6133, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6111, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6287, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6938, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6085, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6375, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6322, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6257, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5912, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6046, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5737, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5886, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.5638, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 008 | Train Loss: 0.5685, Acc: 0.7054, F1: 0.8156 | Val Loss: 0.5385, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5804, Acc: 0.6964, F1: 0.7901 | Val Loss: 0.5347, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 010 | Train Loss: 0.5073, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5454, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 011 | Train Loss: 0.5283, Acc: 0.7054, F1: 0.7950 | Val Loss: 0.5529, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 012 | Train Loss: 0.4901, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5617, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 013 | Train Loss: 0.5241, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5875, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 014 | Train Loss: 0.5622, Acc: 0.7321, F1: 0.7761 | Val Loss: 0.5442, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.5177, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5349, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.5308, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5590, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 017 | Train Loss: 0.5192, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5275, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5072, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5297, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.5162, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5266, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4724, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5406, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 021 | Train Loss: 0.4997, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5513, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 022 | Train Loss: 0.5667, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5344, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4670, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5381, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.5065, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5375, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4931, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5352, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4872, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5322, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4980, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5276, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 27 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6835, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6663, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6617, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6475, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6489, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6325, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6373, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6215, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6296, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6138, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6284, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6092, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6265, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6073, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6248, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6060, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6232, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6047, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6207, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6033, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6185, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6026, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6164, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6012, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6144, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5985, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6111, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5968, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6075, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5942, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6037, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5907, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6001, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5871, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5956, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5852, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5919, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5830, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5883, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5791, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5817, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5736, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6531, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6131, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6342, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6084, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6106, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5826, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6110, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5714, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5764, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5681, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5617, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5452, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 007 | Train Loss: 0.5761, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5373, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 008 | Train Loss: 0.5964, Acc: 0.7143, F1: 0.7681 | Val Loss: 0.5576, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 009 | Train Loss: 0.5437, Acc: 0.7232, F1: 0.8144 | Val Loss: 0.5624, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5331, Acc: 0.7232, F1: 0.8249 | Val Loss: 0.5336, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 011 | Train Loss: 0.5597, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5776, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 012 | Train Loss: 0.5199, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5330, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 013 | Train Loss: 0.5196, Acc: 0.7589, F1: 0.8439 | Val Loss: 0.5420, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 014 | Train Loss: 0.5150, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5304, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.5078, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5402, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.5041, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5330, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 017 | Train Loss: 0.4909, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5408, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4892, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5396, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.5011, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5388, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4894, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5614, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 021 | Train Loss: 0.4865, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5379, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 022 | Train Loss: 0.4936, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5321, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.4850, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5304, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4826, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5295, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4822, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5310, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.4839, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5329, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 26 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.8069, Acc: 0.4107, F1: 0.3774 | Val Loss: 0.6886, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 002 | Train Loss: 0.6596, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6218, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6710, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6087, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6515, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6040, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6222, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6055, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6172, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5879, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5977, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5654, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5700, Acc: 0.6964, F1: 0.8111 | Val Loss: 0.5465, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5255, Acc: 0.7589, F1: 0.8402 | Val Loss: 0.5214, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 010 | Train Loss: 0.5192, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.6109, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5598, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5446, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.5118, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5467, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 013 | Train Loss: 0.5012, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5262, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 014 | Train Loss: 0.4982, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5296, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.5186, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5503, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.5494, Acc: 0.7054, F1: 0.7556 | Val Loss: 0.5365, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 017 | Train Loss: 0.5749, Acc: 0.7411, F1: 0.8284 | Val Loss: 0.5287, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4923, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5326, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.5023, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5332, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4997, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5401, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.5226, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5353, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4900, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5391, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.5077, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5319, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.5278, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5345, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.5431, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5330, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.5385, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5338, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4853, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5493, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 028 | Train Loss: 0.4990, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5257, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 029 | Train Loss: 0.4889, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5410, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4897, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5339, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6882, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6685, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6660, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6448, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6480, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6266, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6326, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6143, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6300, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6063, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6236, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6030, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6219, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6008, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6195, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5992, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6168, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5972, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6141, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5955, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6118, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5931, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6090, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5909, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6057, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5891, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6048, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5884, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6001, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5852, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5967, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5805, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5922, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5772, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5874, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5749, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5855, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5737, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5788, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5687, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5749, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5638, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6534, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6000, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6388, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6062, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6108, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5790, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5964, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5679, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5756, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5597, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5541, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5626, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 007 | Train Loss: 0.5546, Acc: 0.7232, F1: 0.8208 | Val Loss: 0.5357, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 008 | Train Loss: 0.5333, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5309, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 009 | Train Loss: 0.5106, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5334, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.4918, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5634, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 011 | Train Loss: 0.4917, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5477, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 012 | Train Loss: 0.4888, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5744, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 013 | Train Loss: 0.4924, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5474, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.4936, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5530, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 015 | Train Loss: 0.5146, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5371, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4770, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5461, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.5165, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5315, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.5120, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5363, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 019 | Train Loss: 0.5089, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5611, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 020 | Train Loss: 0.4745, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5500, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 021 | Train Loss: 0.5241, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5287, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 022 | Train Loss: 0.5049, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5850, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 023 | Train Loss: 0.4898, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5393, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.5378, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5348, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.5290, Acc: 0.7054, F1: 0.7815 | Val Loss: 0.6006, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 026 | Train Loss: 0.5100, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.5274, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.5155, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5455, Acc: 0.6316, F1: 0.7586\n",
      "Epoch 028 | Train Loss: 0.4919, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5322, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4911, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5658, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 030 | Train Loss: 0.4958, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5304, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4848, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5385, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 032 | Train Loss: 0.4792, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5461, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.4797, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5530, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.4855, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5420, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 035 | Train Loss: 0.4713, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5453, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 036 | Train Loss: 0.4908, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5441, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 037 | Train Loss: 0.4767, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5471, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 038 | Train Loss: 0.4669, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5351, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 039 | Train Loss: 0.4672, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5321, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.4669, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5324, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 041 | Train Loss: 0.4690, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5322, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4642, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5371, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 043 | Train Loss: 0.4611, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5356, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 044 | Train Loss: 0.4617, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5393, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 045 | Train Loss: 0.4572, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5465, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 046 | Train Loss: 0.4614, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5423, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 047 | Train Loss: 0.4667, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5433, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 048 | Train Loss: 0.4606, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5486, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 049 | Train Loss: 0.4818, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5415, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 050 | Train Loss: 0.4580, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5373, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 50 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.8555, Acc: 0.3929, F1: 0.3585 | Val Loss: 0.6777, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6601, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6161, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6373, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6078, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6205, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5893, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6113, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5726, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5849, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5527, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5644, Acc: 0.7054, F1: 0.8156 | Val Loss: 0.5408, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 008 | Train Loss: 0.5401, Acc: 0.7232, F1: 0.8268 | Val Loss: 0.5379, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 009 | Train Loss: 0.5418, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5386, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 010 | Train Loss: 0.5022, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.6052, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 011 | Train Loss: 0.5507, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5843, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 012 | Train Loss: 0.5346, Acc: 0.7500, F1: 0.8000 | Val Loss: 0.5369, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 013 | Train Loss: 0.4881, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5562, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.4970, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5432, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 015 | Train Loss: 0.5253, Acc: 0.7946, F1: 0.8606 | Val Loss: 0.5470, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.5035, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5565, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 017 | Train Loss: 0.5350, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5263, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.5079, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5275, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 019 | Train Loss: 0.4810, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5307, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4929, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5357, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.5052, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5306, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 022 | Train Loss: 0.4907, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5391, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4830, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5327, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4795, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5342, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.5417, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5401, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4807, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5281, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.5016, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5304, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4882, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5538, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.5250, Acc: 0.7500, F1: 0.8000 | Val Loss: 0.5259, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 030 | Train Loss: 0.4905, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5198, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4926, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5199, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 032 | Train Loss: 0.4877, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5251, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 033 | Train Loss: 0.4851, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5431, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.4949, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5440, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4651, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5998, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 036 | Train Loss: 0.4815, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5472, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4914, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5423, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 038 | Train Loss: 0.4698, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5586, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 039 | Train Loss: 0.4670, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5313, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.4996, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5361, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 041 | Train Loss: 0.4825, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5405, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4610, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5540, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 043 | Train Loss: 0.4757, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5332, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 044 | Train Loss: 0.4704, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5411, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 045 | Train Loss: 0.4626, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5536, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 046 | Train Loss: 0.4731, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5501, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 047 | Train Loss: 0.4740, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5466, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 47 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6720, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6489, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6488, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6285, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6342, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6159, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6323, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6099, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6263, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6084, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6255, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6072, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6232, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6066, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6216, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6055, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6198, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6035, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6178, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6015, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6173, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6003, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6128, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5974, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6111, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5948, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6082, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5921, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6065, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5898, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6028, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5875, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5987, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5871, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5963, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5843, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5913, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5802, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5884, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5761, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5856, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5729, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6500, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6566, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6633, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6028, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6294, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6318, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6287, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5901, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6041, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5800, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5972, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5695, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5813, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5679, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5667, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5516, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5535, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5447, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5357, Acc: 0.7321, F1: 0.8315 | Val Loss: 0.5377, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.5220, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5327, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 012 | Train Loss: 0.5071, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5368, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 013 | Train Loss: 0.5026, Acc: 0.8036, F1: 0.8659 | Val Loss: 0.5589, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 014 | Train Loss: 0.5122, Acc: 0.7143, F1: 0.7838 | Val Loss: 0.5340, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.4870, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5410, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.5057, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5522, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 017 | Train Loss: 0.5038, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5410, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4809, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5427, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4983, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5384, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4755, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5454, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4961, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5484, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 022 | Train Loss: 0.4893, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5443, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4853, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5352, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.4945, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5339, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.4959, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5484, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 026 | Train Loss: 0.4807, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5495, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 027 | Train Loss: 0.4870, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5413, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4860, Acc: 0.7232, F1: 0.7862 | Val Loss: 0.5384, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4954, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5348, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 29 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7369, Acc: 0.5893, F1: 0.7013 | Val Loss: 0.6729, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6600, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6189, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6473, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6155, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6317, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6041, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6191, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5907, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5957, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5468, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5872, Acc: 0.7054, F1: 0.8024 | Val Loss: 0.5688, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5497, Acc: 0.6696, F1: 0.7861 | Val Loss: 0.5902, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 009 | Train Loss: 0.5633, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5237, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 010 | Train Loss: 0.5482, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5255, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 011 | Train Loss: 0.5432, Acc: 0.7232, F1: 0.8208 | Val Loss: 0.5224, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 012 | Train Loss: 0.5296, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5438, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 013 | Train Loss: 0.5140, Acc: 0.7054, F1: 0.7843 | Val Loss: 0.5232, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 014 | Train Loss: 0.5175, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5315, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4827, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5508, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.4975, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5428, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4928, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5432, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 018 | Train Loss: 0.4909, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.6650, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 019 | Train Loss: 0.5368, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5511, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 020 | Train Loss: 0.5161, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5254, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 021 | Train Loss: 0.4803, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5239, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 022 | Train Loss: 0.4795, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5298, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 023 | Train Loss: 0.4892, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5284, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.4724, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5515, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4877, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5582, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4886, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5645, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4761, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5439, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4846, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5356, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4816, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5732, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 030 | Train Loss: 0.5178, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5263, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 031 | Train Loss: 0.5115, Acc: 0.7857, F1: 0.8571 | Val Loss: 0.5271, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 032 | Train Loss: 0.4787, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5410, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 033 | Train Loss: 0.4586, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5463, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 034 | Train Loss: 0.4855, Acc: 0.7946, F1: 0.8623 | Val Loss: 0.6536, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 035 | Train Loss: 0.5435, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5467, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 036 | Train Loss: 0.5098, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5598, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 037 | Train Loss: 0.4902, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5350, Acc: 0.6842, F1: 0.7857\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6866, Acc: 0.5982, F1: 0.7458 | Val Loss: 0.6715, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6710, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6522, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6551, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6357, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6397, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6228, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6322, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6132, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6269, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6080, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6256, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6053, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6260, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6035, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6234, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6016, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6001, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6164, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5981, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6143, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5973, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6136, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5966, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6099, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5934, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6074, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5899, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6021, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5879, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5995, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5851, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5960, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5802, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5895, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5763, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5886, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5738, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5808, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5690, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6687, Acc: 0.5000, F1: 0.6364 | Val Loss: 0.6006, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5917, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6021, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6033, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6074, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5783, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5822, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5621, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5701, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5552, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5515, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5502, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 008 | Train Loss: 0.5596, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5341, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5224, Acc: 0.7321, F1: 0.8295 | Val Loss: 0.5281, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.5154, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5323, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 011 | Train Loss: 0.5048, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5332, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 012 | Train Loss: 0.4900, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5545, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 013 | Train Loss: 0.4820, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5564, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 014 | Train Loss: 0.5277, Acc: 0.7232, F1: 0.8025 | Val Loss: 0.5440, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4950, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5345, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.4978, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5405, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4970, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5327, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.4857, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5331, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.4851, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5347, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.5024, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5340, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.5231, Acc: 0.7321, F1: 0.7826 | Val Loss: 0.5587, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 022 | Train Loss: 0.5033, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5666, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 023 | Train Loss: 0.5019, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5359, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.5010, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5244, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4815, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5462, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.5052, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5280, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4811, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5330, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4889, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5325, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7544, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6709, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6534, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6237, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6449, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6185, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6361, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6147, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6283, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5982, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5885, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6021, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5630, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5670, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5640, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 009 | Train Loss: 0.5533, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5327, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 010 | Train Loss: 0.5362, Acc: 0.7232, F1: 0.8098 | Val Loss: 0.6179, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 011 | Train Loss: 0.5442, Acc: 0.6607, F1: 0.7467 | Val Loss: 0.5419, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 012 | Train Loss: 0.5559, Acc: 0.7143, F1: 0.7922 | Val Loss: 0.5468, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 013 | Train Loss: 0.5208, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5255, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 014 | Train Loss: 0.5096, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5430, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.5009, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5308, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.5617, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5387, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.5758, Acc: 0.7232, F1: 0.7737 | Val Loss: 0.5399, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4764, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5529, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 019 | Train Loss: 0.5373, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5298, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.5405, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5263, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4955, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5311, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.4987, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5328, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.5499, Acc: 0.7054, F1: 0.7591 | Val Loss: 0.5479, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 024 | Train Loss: 0.4930, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5432, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 025 | Train Loss: 0.5003, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5468, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.5328, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5460, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4865, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5476, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 028 | Train Loss: 0.5128, Acc: 0.7232, F1: 0.8025 | Val Loss: 0.5439, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4750, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5311, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 030 | Train Loss: 0.5060, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5540, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 031 | Train Loss: 0.4787, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5305, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 032 | Train Loss: 0.5108, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5347, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 033 | Train Loss: 0.4967, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5322, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 034 | Train Loss: 0.5145, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5316, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 035 | Train Loss: 0.4759, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5339, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 036 | Train Loss: 0.4660, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5374, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 037 | Train Loss: 0.4726, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5443, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 038 | Train Loss: 0.4646, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5423, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 039 | Train Loss: 0.4775, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5451, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 040 | Train Loss: 0.4764, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5390, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 041 | Train Loss: 0.4642, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5368, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4712, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5399, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 043 | Train Loss: 0.4755, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5401, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 044 | Train Loss: 0.4625, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5383, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 045 | Train Loss: 0.4797, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5418, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 046 | Train Loss: 0.5013, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5436, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 047 | Train Loss: 0.4678, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5472, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 048 | Train Loss: 0.4840, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5782, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 049 | Train Loss: 0.5436, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.5348, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 050 | Train Loss: 0.4789, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5508, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 051 | Train Loss: 0.4899, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5997, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 052 | Train Loss: 0.4882, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5348, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 52 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6633, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6484, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6512, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6325, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6378, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6218, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6322, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6143, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6280, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6109, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6269, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6089, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6281, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6070, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6227, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6059, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6210, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6054, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6188, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6044, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6194, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6032, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6155, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5999, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6138, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5967, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6100, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5942, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6073, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5910, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6050, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5883, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5989, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5870, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5925, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5815, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5895, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5756, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5885, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5711, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5805, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5672, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6594, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6117, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6270, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6122, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6337, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5849, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5970, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5882, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5958, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5698, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5724, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5562, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5579, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5435, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5448, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5320, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 009 | Train Loss: 0.5256, Acc: 0.7946, F1: 0.8606 | Val Loss: 0.5269, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 010 | Train Loss: 0.5018, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5303, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 011 | Train Loss: 0.5063, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5376, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 012 | Train Loss: 0.5387, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5532, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 013 | Train Loss: 0.5088, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5424, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.5026, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5502, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 015 | Train Loss: 0.5073, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5461, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.4908, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5405, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.4847, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5364, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.4826, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5346, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4910, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5353, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4856, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5352, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.4835, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5337, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4822, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5324, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.5164, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5481, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 024 | Train Loss: 0.5127, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5398, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4742, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5379, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 026 | Train Loss: 0.4895, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5291, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4801, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5365, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 27 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.8288, Acc: 0.5179, F1: 0.6087 | Val Loss: 0.6786, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6721, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6316, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6489, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6120, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6450, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6064, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6328, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6135, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6235, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5957, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6089, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5826, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5962, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5649, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5596, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5622, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6180, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5264, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 011 | Train Loss: 0.5677, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5426, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 012 | Train Loss: 0.5319, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5267, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 013 | Train Loss: 0.5329, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5419, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.5118, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5230, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 015 | Train Loss: 0.5065, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5359, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4933, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.6425, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 017 | Train Loss: 0.5357, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5421, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 018 | Train Loss: 0.4946, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5684, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 019 | Train Loss: 0.4793, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5280, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.4989, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5314, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.4936, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5321, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 022 | Train Loss: 0.5064, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5263, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 023 | Train Loss: 0.4967, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5334, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 024 | Train Loss: 0.4880, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5260, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4812, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5389, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4739, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5581, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 027 | Train Loss: 0.4947, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5543, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 028 | Train Loss: 0.4711, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5441, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6970, Acc: 0.4643, F1: 0.3878 | Val Loss: 0.6784, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6672, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6512, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6517, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6298, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6390, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6165, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6293, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6110, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6276, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6083, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6292, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6068, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6266, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6055, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6249, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6042, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6204, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6030, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6214, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6022, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6200, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6010, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6142, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5993, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6147, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5974, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6130, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5953, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6138, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5925, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6133, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5916, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6061, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5891, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6041, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5879, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5976, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5855, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5988, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5832, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6944, Acc: 0.6250, F1: 0.7667 | Val Loss: 0.6134, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6348, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6307, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6362, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6088, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6168, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5923, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6092, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5827, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5920, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5755, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5893, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5631, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5636, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5651, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5764, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5539, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 010 | Train Loss: 0.5500, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.5389, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5405, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5365, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5259, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.5352, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 013 | Train Loss: 0.5123, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5309, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 014 | Train Loss: 0.5161, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5299, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.5065, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5324, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.4995, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5372, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.5230, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5384, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5275, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5403, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4929, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5444, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4881, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5450, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 021 | Train Loss: 0.5084, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5355, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.4876, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5404, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4945, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5369, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.5013, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5391, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.4828, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5527, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 026 | Train Loss: 0.5045, Acc: 0.7143, F1: 0.7838 | Val Loss: 0.5339, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4797, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5361, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 028 | Train Loss: 0.4757, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5385, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.5537, Acc: 0.7321, F1: 0.7887 | Val Loss: 0.5330, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4634, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5533, Acc: 0.6842, F1: 0.7778\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running gcn hidden=128, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.8967, Acc: 0.5893, F1: 0.7294 | Val Loss: 0.7010, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.6732, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6302, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6441, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6215, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6489, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6242, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6414, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6176, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6342, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6040, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6182, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5843, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5976, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5557, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5416, Acc: 0.7232, F1: 0.8208 | Val Loss: 0.5317, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 010 | Train Loss: 0.4939, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.6587, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 011 | Train Loss: 0.5575, Acc: 0.7232, F1: 0.7862 | Val Loss: 0.6130, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 012 | Train Loss: 0.5535, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.6060, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 013 | Train Loss: 0.5749, Acc: 0.7500, F1: 0.7910 | Val Loss: 0.5408, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 014 | Train Loss: 0.5595, Acc: 0.7411, F1: 0.8343 | Val Loss: 0.5281, Acc: 0.6316, F1: 0.7586\n",
      "Epoch 015 | Train Loss: 0.5500, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5410, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.5028, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5257, Acc: 0.6316, F1: 0.7586\n",
      "Epoch 017 | Train Loss: 0.5104, Acc: 0.7232, F1: 0.8166 | Val Loss: 0.5277, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 018 | Train Loss: 0.5151, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5329, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 019 | Train Loss: 0.5186, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5295, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 020 | Train Loss: 0.4941, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5360, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4914, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5411, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4760, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5631, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 023 | Train Loss: 0.5009, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5483, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4945, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5764, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 025 | Train Loss: 0.4948, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.5401, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.5365, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5424, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.5104, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5394, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.5029, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5410, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 029 | Train Loss: 0.5200, Acc: 0.7589, F1: 0.8402 | Val Loss: 0.5331, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.5233, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5440, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5284, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5294, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 032 | Train Loss: 0.4834, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5531, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.4909, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5398, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 034 | Train Loss: 0.4998, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5397, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.5032, Acc: 0.7232, F1: 0.7862 | Val Loss: 0.5591, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 036 | Train Loss: 0.4765, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5331, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 037 | Train Loss: 0.5001, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5374, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 038 | Train Loss: 0.5028, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5440, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 039 | Train Loss: 0.4931, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5299, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.4736, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5325, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 041 | Train Loss: 0.4746, Acc: 0.8036, F1: 0.8608 | Val Loss: 0.5326, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4703, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5402, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4872, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5488, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 044 | Train Loss: 0.4984, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5439, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 045 | Train Loss: 0.4796, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5603, Acc: 0.7105, F1: 0.7843\n",
      "\n",
      "Early stopping triggered after 45 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7001, Acc: 0.5000, F1: 0.4909 | Val Loss: 0.6693, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6613, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6342, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6371, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6141, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6304, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6081, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6267, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6057, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6244, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6035, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6182, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6006, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6179, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6008, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6148, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5976, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6109, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5940, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6078, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5900, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6042, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5864, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6011, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5856, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5955, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5815, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5904, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5766, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5852, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5705, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5791, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5666, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5745, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5623, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 020 | Train Loss: 0.5653, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5560, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 021 | Train Loss: 0.5588, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5509, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 022 | Train Loss: 0.5519, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5487, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 023 | Train Loss: 0.5457, Acc: 0.7679, F1: 0.8471 | Val Loss: 0.5425, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 024 | Train Loss: 0.5374, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.5374, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 025 | Train Loss: 0.5326, Acc: 0.7589, F1: 0.8421 | Val Loss: 0.5352, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 026 | Train Loss: 0.5315, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5348, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 027 | Train Loss: 0.5163, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5300, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 028 | Train Loss: 0.5196, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5314, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 029 | Train Loss: 0.5192, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5269, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 030 | Train Loss: 0.5111, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5276, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5050, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5277, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.5013, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5277, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.5019, Acc: 0.7232, F1: 0.8025 | Val Loss: 0.5308, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.4968, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5266, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.4945, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5269, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 036 | Train Loss: 0.4997, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5281, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 037 | Train Loss: 0.4953, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5285, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 038 | Train Loss: 0.4924, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5293, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 039 | Train Loss: 0.4899, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5345, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 040 | Train Loss: 0.4911, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5320, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 041 | Train Loss: 0.4950, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5290, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 042 | Train Loss: 0.4905, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5303, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 42 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6650, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6053, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5902, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.5975, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5633, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5563, Acc: 0.6786, F1: 0.8022 | Val Loss: 0.6036, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 005 | Train Loss: 0.5503, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5281, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 006 | Train Loss: 0.5423, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5340, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 007 | Train Loss: 0.5040, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5666, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 008 | Train Loss: 0.5217, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5497, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 009 | Train Loss: 0.5106, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5506, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 010 | Train Loss: 0.5134, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5354, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 011 | Train Loss: 0.4946, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5382, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 012 | Train Loss: 0.5229, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5301, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.5084, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5318, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 014 | Train Loss: 0.4874, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5419, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4953, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5336, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4951, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5383, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.4783, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5373, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4786, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5355, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4910, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5362, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4924, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5331, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4670, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5458, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.4864, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5306, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4810, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5264, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4782, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5312, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4755, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5367, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 026 | Train Loss: 0.4739, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5335, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4619, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5481, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4806, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5497, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 029 | Train Loss: 0.4819, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5321, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4897, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5504, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 031 | Train Loss: 0.5516, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5311, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 032 | Train Loss: 0.4684, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5377, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 033 | Train Loss: 0.4585, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5429, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 034 | Train Loss: 0.4718, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5397, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 035 | Train Loss: 0.4706, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5387, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 036 | Train Loss: 0.4852, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5406, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4527, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5467, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 038 | Train Loss: 0.4698, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5346, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 039 | Train Loss: 0.4645, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5351, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.4504, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5472, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 041 | Train Loss: 0.4804, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5420, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4581, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5460, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4579, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5448, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 044 | Train Loss: 0.4550, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5411, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 045 | Train Loss: 0.4524, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5414, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 45 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.0983, Acc: 0.5179, F1: 0.6087 | Val Loss: 0.6304, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6712, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6552, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6538, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6166, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6343, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6015, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6116, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5665, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5885, Acc: 0.6696, F1: 0.7811 | Val Loss: 0.5758, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6013, Acc: 0.6696, F1: 0.7886 | Val Loss: 0.5375, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 008 | Train Loss: 0.5208, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5258, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 009 | Train Loss: 0.5122, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5383, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 010 | Train Loss: 0.4940, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5437, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 011 | Train Loss: 0.5121, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.6942, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 012 | Train Loss: 0.6520, Acc: 0.7232, F1: 0.8229 | Val Loss: 0.6315, Acc: 0.6579, F1: 0.7111\n",
      "Epoch 013 | Train Loss: 0.5317, Acc: 0.6875, F1: 0.7518 | Val Loss: 0.5325, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 014 | Train Loss: 0.5128, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5287, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 015 | Train Loss: 0.4800, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.6224, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 016 | Train Loss: 0.5571, Acc: 0.7321, F1: 0.7887 | Val Loss: 0.5569, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 017 | Train Loss: 0.4844, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5760, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 018 | Train Loss: 0.4807, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5351, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 019 | Train Loss: 0.4823, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5517, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 020 | Train Loss: 0.4757, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5435, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.4677, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5391, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 022 | Train Loss: 0.4695, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5419, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4664, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5542, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 024 | Train Loss: 0.4695, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5388, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4811, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5382, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 026 | Train Loss: 0.4777, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5375, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 027 | Train Loss: 0.4738, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5414, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 028 | Train Loss: 0.5391, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5488, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 029 | Train Loss: 0.5069, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.5988, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 030 | Train Loss: 0.5822, Acc: 0.7500, F1: 0.8409 | Val Loss: 0.5331, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.6044, Acc: 0.6607, F1: 0.7121 | Val Loss: 0.5402, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 032 | Train Loss: 0.5161, Acc: 0.7589, F1: 0.8421 | Val Loss: 0.5308, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 033 | Train Loss: 0.4746, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5788, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 034 | Train Loss: 0.4824, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5760, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 34 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6845, Acc: 0.5357, F1: 0.6623 | Val Loss: 0.6512, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6466, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6238, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6338, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6088, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6246, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6045, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6266, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6026, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6226, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6004, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6186, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5994, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6156, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5975, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6124, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5975, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6113, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5961, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6077, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5909, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6031, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5875, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5979, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5823, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5935, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5782, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5898, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5741, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5866, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5701, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5792, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5664, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5710, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5657, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 019 | Train Loss: 0.5686, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5630, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 020 | Train Loss: 0.5646, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5616, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 021 | Train Loss: 0.5589, Acc: 0.7232, F1: 0.8268 | Val Loss: 0.5544, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 022 | Train Loss: 0.5502, Acc: 0.7321, F1: 0.8315 | Val Loss: 0.5483, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 023 | Train Loss: 0.5440, Acc: 0.7500, F1: 0.8391 | Val Loss: 0.5422, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 024 | Train Loss: 0.5356, Acc: 0.7768, F1: 0.8555 | Val Loss: 0.5382, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 025 | Train Loss: 0.5329, Acc: 0.7500, F1: 0.8409 | Val Loss: 0.5346, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 026 | Train Loss: 0.5296, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5367, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 027 | Train Loss: 0.5213, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5319, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 028 | Train Loss: 0.5128, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5285, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 029 | Train Loss: 0.5118, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5275, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 030 | Train Loss: 0.5044, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5315, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5088, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5333, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.5012, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5275, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.4981, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5279, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.5049, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5307, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.5008, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5297, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 036 | Train Loss: 0.4996, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5284, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 037 | Train Loss: 0.4922, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5385, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 038 | Train Loss: 0.4962, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5363, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 039 | Train Loss: 0.4905, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5394, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 040 | Train Loss: 0.4883, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5308, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 40 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6536, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6177, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6822, Acc: 0.6786, F1: 0.7931 | Val Loss: 0.6631, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6468, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6033, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6123, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5993, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6126, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5801, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5926, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5971, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5962, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5625, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5717, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5537, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5646, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5505, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5459, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5369, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5107, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5596, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 012 | Train Loss: 0.5088, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5404, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 013 | Train Loss: 0.5075, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5492, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.5036, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5531, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.5712, Acc: 0.7500, F1: 0.7941 | Val Loss: 0.5522, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 016 | Train Loss: 0.6086, Acc: 0.7500, F1: 0.8353 | Val Loss: 0.5484, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 017 | Train Loss: 0.5564, Acc: 0.7143, F1: 0.7746 | Val Loss: 0.6602, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 018 | Train Loss: 0.5559, Acc: 0.7143, F1: 0.7612 | Val Loss: 0.5329, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 019 | Train Loss: 0.5465, Acc: 0.7500, F1: 0.8391 | Val Loss: 0.5428, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 020 | Train Loss: 0.5206, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5386, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.5201, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5319, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4951, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5260, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 023 | Train Loss: 0.5001, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5320, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 024 | Train Loss: 0.4915, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5422, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4888, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5370, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4835, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5419, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4831, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5441, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4832, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5433, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.5025, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5516, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4791, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5393, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.0752, Acc: 0.5893, F1: 0.7013 | Val Loss: 0.6293, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6530, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6235, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6812, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6213, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6326, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6376, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6611, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6292, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6290, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6109, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6408, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6017, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6292, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5980, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6186, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5851, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6069, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5743, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5796, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5432, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5567, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5700, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 013 | Train Loss: 0.5738, Acc: 0.7054, F1: 0.7660 | Val Loss: 0.5507, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 014 | Train Loss: 0.5297, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5538, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 015 | Train Loss: 0.5156, Acc: 0.8125, F1: 0.8679 | Val Loss: 0.5280, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.4912, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5304, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.4824, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5502, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5145, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5483, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 019 | Train Loss: 0.4679, Acc: 0.7232, F1: 0.7862 | Val Loss: 0.5500, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 020 | Train Loss: 0.4955, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5743, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 021 | Train Loss: 0.4899, Acc: 0.7232, F1: 0.7832 | Val Loss: 0.5396, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.5045, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5269, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4849, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5221, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4862, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5204, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4803, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5227, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 026 | Train Loss: 0.4756, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5279, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4700, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5439, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4904, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5438, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 029 | Train Loss: 0.4754, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5243, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 030 | Train Loss: 0.4749, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5197, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.4772, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5377, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 31 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6874, Acc: 0.5893, F1: 0.7013 | Val Loss: 0.6588, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6608, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6309, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6346, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6154, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6313, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6066, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6226, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6039, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6210, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6014, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6191, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5992, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6165, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5973, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6135, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5957, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6095, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5955, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6085, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5952, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6070, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5914, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5991, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5834, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5948, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5796, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5924, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5750, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5869, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5706, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5774, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5670, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5728, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5622, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 019 | Train Loss: 0.5627, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5542, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 020 | Train Loss: 0.5573, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5490, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 021 | Train Loss: 0.5518, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5449, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 022 | Train Loss: 0.5431, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.5414, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 023 | Train Loss: 0.5337, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5353, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 024 | Train Loss: 0.5331, Acc: 0.7411, F1: 0.8343 | Val Loss: 0.5335, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 025 | Train Loss: 0.5264, Acc: 0.7589, F1: 0.8402 | Val Loss: 0.5296, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 026 | Train Loss: 0.5150, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5375, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.5202, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5333, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.5108, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5254, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 029 | Train Loss: 0.5111, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5234, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 030 | Train Loss: 0.5010, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5307, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5004, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5274, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.4963, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5241, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 033 | Train Loss: 0.5002, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5248, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 034 | Train Loss: 0.4946, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5287, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.5033, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5547, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 036 | Train Loss: 0.5041, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5308, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 037 | Train Loss: 0.4883, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5277, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 038 | Train Loss: 0.4980, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5261, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 039 | Train Loss: 0.4890, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5308, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 040 | Train Loss: 0.4907, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5281, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 041 | Train Loss: 0.4859, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5266, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 41 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7292, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6099, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6432, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6549, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6551, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6324, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6322, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5968, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6175, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5843, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5995, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5919, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5896, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5667, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5865, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5639, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5739, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5628, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 010 | Train Loss: 0.5550, Acc: 0.7589, F1: 0.8439 | Val Loss: 0.5354, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 011 | Train Loss: 0.5255, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5280, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 012 | Train Loss: 0.5023, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5344, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.5053, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5394, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.5093, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5469, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.5148, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5494, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 016 | Train Loss: 0.5010, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5477, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 017 | Train Loss: 0.4964, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5323, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4906, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5321, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4921, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5316, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 020 | Train Loss: 0.4849, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5434, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4821, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5333, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 022 | Train Loss: 0.5073, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5351, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.5062, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5693, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 024 | Train Loss: 0.5156, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5502, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 025 | Train Loss: 0.5097, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5294, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4841, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5293, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4785, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5278, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 028 | Train Loss: 0.4794, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5401, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4970, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5349, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4754, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5338, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 031 | Train Loss: 0.4734, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5336, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 032 | Train Loss: 0.4786, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5354, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 033 | Train Loss: 0.4856, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5363, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 034 | Train Loss: 0.4797, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5342, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 035 | Train Loss: 0.4668, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5348, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 036 | Train Loss: 0.4865, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5363, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 037 | Train Loss: 0.4688, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5381, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 038 | Train Loss: 0.4892, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5370, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 039 | Train Loss: 0.4723, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5472, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 040 | Train Loss: 0.4855, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5369, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 041 | Train Loss: 0.4695, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5414, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 042 | Train Loss: 0.4918, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5347, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4742, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5362, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 044 | Train Loss: 0.4632, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5435, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 045 | Train Loss: 0.4776, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5384, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 046 | Train Loss: 0.4740, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5390, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 047 | Train Loss: 0.4633, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5316, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 47 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.4301, Acc: 0.4911, F1: 0.5581 | Val Loss: 0.6521, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6482, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6183, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6580, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6063, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6301, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5957, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6107, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5778, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5871, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5517, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5622, Acc: 0.6607, F1: 0.7912 | Val Loss: 0.6239, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 008 | Train Loss: 0.5844, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5455, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5375, Acc: 0.7232, F1: 0.8050 | Val Loss: 0.5438, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 010 | Train Loss: 0.5245, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5299, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 011 | Train Loss: 0.4999, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5425, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.5048, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5478, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.4961, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5509, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.5074, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.6355, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 015 | Train Loss: 0.5593, Acc: 0.7143, F1: 0.7681 | Val Loss: 0.5702, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 016 | Train Loss: 0.5428, Acc: 0.7768, F1: 0.8521 | Val Loss: 0.5255, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 017 | Train Loss: 0.4965, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5273, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.5021, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5377, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.5191, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5392, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.5203, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5228, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 021 | Train Loss: 0.4896, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5375, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.5046, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5419, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.5264, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5861, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 024 | Train Loss: 0.5914, Acc: 0.7500, F1: 0.8391 | Val Loss: 0.5695, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 025 | Train Loss: 0.5864, Acc: 0.6786, F1: 0.7273 | Val Loss: 0.5368, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.5301, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5232, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 027 | Train Loss: 0.5120, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5342, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4967, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5295, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6780, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6501, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6480, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6287, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6358, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6131, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6257, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6072, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6255, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6051, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6254, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6032, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6233, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6012, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6178, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5997, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6156, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5982, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6129, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5983, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6099, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5938, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6057, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5895, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6016, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5841, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5970, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5801, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5907, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5755, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5855, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5710, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5813, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5666, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5748, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5622, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5700, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5578, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5611, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5534, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5562, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5488, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 022 | Train Loss: 0.5471, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5467, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 023 | Train Loss: 0.5406, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.5449, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 024 | Train Loss: 0.5349, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5389, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 025 | Train Loss: 0.5286, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5374, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 026 | Train Loss: 0.5194, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5313, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 027 | Train Loss: 0.5178, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5303, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 028 | Train Loss: 0.5109, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5319, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.5054, Acc: 0.7232, F1: 0.8025 | Val Loss: 0.5341, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.5020, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5300, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 031 | Train Loss: 0.5015, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5319, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 032 | Train Loss: 0.4988, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5383, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.4938, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5389, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.4944, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5357, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.4929, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5314, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 036 | Train Loss: 0.4926, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5316, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 037 | Train Loss: 0.4872, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5317, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 038 | Train Loss: 0.4856, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5351, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 039 | Train Loss: 0.4873, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5360, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 040 | Train Loss: 0.4851, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5286, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 041 | Train Loss: 0.4874, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5294, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 41 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7126, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6138, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6450, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6443, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6406, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6078, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6097, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5924, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6107, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5813, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5941, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5774, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5813, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5617, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5792, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5533, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5612, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5651, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 010 | Train Loss: 0.5696, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5480, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.5420, Acc: 0.7232, F1: 0.8229 | Val Loss: 0.5484, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 012 | Train Loss: 0.5171, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5367, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 013 | Train Loss: 0.5211, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5278, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.4938, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5318, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.4872, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5398, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4996, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5494, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.5033, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5438, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.4832, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5373, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4839, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5336, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.5018, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5303, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.5033, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5313, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 022 | Train Loss: 0.5226, Acc: 0.7857, F1: 0.8554 | Val Loss: 0.5301, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4949, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5366, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4893, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5359, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 025 | Train Loss: 0.4793, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5479, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 026 | Train Loss: 0.4930, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5303, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4872, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5311, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4784, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5349, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4772, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5371, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4800, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5352, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.9774, Acc: 0.6786, F1: 0.8022 | Val Loss: 0.6660, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6469, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6265, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6607, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6164, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6115, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5784, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5735, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.7139, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 006 | Train Loss: 0.6967, Acc: 0.5089, F1: 0.5133 | Val Loss: 0.6360, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6422, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5789, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6290, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5497, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5676, Acc: 0.7143, F1: 0.7895 | Val Loss: 0.5394, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5219, Acc: 0.7411, F1: 0.8362 | Val Loss: 0.5510, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5226, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5440, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 012 | Train Loss: 0.5028, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.6078, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 013 | Train Loss: 0.5319, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.6170, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 014 | Train Loss: 0.5399, Acc: 0.7411, F1: 0.8284 | Val Loss: 0.6368, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 015 | Train Loss: 0.5642, Acc: 0.6875, F1: 0.7482 | Val Loss: 0.5284, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.4977, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5303, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.4949, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5594, Acc: 0.6316, F1: 0.7586\n",
      "Epoch 018 | Train Loss: 0.5511, Acc: 0.7768, F1: 0.8521 | Val Loss: 0.7381, Acc: 0.6579, F1: 0.6977\n",
      "Epoch 019 | Train Loss: 0.6823, Acc: 0.6250, F1: 0.6316 | Val Loss: 0.5316, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 020 | Train Loss: 0.5848, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5644, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 021 | Train Loss: 0.5475, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5460, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 022 | Train Loss: 0.5195, Acc: 0.7500, F1: 0.8353 | Val Loss: 0.5276, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 023 | Train Loss: 0.5037, Acc: 0.7946, F1: 0.8623 | Val Loss: 0.5592, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 024 | Train Loss: 0.5093, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5531, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.5093, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5436, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 026 | Train Loss: 0.5074, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5240, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 027 | Train Loss: 0.5042, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5557, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.5010, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5284, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 029 | Train Loss: 0.4886, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5371, Acc: 0.7105, F1: 0.7843\n",
      "\n",
      "Early stopping triggered after 29 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6730, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6434, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6487, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6192, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6316, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6093, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6262, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6055, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6234, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6029, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6219, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6004, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6190, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5987, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6147, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5964, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6122, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5951, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6084, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5923, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6048, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5873, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5990, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5824, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5933, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5774, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5900, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5754, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5822, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5706, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5744, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5635, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5739, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5593, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5631, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5541, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 019 | Train Loss: 0.5578, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5509, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 020 | Train Loss: 0.5474, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5448, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 021 | Train Loss: 0.5390, Acc: 0.7321, F1: 0.8315 | Val Loss: 0.5409, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 022 | Train Loss: 0.5345, Acc: 0.7054, F1: 0.8114 | Val Loss: 0.5378, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 023 | Train Loss: 0.5244, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5366, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 024 | Train Loss: 0.5206, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5355, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 025 | Train Loss: 0.5163, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5313, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 026 | Train Loss: 0.5142, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5303, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 027 | Train Loss: 0.5117, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5318, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 028 | Train Loss: 0.5070, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5296, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 029 | Train Loss: 0.4995, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5351, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.5003, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5338, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.4972, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5309, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 032 | Train Loss: 0.4966, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5306, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 033 | Train Loss: 0.5001, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5307, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.4926, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5432, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 035 | Train Loss: 0.4950, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5324, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 036 | Train Loss: 0.4947, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5300, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 037 | Train Loss: 0.4885, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5351, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 038 | Train Loss: 0.4944, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5365, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 039 | Train Loss: 0.4875, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5317, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 39 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6563, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6053, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6248, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5962, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6015, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5732, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5719, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5598, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 005 | Train Loss: 0.5768, Acc: 0.6518, F1: 0.7869 | Val Loss: 0.5427, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 006 | Train Loss: 0.5236, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5288, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 007 | Train Loss: 0.5091, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5803, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 008 | Train Loss: 0.5256, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.5368, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 009 | Train Loss: 0.5050, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5468, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 010 | Train Loss: 0.5041, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5417, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 011 | Train Loss: 0.4956, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5454, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 012 | Train Loss: 0.4904, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5430, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.4974, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5432, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.4794, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5372, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 015 | Train Loss: 0.5031, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5376, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4850, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5342, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 017 | Train Loss: 0.4908, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5420, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4898, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5364, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 019 | Train Loss: 0.4869, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5400, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4744, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5339, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4713, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5320, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4770, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5345, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.5016, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5311, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4840, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5333, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4681, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5318, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4722, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5351, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 027 | Train Loss: 0.4646, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5422, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4779, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5480, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4759, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5381, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 030 | Train Loss: 0.4713, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5385, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4679, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5360, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 032 | Train Loss: 0.5022, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5313, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4745, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5256, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 034 | Train Loss: 0.4538, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5397, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4694, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5378, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 036 | Train Loss: 0.4786, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5366, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4875, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5497, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 038 | Train Loss: 0.4586, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5544, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 039 | Train Loss: 0.4484, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5526, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.4655, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5346, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 041 | Train Loss: 0.4643, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5276, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4804, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5347, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4736, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5480, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 044 | Train Loss: 0.4552, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5666, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 045 | Train Loss: 0.4714, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5576, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4621, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5438, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 047 | Train Loss: 0.4515, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5431, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 048 | Train Loss: 0.4520, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5469, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 049 | Train Loss: 0.4621, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5528, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 050 | Train Loss: 0.4607, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5478, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 051 | Train Loss: 0.4746, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5442, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 052 | Train Loss: 0.4724, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5613, Acc: 0.7368, F1: 0.8077\n",
      "\n",
      "Early stopping triggered after 52 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.7447, Acc: 0.4732, F1: 0.5816 | Val Loss: 0.6604, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6492, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6245, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6470, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6197, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6395, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6159, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6290, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5974, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6016, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5726, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 007 | Train Loss: 0.5894, Acc: 0.7054, F1: 0.8114 | Val Loss: 0.5312, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 008 | Train Loss: 0.5488, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5926, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5594, Acc: 0.7143, F1: 0.8118 | Val Loss: 0.5826, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 010 | Train Loss: 0.5698, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5270, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 011 | Train Loss: 0.5342, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5327, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.5384, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.5202, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 013 | Train Loss: 0.5372, Acc: 0.7143, F1: 0.7838 | Val Loss: 0.5216, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 014 | Train Loss: 0.5185, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5221, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 015 | Train Loss: 0.5090, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5297, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 016 | Train Loss: 0.5339, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5229, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 017 | Train Loss: 0.4949, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5334, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4855, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5305, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 019 | Train Loss: 0.4747, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5447, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4835, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5617, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 021 | Train Loss: 0.4748, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5402, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 022 | Train Loss: 0.4802, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5641, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 023 | Train Loss: 0.4942, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5335, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.5166, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5654, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 025 | Train Loss: 0.5098, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5209, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4805, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5568, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 26 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6765, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6493, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6483, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6232, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6318, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6112, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6259, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6076, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6285, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6057, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6259, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6033, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6224, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6015, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6216, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6022, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6180, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6013, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6136, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5980, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6113, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5947, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6067, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5892, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6024, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5858, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5994, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5813, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5951, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5791, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5873, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5729, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5817, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5679, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5770, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5636, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5678, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5586, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 020 | Train Loss: 0.5615, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5563, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 021 | Train Loss: 0.5523, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.5535, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 022 | Train Loss: 0.5517, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.5434, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 023 | Train Loss: 0.5355, Acc: 0.7500, F1: 0.8409 | Val Loss: 0.5390, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 024 | Train Loss: 0.5238, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5364, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 025 | Train Loss: 0.5277, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5348, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 026 | Train Loss: 0.5098, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5338, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.5134, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5299, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 028 | Train Loss: 0.5183, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5297, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 029 | Train Loss: 0.5101, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5338, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.5027, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5309, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 031 | Train Loss: 0.5057, Acc: 0.7232, F1: 0.8050 | Val Loss: 0.5333, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.5007, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5391, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.5027, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5379, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.5030, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5366, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 035 | Train Loss: 0.5055, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5322, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 036 | Train Loss: 0.4950, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5546, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 037 | Train Loss: 0.4997, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5435, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 038 | Train Loss: 0.4915, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5315, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 039 | Train Loss: 0.4954, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5319, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 040 | Train Loss: 0.4999, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5319, Acc: 0.7105, F1: 0.7843\n",
      "\n",
      "Early stopping triggered after 40 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6422, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6119, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6834, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5958, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6158, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6216, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6243, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5946, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5932, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5713, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5825, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5587, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5637, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5585, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5607, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5449, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5461, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5359, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5279, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5293, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.5171, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5282, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 012 | Train Loss: 0.5053, Acc: 0.7857, F1: 0.8571 | Val Loss: 0.5254, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.4902, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5398, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.4965, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5574, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.4945, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5822, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 016 | Train Loss: 0.5848, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5913, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 017 | Train Loss: 0.5359, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5493, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.4719, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5435, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 019 | Train Loss: 0.4978, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5333, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4975, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5297, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.4949, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5422, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 022 | Train Loss: 0.4860, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5435, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 023 | Train Loss: 0.4908, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5333, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.4913, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5294, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.4929, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5330, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4975, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5322, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4935, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5447, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 028 | Train Loss: 0.4897, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5392, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4936, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5384, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 29 epochs.\n",
      "\n",
      ">>> Running gcn hidden=256, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.6324, Acc: 0.6518, F1: 0.7607 | Val Loss: 0.6761, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6730, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6340, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6538, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6195, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6366, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6103, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6367, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5943, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6018, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5730, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 007 | Train Loss: 0.5833, Acc: 0.6786, F1: 0.7882 | Val Loss: 0.5572, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 008 | Train Loss: 0.6318, Acc: 0.5982, F1: 0.6853 | Val Loss: 0.5683, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5860, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5634, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 010 | Train Loss: 0.5248, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.6286, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5456, Acc: 0.7054, F1: 0.8070 | Val Loss: 0.7426, Acc: 0.6579, F1: 0.6977\n",
      "Epoch 012 | Train Loss: 0.5581, Acc: 0.7500, F1: 0.7971 | Val Loss: 0.5859, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 013 | Train Loss: 0.5990, Acc: 0.6786, F1: 0.7805 | Val Loss: 0.5300, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 014 | Train Loss: 0.5053, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5278, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 015 | Train Loss: 0.4954, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5326, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 016 | Train Loss: 0.5080, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.6434, Acc: 0.6579, F1: 0.7111\n",
      "Epoch 017 | Train Loss: 0.5248, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5559, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 018 | Train Loss: 0.4815, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5752, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 019 | Train Loss: 0.4974, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5625, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 020 | Train Loss: 0.5225, Acc: 0.7768, F1: 0.8521 | Val Loss: 0.6019, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 021 | Train Loss: 0.5135, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5411, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 022 | Train Loss: 0.4808, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5330, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4927, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5323, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.5012, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5342, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4933, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5321, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.5252, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5296, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 26 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6523, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6404, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6484, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6363, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6454, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6328, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6430, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6300, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6403, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6274, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6390, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6251, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6374, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6234, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6372, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6216, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6361, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6204, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6352, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6192, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6344, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6183, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6337, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6179, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6333, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6173, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6327, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6165, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6321, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6161, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6316, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6153, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6308, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6146, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6306, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6137, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6298, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6131, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6290, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6124, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6284, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6118, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6789, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6209, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6351, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6149, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6321, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6083, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6221, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6076, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6188, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6000, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6112, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5924, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6053, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5862, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6003, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5799, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5965, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5736, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5891, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5719, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5782, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5606, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5755, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5540, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5760, Acc: 0.6875, F1: 0.8000 | Val Loss: 0.5613, Acc: 0.6316, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.5545, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.5454, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5576, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5392, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 016 | Train Loss: 0.5453, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5466, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.5284, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5329, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 018 | Train Loss: 0.5293, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5297, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 019 | Train Loss: 0.5166, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5356, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.5283, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5361, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.5171, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5425, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 022 | Train Loss: 0.5325, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5330, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.5046, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5338, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.5055, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5346, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.5016, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5313, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.5081, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5312, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.5018, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5379, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4987, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5316, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.5129, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5310, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.5272, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5468, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 031 | Train Loss: 0.5124, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5371, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 032 | Train Loss: 0.5048, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5354, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4944, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5360, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 034 | Train Loss: 0.4913, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5306, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 035 | Train Loss: 0.5009, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5312, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6360, Acc: 0.6250, F1: 0.7273 | Val Loss: 0.6420, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6605, Acc: 0.6696, F1: 0.7933 | Val Loss: 0.6030, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6282, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5874, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6222, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6080, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5951, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5854, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6081, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5649, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5849, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.6092, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 008 | Train Loss: 0.5889, Acc: 0.6786, F1: 0.8022 | Val Loss: 0.5495, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5570, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5443, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5473, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5381, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 011 | Train Loss: 0.5319, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5333, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 012 | Train Loss: 0.5284, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5292, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 013 | Train Loss: 0.5093, Acc: 0.7768, F1: 0.8538 | Val Loss: 0.5272, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 014 | Train Loss: 0.4981, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5337, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.4996, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5329, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 016 | Train Loss: 0.4932, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5464, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 017 | Train Loss: 0.5344, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5370, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4834, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5403, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4912, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5503, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 020 | Train Loss: 0.4854, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5444, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 021 | Train Loss: 0.5150, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5565, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 022 | Train Loss: 0.4897, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5504, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 023 | Train Loss: 0.4904, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5478, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.4745, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5581, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 025 | Train Loss: 0.5055, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.5479, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.5081, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5567, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 027 | Train Loss: 0.4747, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5664, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 028 | Train Loss: 0.5161, Acc: 0.7500, F1: 0.8000 | Val Loss: 0.5452, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4851, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5757, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 030 | Train Loss: 0.5314, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5408, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.4837, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5444, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 032 | Train Loss: 0.4952, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5423, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 033 | Train Loss: 0.5317, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5375, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 034 | Train Loss: 0.5244, Acc: 0.7589, F1: 0.8029 | Val Loss: 0.5766, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 035 | Train Loss: 0.5319, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5750, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 036 | Train Loss: 0.5049, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5404, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 037 | Train Loss: 0.4935, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5363, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 038 | Train Loss: 0.4826, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5358, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 039 | Train Loss: 0.4844, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5353, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 040 | Train Loss: 0.4906, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5341, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 041 | Train Loss: 0.4983, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5344, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 042 | Train Loss: 0.4864, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5445, Acc: 0.7105, F1: 0.7660\n",
      "\n",
      "Early stopping triggered after 42 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6837, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6761, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6775, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6704, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6723, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6653, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6680, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6603, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6636, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6557, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6600, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6515, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6570, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6476, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6532, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6441, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6513, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6407, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6482, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6379, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6463, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6351, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6444, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6325, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6429, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6302, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6406, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6285, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6406, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6264, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6382, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6248, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6379, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6230, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6364, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6216, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6356, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6204, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6347, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6193, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6341, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6182, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7214, Acc: 0.3750, F1: 0.2222 | Val Loss: 0.6537, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6562, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6176, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6364, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6144, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6343, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6117, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6297, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6095, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6270, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6138, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6269, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6090, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6201, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5974, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6106, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5868, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6047, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5783, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5927, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5685, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5888, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5626, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5761, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5531, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5622, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5496, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 015 | Train Loss: 0.5522, Acc: 0.7321, F1: 0.8214 | Val Loss: 0.5476, Acc: 0.6316, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.5396, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5310, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 017 | Train Loss: 0.5348, Acc: 0.7411, F1: 0.8242 | Val Loss: 0.5276, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 018 | Train Loss: 0.5163, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5256, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 019 | Train Loss: 0.5194, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5258, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.5198, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5267, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.5084, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5532, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 022 | Train Loss: 0.5046, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5286, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.5138, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5329, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 024 | Train Loss: 0.5113, Acc: 0.7232, F1: 0.8000 | Val Loss: 0.5421, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 025 | Train Loss: 0.5007, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5283, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.5067, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5263, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 027 | Train Loss: 0.5187, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5471, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 028 | Train Loss: 0.4988, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5285, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 029 | Train Loss: 0.5032, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5261, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.4864, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5367, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4932, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5286, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.4874, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5243, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 033 | Train Loss: 0.4948, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5240, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 034 | Train Loss: 0.4829, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5347, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 34 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6743, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6256, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6402, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6065, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6329, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6004, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6307, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5910, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6212, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5986, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6115, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5749, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5876, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5748, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5751, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5502, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5481, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5361, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 010 | Train Loss: 0.5514, Acc: 0.7143, F1: 0.7949 | Val Loss: 0.5492, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 011 | Train Loss: 0.5440, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.6283, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 012 | Train Loss: 0.6173, Acc: 0.6696, F1: 0.6942 | Val Loss: 0.5311, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 013 | Train Loss: 0.5133, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.5948, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 014 | Train Loss: 0.5381, Acc: 0.7321, F1: 0.8214 | Val Loss: 0.5786, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 015 | Train Loss: 0.5447, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.5253, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 016 | Train Loss: 0.5334, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5389, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 017 | Train Loss: 0.5136, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5648, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 018 | Train Loss: 0.5199, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5313, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 019 | Train Loss: 0.5118, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5284, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4905, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5452, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 021 | Train Loss: 0.5029, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5428, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 022 | Train Loss: 0.4938, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5394, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4985, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5357, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.5259, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5521, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 025 | Train Loss: 0.4909, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5340, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4846, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5318, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.5107, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5369, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.5036, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5423, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 029 | Train Loss: 0.5065, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5475, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4809, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5330, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 031 | Train Loss: 0.4710, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5383, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.4801, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5249, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 033 | Train Loss: 0.4703, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5251, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4915, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5271, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4562, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5751, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.4732, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5234, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4667, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5364, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 038 | Train Loss: 0.4907, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5602, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 039 | Train Loss: 0.4882, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5237, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 040 | Train Loss: 0.4579, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5478, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 041 | Train Loss: 0.4832, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5345, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4609, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5248, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 043 | Train Loss: 0.4581, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5376, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 044 | Train Loss: 0.4389, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5383, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4347, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5436, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 046 | Train Loss: 0.4391, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5486, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 047 | Train Loss: 0.4404, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5278, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4562, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5266, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 049 | Train Loss: 0.4487, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5541, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 050 | Train Loss: 0.4471, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5314, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 051 | Train Loss: 0.4408, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5336, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 052 | Train Loss: 0.4371, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5376, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 053 | Train Loss: 0.4358, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5396, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 054 | Train Loss: 0.4329, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5340, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 055 | Train Loss: 0.4618, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5368, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 056 | Train Loss: 0.4569, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5569, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 057 | Train Loss: 0.4428, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5353, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 058 | Train Loss: 0.4411, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5402, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 58 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6635, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6518, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6553, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6419, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6470, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6343, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6423, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6278, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6360, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6231, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6329, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6196, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6316, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6168, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6306, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6148, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6292, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6133, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6291, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6120, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6290, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6110, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6273, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6102, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6254, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6093, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6233, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6083, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6226, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6072, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6233, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6065, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6209, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6056, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6198, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6045, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6194, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6036, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6181, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6029, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6161, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6021, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6697, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6328, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6462, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6151, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6360, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6121, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6307, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6100, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6278, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6066, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6223, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5994, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6139, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5930, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6051, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5801, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5894, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5646, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5746, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5595, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5542, Acc: 0.7054, F1: 0.8070 | Val Loss: 0.5425, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5451, Acc: 0.7143, F1: 0.8118 | Val Loss: 0.5365, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 013 | Train Loss: 0.5375, Acc: 0.7143, F1: 0.8118 | Val Loss: 0.5354, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 014 | Train Loss: 0.5288, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5552, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.5175, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5353, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 016 | Train Loss: 0.5266, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5364, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 017 | Train Loss: 0.5400, Acc: 0.7232, F1: 0.7862 | Val Loss: 0.5682, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 018 | Train Loss: 0.5108, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5431, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 019 | Train Loss: 0.5373, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5332, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 020 | Train Loss: 0.5127, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5360, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 021 | Train Loss: 0.5097, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5383, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 022 | Train Loss: 0.4920, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5239, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 023 | Train Loss: 0.5011, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5287, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 024 | Train Loss: 0.5044, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5218, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.5152, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5385, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 026 | Train Loss: 0.4851, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5201, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 027 | Train Loss: 0.4882, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5197, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 028 | Train Loss: 0.4832, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5251, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4841, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5291, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4877, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5183, Acc: 0.7105, F1: 0.7843\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7017, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6300, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6487, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6316, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6408, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6168, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6288, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6303, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5929, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6047, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6075, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6401, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6055, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6104, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5722, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5885, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5657, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5762, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5567, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5616, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5510, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 012 | Train Loss: 0.5495, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5382, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 013 | Train Loss: 0.5427, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5312, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 014 | Train Loss: 0.5362, Acc: 0.7054, F1: 0.8047 | Val Loss: 0.5354, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 015 | Train Loss: 0.5195, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5283, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 016 | Train Loss: 0.5148, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5378, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.5080, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5272, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 018 | Train Loss: 0.5122, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5274, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.5180, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5332, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.5165, Acc: 0.7232, F1: 0.8000 | Val Loss: 0.5347, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 021 | Train Loss: 0.4796, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5442, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.5048, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5301, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.4857, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5337, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.5422, Acc: 0.7143, F1: 0.7838 | Val Loss: 0.5321, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.5227, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5540, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.5011, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5559, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 027 | Train Loss: 0.5090, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5330, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4967, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5337, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4818, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5371, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4998, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5356, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5044, Acc: 0.7143, F1: 0.7838 | Val Loss: 0.5401, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.4786, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5608, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 32 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7494, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7430, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7289, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7232, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.7134, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7050, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 004 | Train Loss: 0.6990, Acc: 0.3839, F1: 0.2737 | Val Loss: 0.6885, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6841, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.6746, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6738, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6617, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6643, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6507, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6557, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6427, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6469, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6366, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6468, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6312, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6411, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6273, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6397, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6241, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6353, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6215, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6364, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6193, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6352, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6178, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6337, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6165, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6331, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6153, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6328, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6141, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6299, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6130, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6309, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6121, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6261, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6111, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6292, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6104, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6249, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6096, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 024 | Train Loss: 0.6262, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6087, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 24 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7257, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6916, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6794, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.6592, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6572, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6346, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6459, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6250, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6463, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6231, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6439, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6197, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6423, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6191, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6340, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6151, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6310, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6116, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6283, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6071, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6241, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6016, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6173, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5941, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6097, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5887, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5976, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5732, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5884, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5613, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5713, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5606, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 017 | Train Loss: 0.5663, Acc: 0.7321, F1: 0.8214 | Val Loss: 0.5423, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 018 | Train Loss: 0.5734, Acc: 0.6786, F1: 0.7722 | Val Loss: 0.5394, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 019 | Train Loss: 0.5511, Acc: 0.6964, F1: 0.8000 | Val Loss: 0.5405, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 020 | Train Loss: 0.5474, Acc: 0.6696, F1: 0.7886 | Val Loss: 0.5393, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.5404, Acc: 0.7143, F1: 0.7949 | Val Loss: 0.5406, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 022 | Train Loss: 0.5393, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5265, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 023 | Train Loss: 0.5244, Acc: 0.7589, F1: 0.8402 | Val Loss: 0.5243, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 024 | Train Loss: 0.5178, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5356, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 025 | Train Loss: 0.5148, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5276, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.5136, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5208, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 027 | Train Loss: 0.5225, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5195, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 028 | Train Loss: 0.5006, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5353, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 029 | Train Loss: 0.5005, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5224, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.5043, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5185, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 031 | Train Loss: 0.4927, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5209, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.4933, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5437, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 033 | Train Loss: 0.4944, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5184, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 034 | Train Loss: 0.4923, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5175, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 035 | Train Loss: 0.4827, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5185, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 036 | Train Loss: 0.4838, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5278, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 36 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6718, Acc: 0.6339, F1: 0.7760 | Val Loss: 0.6227, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6507, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6393, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6457, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6146, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6317, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5994, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6110, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5761, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6014, Acc: 0.6786, F1: 0.8000 | Val Loss: 0.5454, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.5676, Acc: 0.6786, F1: 0.8022 | Val Loss: 0.6047, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 008 | Train Loss: 0.5645, Acc: 0.6964, F1: 0.7606 | Val Loss: 0.5274, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 009 | Train Loss: 0.5276, Acc: 0.7768, F1: 0.8521 | Val Loss: 0.5197, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 010 | Train Loss: 0.5418, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5424, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 011 | Train Loss: 0.4872, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5272, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 012 | Train Loss: 0.4962, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5179, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 013 | Train Loss: 0.5052, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5519, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4631, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5157, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 015 | Train Loss: 0.5390, Acc: 0.7500, F1: 0.8372 | Val Loss: 0.5123, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.4911, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.6084, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 017 | Train Loss: 0.5218, Acc: 0.7857, F1: 0.8235 | Val Loss: 0.5214, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 018 | Train Loss: 0.5105, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5305, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 019 | Train Loss: 0.4986, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5301, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 020 | Train Loss: 0.4684, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5265, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 021 | Train Loss: 0.4738, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5117, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 022 | Train Loss: 0.4608, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5065, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4669, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5068, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.4606, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5223, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.4516, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5165, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4443, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5161, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 027 | Train Loss: 0.4435, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5230, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4462, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5411, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4406, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5272, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4378, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5304, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4601, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5308, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 032 | Train Loss: 0.4500, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5145, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 033 | Train Loss: 0.4489, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5220, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4402, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5180, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4309, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5194, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4621, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5190, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 037 | Train Loss: 0.4440, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.5520, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 038 | Train Loss: 0.4668, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5235, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 039 | Train Loss: 0.4380, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5210, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 040 | Train Loss: 0.4505, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5236, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 041 | Train Loss: 0.4443, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5304, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4431, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5476, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 043 | Train Loss: 0.4518, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.5371, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 044 | Train Loss: 0.4490, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5235, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 045 | Train Loss: 0.4582, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5183, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 046 | Train Loss: 0.4537, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5676, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 047 | Train Loss: 0.4871, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5164, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4436, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5162, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 049 | Train Loss: 0.4388, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5197, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 050 | Train Loss: 0.4370, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5253, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 051 | Train Loss: 0.4554, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5138, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 51 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6869, Acc: 0.6696, F1: 0.7978 | Val Loss: 0.6802, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6783, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6703, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6699, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6618, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6625, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6544, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6547, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6474, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6495, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6397, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6440, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6338, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6410, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6287, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6391, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6240, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6367, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6209, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6345, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6190, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6336, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6168, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6340, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6150, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6324, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6134, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6274, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6116, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6252, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6101, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6279, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6085, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6248, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6073, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6273, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6061, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6210, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6049, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6212, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6040, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7416, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6917, Acc: 0.5526, F1: 0.7018\n",
      "Epoch 002 | Train Loss: 0.6731, Acc: 0.6071, F1: 0.7556 | Val Loss: 0.6330, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6619, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6323, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6474, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6217, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6359, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6194, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6335, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6189, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6352, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6178, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6365, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6083, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6275, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6013, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6187, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5975, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6234, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5923, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6149, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5948, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6075, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5903, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5974, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5783, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5962, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5743, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6006, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5691, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5889, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5730, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5791, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5656, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5782, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5550, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5720, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5510, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5714, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5458, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 022 | Train Loss: 0.5531, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5437, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 023 | Train Loss: 0.5491, Acc: 0.7500, F1: 0.8353 | Val Loss: 0.5360, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.5411, Acc: 0.7143, F1: 0.8140 | Val Loss: 0.5295, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.5334, Acc: 0.7500, F1: 0.8333 | Val Loss: 0.5304, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.5161, Acc: 0.7946, F1: 0.8589 | Val Loss: 0.5311, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.5110, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5382, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 028 | Train Loss: 0.5125, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5309, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.5049, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5336, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.5176, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5411, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4900, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5406, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 032 | Train Loss: 0.5415, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5380, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.5071, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5509, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 034 | Train Loss: 0.4892, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5343, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 035 | Train Loss: 0.5139, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5361, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 036 | Train Loss: 0.4878, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5348, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 037 | Train Loss: 0.5167, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5378, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 038 | Train Loss: 0.5050, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5333, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 039 | Train Loss: 0.5160, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5326, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 040 | Train Loss: 0.4980, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5411, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 041 | Train Loss: 0.4931, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5271, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 41 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6513, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.7104, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.7127, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6368, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6547, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6359, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6367, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6071, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6232, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6024, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5926, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6058, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5791, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6079, Acc: 0.6875, F1: 0.8023 | Val Loss: 0.5578, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5647, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5364, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 010 | Train Loss: 0.5612, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5288, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 011 | Train Loss: 0.5178, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5396, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5510, Acc: 0.6964, F1: 0.7875 | Val Loss: 0.5390, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 013 | Train Loss: 0.5369, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5269, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.5319, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5116, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 015 | Train Loss: 0.5097, Acc: 0.7857, F1: 0.8554 | Val Loss: 0.5112, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 016 | Train Loss: 0.4914, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5179, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4803, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5245, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 018 | Train Loss: 0.5313, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5115, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 019 | Train Loss: 0.4918, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5213, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.5254, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5059, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.5265, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5763, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 022 | Train Loss: 0.5207, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5071, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4899, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5082, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.4735, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5478, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 025 | Train Loss: 0.5058, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.5079, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4837, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.4988, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4582, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5120, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4819, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5338, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 029 | Train Loss: 0.4590, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4994, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4610, Acc: 0.7946, F1: 0.8589 | Val Loss: 0.5099, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4470, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.5325, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 032 | Train Loss: 0.4839, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5478, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 033 | Train Loss: 0.4625, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5143, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 034 | Train Loss: 0.4943, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5009, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 035 | Train Loss: 0.5134, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5754, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 036 | Train Loss: 0.5166, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4934, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 037 | Train Loss: 0.4704, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.4949, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 038 | Train Loss: 0.4542, Acc: 0.8214, F1: 0.8765 | Val Loss: 0.5041, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4548, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5073, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4372, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.4917, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4761, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.4928, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4401, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.4956, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 043 | Train Loss: 0.4633, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5021, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4689, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4949, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4440, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.4916, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4628, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4905, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 047 | Train Loss: 0.4682, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.4923, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 048 | Train Loss: 0.4529, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5029, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 48 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6932, Acc: 0.4375, F1: 0.3636 | Val Loss: 0.6956, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.6883, Acc: 0.6161, F1: 0.7425 | Val Loss: 0.6845, Acc: 0.6842, F1: 0.8065\n",
      "Epoch 003 | Train Loss: 0.6785, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6743, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6729, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6644, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6613, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6564, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6574, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6486, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6517, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6418, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6507, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6349, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6487, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6294, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6398, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6256, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6384, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6214, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6308, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6185, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6298, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6163, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6329, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6146, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6324, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6132, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6286, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6124, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6342, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6119, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6332, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6118, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6233, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6112, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6288, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6108, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6320, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6103, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6274, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6098, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 22 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6749, Acc: 0.6071, F1: 0.7143 | Val Loss: 0.6146, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6385, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6108, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6336, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6055, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6170, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6085, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6238, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6058, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6148, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5971, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6094, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5856, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6112, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5792, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5932, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5838, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5980, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5793, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5791, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5649, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5752, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5572, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5895, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5485, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5701, Acc: 0.6696, F1: 0.7978 | Val Loss: 0.5417, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 015 | Train Loss: 0.5537, Acc: 0.7500, F1: 0.8353 | Val Loss: 0.5434, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 016 | Train Loss: 0.5578, Acc: 0.7232, F1: 0.8121 | Val Loss: 0.5297, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 017 | Train Loss: 0.5311, Acc: 0.7143, F1: 0.8072 | Val Loss: 0.5234, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 018 | Train Loss: 0.5346, Acc: 0.7232, F1: 0.8121 | Val Loss: 0.5399, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 019 | Train Loss: 0.5112, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5329, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 020 | Train Loss: 0.5088, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5313, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 021 | Train Loss: 0.5383, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5286, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.5168, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5600, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 023 | Train Loss: 0.5321, Acc: 0.7143, F1: 0.7922 | Val Loss: 0.5266, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.5122, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5307, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4742, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5371, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.5392, Acc: 0.7054, F1: 0.7755 | Val Loss: 0.5231, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4962, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5225, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 028 | Train Loss: 0.5039, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5223, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.5407, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5333, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4855, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5199, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5072, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5179, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 032 | Train Loss: 0.5048, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5183, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 033 | Train Loss: 0.4975, Acc: 0.8036, F1: 0.8625 | Val Loss: 0.5188, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 034 | Train Loss: 0.5028, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5325, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 34 epochs.\n",
      "\n",
      ">>> Running sage hidden=16, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7215, Acc: 0.6607, F1: 0.7912 | Val Loss: 0.6237, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6646, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6626, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6620, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6439, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6538, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6246, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6392, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6212, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6385, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6197, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6415, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6190, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6377, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6149, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6322, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6109, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6301, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6090, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6214, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6043, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6158, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5817, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6147, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5700, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5940, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.6055, Acc: 0.6316, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.5993, Acc: 0.6786, F1: 0.7882 | Val Loss: 0.5488, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 016 | Train Loss: 0.5885, Acc: 0.6696, F1: 0.7978 | Val Loss: 0.5387, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 017 | Train Loss: 0.5752, Acc: 0.6786, F1: 0.7692 | Val Loss: 0.5377, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5660, Acc: 0.6786, F1: 0.7857 | Val Loss: 0.5311, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 019 | Train Loss: 0.5509, Acc: 0.7054, F1: 0.8047 | Val Loss: 0.5575, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 020 | Train Loss: 0.5739, Acc: 0.7143, F1: 0.7895 | Val Loss: 0.5330, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.5536, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5269, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 022 | Train Loss: 0.5510, Acc: 0.7143, F1: 0.7949 | Val Loss: 0.5406, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.5430, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5245, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 024 | Train Loss: 0.5316, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5274, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.5158, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5257, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.5688, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5275, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.5118, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5253, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.5267, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5473, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 029 | Train Loss: 0.5321, Acc: 0.7232, F1: 0.8025 | Val Loss: 0.5188, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 030 | Train Loss: 0.5297, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5278, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5559, Acc: 0.6875, F1: 0.7682 | Val Loss: 0.5135, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 032 | Train Loss: 0.5089, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5096, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 033 | Train Loss: 0.5625, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5625, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 034 | Train Loss: 0.5322, Acc: 0.7232, F1: 0.8000 | Val Loss: 0.5052, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.5113, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.5057, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7138, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7035, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.6976, Acc: 0.3839, F1: 0.3894 | Val Loss: 0.6863, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6855, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6723, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6725, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6619, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6651, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6524, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6560, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6454, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6518, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6387, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6465, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6336, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6423, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6295, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6397, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6256, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6368, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6224, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6356, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6197, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6337, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6177, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6318, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6160, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6303, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6141, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6286, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6122, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6262, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6104, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6238, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6090, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6220, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6080, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6204, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6068, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6180, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6046, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6155, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6017, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 22 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6543, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6180, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6397, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6119, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6291, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6140, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6309, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6068, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6147, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5935, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6098, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5846, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6020, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5792, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5924, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5692, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5757, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5485, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5604, Acc: 0.6786, F1: 0.7978 | Val Loss: 0.5414, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 011 | Train Loss: 0.5379, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5304, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 012 | Train Loss: 0.5282, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5295, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 013 | Train Loss: 0.5143, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5294, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 014 | Train Loss: 0.5241, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5321, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.5343, Acc: 0.7054, F1: 0.7660 | Val Loss: 0.5509, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.5187, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5334, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 017 | Train Loss: 0.5195, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5474, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 018 | Train Loss: 0.5058, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5213, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 019 | Train Loss: 0.5073, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5178, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4916, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5209, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4981, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5255, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4889, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5159, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4878, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5128, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 024 | Train Loss: 0.4906, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5208, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4803, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5106, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4872, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5079, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 027 | Train Loss: 0.4904, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5110, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 028 | Train Loss: 0.4781, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5099, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 029 | Train Loss: 0.4707, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5101, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4682, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5246, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4807, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5129, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4788, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5158, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 033 | Train Loss: 0.4709, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5129, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 034 | Train Loss: 0.4676, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5302, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4686, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5126, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 036 | Train Loss: 0.4663, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5223, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 037 | Train Loss: 0.4623, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5105, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4614, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5168, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4535, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5086, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.4720, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5111, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4585, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5315, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4593, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5107, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 043 | Train Loss: 0.4598, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5094, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4548, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5104, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4497, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5168, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4500, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5130, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4526, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5168, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4490, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5328, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 049 | Train Loss: 0.4549, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5286, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 050 | Train Loss: 0.4497, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5130, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 051 | Train Loss: 0.4498, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5117, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4553, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5141, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 053 | Train Loss: 0.4657, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5197, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 054 | Train Loss: 0.4484, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5111, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 055 | Train Loss: 0.4482, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5269, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 056 | Train Loss: 0.4724, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5224, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 057 | Train Loss: 0.4416, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5197, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 058 | Train Loss: 0.4699, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5084, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 059 | Train Loss: 0.4478, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5541, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 060 | Train Loss: 0.4652, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5050, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 061 | Train Loss: 0.4445, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5070, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 61 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7456, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6378, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6521, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6123, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6325, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6018, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6161, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6011, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6117, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5770, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5846, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5731, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5716, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5717, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 008 | Train Loss: 0.5418, Acc: 0.7232, F1: 0.8050 | Val Loss: 0.5338, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 009 | Train Loss: 0.5096, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5621, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 010 | Train Loss: 0.5244, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.6293, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 011 | Train Loss: 0.5470, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5320, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 012 | Train Loss: 0.4934, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5332, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.5367, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5233, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 014 | Train Loss: 0.5108, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5385, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4744, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5675, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 016 | Train Loss: 0.5181, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5609, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 017 | Train Loss: 0.5035, Acc: 0.7589, F1: 0.8058 | Val Loss: 0.5222, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.5088, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5175, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 019 | Train Loss: 0.5354, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5268, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.5055, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5116, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 021 | Train Loss: 0.4682, Acc: 0.8125, F1: 0.8712 | Val Loss: 0.5333, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 022 | Train Loss: 0.4775, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5176, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.4795, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5231, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 024 | Train Loss: 0.4600, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5729, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 025 | Train Loss: 0.4564, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5269, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4480, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5285, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4535, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5247, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4405, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5254, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4387, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5230, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4583, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5296, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 031 | Train Loss: 0.4506, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5322, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 032 | Train Loss: 0.4477, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5496, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 033 | Train Loss: 0.4385, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5238, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4501, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5191, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4326, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5468, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 036 | Train Loss: 0.4695, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5339, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 037 | Train Loss: 0.4577, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5540, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 038 | Train Loss: 0.4477, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5107, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 039 | Train Loss: 0.4485, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5169, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4297, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5426, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 041 | Train Loss: 0.4424, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5336, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4329, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5116, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4384, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5126, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 044 | Train Loss: 0.4291, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5333, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 045 | Train Loss: 0.4407, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5255, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 046 | Train Loss: 0.4324, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5220, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4354, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5253, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4342, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5397, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 049 | Train Loss: 0.4408, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5128, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 050 | Train Loss: 0.4322, Acc: 0.8125, F1: 0.8662 | Val Loss: 0.5229, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 051 | Train Loss: 0.4383, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5202, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4560, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5403, Acc: 0.7895, F1: 0.8400\n",
      "\n",
      "Early stopping triggered after 52 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6795, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6602, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6607, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6403, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6485, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6271, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6387, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6209, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6364, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6182, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6358, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6163, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6342, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6150, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6344, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6140, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6333, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6130, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6317, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6120, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6319, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6111, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6299, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6102, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6292, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6096, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6264, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6096, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6255, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6103, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6262, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6096, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6246, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6089, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6232, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6078, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6223, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6060, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6207, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6044, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6194, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6018, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6465, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6149, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6430, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6114, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6305, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6102, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6240, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6073, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6192, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6020, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6124, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5883, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5972, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5768, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5833, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5684, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5625, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5502, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5515, Acc: 0.6875, F1: 0.7929 | Val Loss: 0.5416, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 011 | Train Loss: 0.5350, Acc: 0.7143, F1: 0.8072 | Val Loss: 0.5389, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 012 | Train Loss: 0.5366, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5392, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 013 | Train Loss: 0.5728, Acc: 0.7143, F1: 0.8161 | Val Loss: 0.5472, Acc: 0.6316, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.5047, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.6268, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 015 | Train Loss: 0.5787, Acc: 0.7054, F1: 0.7591 | Val Loss: 0.5364, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.5056, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5323, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 017 | Train Loss: 0.5104, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5308, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 018 | Train Loss: 0.5040, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5323, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.5014, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5316, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4949, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5285, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4996, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5287, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4873, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5274, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 023 | Train Loss: 0.4977, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5255, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.4926, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5372, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4883, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5243, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 026 | Train Loss: 0.4944, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5260, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.5213, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5305, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.5475, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5466, Acc: 0.6842, F1: 0.7931\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6896, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.7020, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.6767, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6328, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6269, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6150, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6381, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6069, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6202, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5990, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6020, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5663, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5689, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5622, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 008 | Train Loss: 0.5378, Acc: 0.7143, F1: 0.8000 | Val Loss: 0.5366, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 009 | Train Loss: 0.5403, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5484, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 010 | Train Loss: 0.5208, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5767, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 011 | Train Loss: 0.5229, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5579, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 012 | Train Loss: 0.5207, Acc: 0.7946, F1: 0.8589 | Val Loss: 0.5948, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 013 | Train Loss: 0.5128, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5200, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 014 | Train Loss: 0.4824, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5179, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 015 | Train Loss: 0.4842, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5339, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.5004, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5168, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 017 | Train Loss: 0.5038, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5186, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 018 | Train Loss: 0.4749, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5270, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4775, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5217, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 020 | Train Loss: 0.4746, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.5221, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.4576, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5638, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 022 | Train Loss: 0.4919, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5226, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4510, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5438, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.4646, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5268, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.4624, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5253, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4452, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5545, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 027 | Train Loss: 0.4498, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5249, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4489, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5200, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4509, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5395, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 030 | Train Loss: 0.4451, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5182, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4475, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5229, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4379, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5222, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4463, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5282, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4467, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5204, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 035 | Train Loss: 0.4502, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5460, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4460, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5264, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 037 | Train Loss: 0.4438, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5366, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 038 | Train Loss: 0.4568, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5285, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 039 | Train Loss: 0.4529, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5548, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 040 | Train Loss: 0.4469, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5216, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 041 | Train Loss: 0.4660, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5245, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 042 | Train Loss: 0.4360, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5516, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 043 | Train Loss: 0.4656, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.5639, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 044 | Train Loss: 0.4384, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5198, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 045 | Train Loss: 0.4688, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5119, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 046 | Train Loss: 0.4460, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5348, Acc: 0.7895, F1: 0.8462\n",
      "\n",
      "Early stopping triggered after 46 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6931, Acc: 0.5089, F1: 0.5133 | Val Loss: 0.6806, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6771, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6621, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6654, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6489, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6515, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6390, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6467, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6307, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6418, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6248, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6364, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6210, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6359, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6179, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6349, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6163, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6338, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6148, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6316, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6137, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6317, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6126, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6307, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6121, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6300, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6118, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6290, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6103, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6273, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6092, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6263, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6080, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6241, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6066, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6236, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6050, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6217, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6033, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6189, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6013, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6811, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6288, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6477, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6191, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6396, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6124, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6423, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6248, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6274, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6004, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6135, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5884, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6052, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5806, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5875, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5625, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5695, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5481, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5474, Acc: 0.7232, F1: 0.8187 | Val Loss: 0.5392, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 011 | Train Loss: 0.5405, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5331, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 012 | Train Loss: 0.5216, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5339, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 013 | Train Loss: 0.5075, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5383, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.5044, Acc: 0.7054, F1: 0.7785 | Val Loss: 0.5415, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 015 | Train Loss: 0.5162, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5339, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.5062, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5439, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 017 | Train Loss: 0.4999, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5254, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 018 | Train Loss: 0.4899, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5223, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 019 | Train Loss: 0.4914, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5214, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4876, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5176, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.4772, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5514, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 022 | Train Loss: 0.5007, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5148, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4811, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5146, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.5050, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5231, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4900, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5134, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4935, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5232, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4678, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5164, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 028 | Train Loss: 0.4690, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5278, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4782, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5163, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4609, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5139, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4593, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5369, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.4720, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5175, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4656, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5083, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4485, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5167, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4637, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5131, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4839, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.5179, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4247, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5716, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.5249, Acc: 0.7679, F1: 0.8116 | Val Loss: 0.5132, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4816, Acc: 0.7946, F1: 0.8589 | Val Loss: 0.5647, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 040 | Train Loss: 0.5034, Acc: 0.7679, F1: 0.8471 | Val Loss: 0.5103, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4560, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5402, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 042 | Train Loss: 0.4689, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5043, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 043 | Train Loss: 0.4527, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5037, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4465, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5072, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4455, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5125, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4481, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5128, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4472, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5167, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4486, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5139, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4485, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5154, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 050 | Train Loss: 0.4470, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5131, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 051 | Train Loss: 0.4485, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5199, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4420, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5108, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 053 | Train Loss: 0.4392, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5088, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 054 | Train Loss: 0.4454, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5091, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 055 | Train Loss: 0.4602, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5124, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 056 | Train Loss: 0.4606, Acc: 0.8036, F1: 0.8608 | Val Loss: 0.5146, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 057 | Train Loss: 0.4412, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5128, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 058 | Train Loss: 0.4411, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5049, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 059 | Train Loss: 0.4491, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5071, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 060 | Train Loss: 0.4401, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5175, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 061 | Train Loss: 0.4380, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5134, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 61 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7748, Acc: 0.4911, F1: 0.5899 | Val Loss: 0.6727, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6496, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6126, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6468, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6082, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6237, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5996, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6141, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5815, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5947, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5716, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 007 | Train Loss: 0.5738, Acc: 0.7232, F1: 0.8187 | Val Loss: 0.5319, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 008 | Train Loss: 0.5518, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5384, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 009 | Train Loss: 0.5164, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5263, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 010 | Train Loss: 0.5055, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5341, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 011 | Train Loss: 0.4975, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5235, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 012 | Train Loss: 0.5214, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5232, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 013 | Train Loss: 0.4908, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5276, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.5364, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5206, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 015 | Train Loss: 0.4792, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5245, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.4706, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5141, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 017 | Train Loss: 0.4723, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5170, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 018 | Train Loss: 0.4550, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5186, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 019 | Train Loss: 0.4655, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5391, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 020 | Train Loss: 0.4581, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5265, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.4569, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5316, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4567, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5332, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.4486, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5277, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.4689, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5397, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 025 | Train Loss: 0.4574, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5272, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4517, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5196, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4424, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5289, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 028 | Train Loss: 0.4391, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5319, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 029 | Train Loss: 0.4499, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5172, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 030 | Train Loss: 0.4574, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5298, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 031 | Train Loss: 0.4298, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5145, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 032 | Train Loss: 0.4559, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5197, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4341, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5566, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 034 | Train Loss: 0.4574, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5272, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4405, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5265, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4451, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5161, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 037 | Train Loss: 0.4362, Acc: 0.8214, F1: 0.8701 | Val Loss: 0.5382, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 038 | Train Loss: 0.4381, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5285, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4420, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5443, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4351, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5503, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 041 | Train Loss: 0.4343, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5272, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4329, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5182, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 043 | Train Loss: 0.4368, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5382, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 044 | Train Loss: 0.4438, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5296, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 44 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7205, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7150, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7065, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7001, Acc: 0.3684, F1: 0.2500\n",
      "Epoch 003 | Train Loss: 0.6947, Acc: 0.4196, F1: 0.5255 | Val Loss: 0.6862, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6816, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6730, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6722, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6608, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6621, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6509, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6542, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6427, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6469, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6357, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6450, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6309, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6422, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6278, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6408, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6248, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6376, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6231, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6331, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6217, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6345, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6203, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6330, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6186, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6320, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6172, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6299, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6157, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6279, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6142, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6295, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6127, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6273, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6113, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6270, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6099, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6257, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6091, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6214, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6076, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 23 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6570, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.6171, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6814, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6126, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6419, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6297, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6415, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6322, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6334, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6134, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6211, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5955, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6121, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5893, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6067, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5829, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5966, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5831, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5954, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5683, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5807, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5625, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5736, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5567, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5653, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5541, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5683, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5576, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5549, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5455, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 016 | Train Loss: 0.5562, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5439, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 017 | Train Loss: 0.5435, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5427, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 018 | Train Loss: 0.5355, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5345, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 019 | Train Loss: 0.5192, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5310, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 020 | Train Loss: 0.5049, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5308, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 021 | Train Loss: 0.5031, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5359, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.4894, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5423, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.4988, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5438, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.5083, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5413, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.4874, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5403, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.4892, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5394, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4810, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5404, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 028 | Train Loss: 0.4943, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5347, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4833, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5370, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4765, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5322, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 031 | Train Loss: 0.4821, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5321, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 032 | Train Loss: 0.4745, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5295, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 033 | Train Loss: 0.4925, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5310, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 034 | Train Loss: 0.4992, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5317, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 34 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7339, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.6818, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6694, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6275, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6383, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6069, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6253, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5987, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6311, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6003, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6019, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5793, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5928, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5682, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5933, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5673, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6071, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5756, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5607, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5519, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5863, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5416, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5588, Acc: 0.7857, F1: 0.8554 | Val Loss: 0.6001, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.5794, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5415, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 014 | Train Loss: 0.5140, Acc: 0.7589, F1: 0.8421 | Val Loss: 0.5318, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 015 | Train Loss: 0.5343, Acc: 0.7232, F1: 0.8187 | Val Loss: 0.5653, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 016 | Train Loss: 0.5170, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5382, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 017 | Train Loss: 0.5111, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5384, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5143, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5318, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.5127, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5362, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.5031, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5442, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 021 | Train Loss: 0.4801, Acc: 0.7946, F1: 0.8589 | Val Loss: 0.5577, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 022 | Train Loss: 0.5901, Acc: 0.7143, F1: 0.7576 | Val Loss: 0.5262, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.5493, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.5624, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 024 | Train Loss: 0.5195, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5522, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.5034, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5211, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 026 | Train Loss: 0.4863, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5236, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4813, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5206, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 028 | Train Loss: 0.4867, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5324, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4774, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5380, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4787, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5465, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 031 | Train Loss: 0.4770, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5254, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4672, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5396, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 033 | Train Loss: 0.4607, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5242, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4493, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5311, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4547, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5340, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 036 | Train Loss: 0.4807, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5501, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 037 | Train Loss: 0.4628, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.5490, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 038 | Train Loss: 0.4478, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5331, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 039 | Train Loss: 0.4581, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5458, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4483, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5404, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4547, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5309, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4420, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5235, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4559, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5263, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4473, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5360, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 045 | Train Loss: 0.4428, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5204, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4483, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5306, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 047 | Train Loss: 0.4475, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5547, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 048 | Train Loss: 0.4488, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5228, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4487, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5267, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 050 | Train Loss: 0.4387, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5402, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 051 | Train Loss: 0.4415, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5259, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 052 | Train Loss: 0.4457, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5266, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 053 | Train Loss: 0.4417, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5342, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 054 | Train Loss: 0.4395, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5494, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 055 | Train Loss: 0.4478, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5528, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 056 | Train Loss: 0.4450, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5409, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 56 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7422, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7291, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7174, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7074, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.6968, Acc: 0.4464, F1: 0.3111 | Val Loss: 0.6862, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 004 | Train Loss: 0.6781, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.6673, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6655, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6502, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6541, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6371, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6464, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6277, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6382, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6220, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6319, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6186, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6318, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6166, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6325, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6149, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6308, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6139, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6330, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6132, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6284, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6129, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6291, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6127, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6280, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6124, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6257, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6113, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6233, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6096, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6259, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6089, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6260, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6077, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6252, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6061, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6211, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6046, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6192, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6030, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 23 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6617, Acc: 0.6071, F1: 0.7143 | Val Loss: 0.6299, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6527, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6177, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6444, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6419, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6426, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6155, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6326, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6029, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6197, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5970, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6120, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5931, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6029, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5866, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5986, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5764, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5852, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5674, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5836, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5484, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 012 | Train Loss: 0.5580, Acc: 0.6696, F1: 0.7886 | Val Loss: 0.5504, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 013 | Train Loss: 0.5371, Acc: 0.7411, F1: 0.8242 | Val Loss: 0.5349, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 014 | Train Loss: 0.5473, Acc: 0.7054, F1: 0.8156 | Val Loss: 0.5280, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 015 | Train Loss: 0.5307, Acc: 0.7232, F1: 0.8025 | Val Loss: 0.5349, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.5125, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5228, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 017 | Train Loss: 0.5150, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5286, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4994, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5288, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.5026, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5292, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4979, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5266, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4870, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5206, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4860, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.5327, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4999, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5247, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4704, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5214, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 025 | Train Loss: 0.4928, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5415, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4898, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5196, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 027 | Train Loss: 0.4963, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5316, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.5224, Acc: 0.7143, F1: 0.7714 | Val Loss: 0.5165, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4625, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5350, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 030 | Train Loss: 0.4900, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5402, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7331, Acc: 0.6071, F1: 0.7412 | Val Loss: 0.6758, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 002 | Train Loss: 0.6557, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6241, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6394, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6183, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6380, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6158, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6358, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6147, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6371, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6151, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6307, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6061, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6160, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5880, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5871, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5359, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5587, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5251, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 011 | Train Loss: 0.5268, Acc: 0.7500, F1: 0.8333 | Val Loss: 0.6201, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 012 | Train Loss: 0.5470, Acc: 0.6875, F1: 0.7368 | Val Loss: 0.5081, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 013 | Train Loss: 0.5742, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.5056, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 014 | Train Loss: 0.5015, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5988, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 015 | Train Loss: 0.5263, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5070, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 016 | Train Loss: 0.5098, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.5074, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 017 | Train Loss: 0.4903, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5119, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 018 | Train Loss: 0.4809, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5269, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 019 | Train Loss: 0.4949, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5138, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 020 | Train Loss: 0.4514, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5518, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 021 | Train Loss: 0.4837, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5112, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 022 | Train Loss: 0.4815, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5053, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4803, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5067, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.4489, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5301, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.4499, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5213, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4655, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5197, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4624, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5049, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4472, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.4995, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 029 | Train Loss: 0.4606, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5026, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4561, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5186, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 031 | Train Loss: 0.4464, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5010, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4873, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5050, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4387, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5348, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 034 | Train Loss: 0.4615, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5039, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4673, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.4977, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4911, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5110, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 037 | Train Loss: 0.4773, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4989, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 038 | Train Loss: 0.4602, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5282, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 039 | Train Loss: 0.4915, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5004, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 040 | Train Loss: 0.4583, Acc: 0.7946, F1: 0.8589 | Val Loss: 0.5018, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 041 | Train Loss: 0.4760, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5288, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4752, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5256, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 043 | Train Loss: 0.4530, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5530, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 044 | Train Loss: 0.4352, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5235, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4755, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5215, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4658, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5292, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4510, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5046, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 048 | Train Loss: 0.4728, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5082, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 049 | Train Loss: 0.4591, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5031, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 050 | Train Loss: 0.4462, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5232, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 50 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6710, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6561, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6585, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6434, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6466, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6342, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6475, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6266, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6396, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6242, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6369, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6218, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6336, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6197, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6347, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6177, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6313, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6156, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6288, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6139, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6266, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6127, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6331, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6116, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6321, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6106, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6255, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6096, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6280, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6087, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6224, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6078, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6265, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6066, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6188, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6052, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6204, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6040, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6210, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6035, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6167, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6028, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7078, Acc: 0.5089, F1: 0.5926 | Val Loss: 0.6143, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6368, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6061, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6202, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6136, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5902, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6155, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5913, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5999, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5757, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5862, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5697, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5895, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5633, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5730, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5572, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5840, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5658, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.5709, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5488, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 012 | Train Loss: 0.5742, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5455, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 013 | Train Loss: 0.5556, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5434, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5510, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5415, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5491, Acc: 0.6964, F1: 0.8090 | Val Loss: 0.5503, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 016 | Train Loss: 0.5377, Acc: 0.7500, F1: 0.8372 | Val Loss: 0.5372, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 017 | Train Loss: 0.5302, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5358, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 018 | Train Loss: 0.5468, Acc: 0.7143, F1: 0.8202 | Val Loss: 0.5350, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 019 | Train Loss: 0.5005, Acc: 0.7500, F1: 0.8372 | Val Loss: 0.5338, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 020 | Train Loss: 0.5303, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5314, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 021 | Train Loss: 0.5156, Acc: 0.7857, F1: 0.8571 | Val Loss: 0.5314, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 022 | Train Loss: 0.5102, Acc: 0.7500, F1: 0.8353 | Val Loss: 0.5281, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 023 | Train Loss: 0.5179, Acc: 0.7500, F1: 0.8333 | Val Loss: 0.5262, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 024 | Train Loss: 0.5195, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5283, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.4978, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5343, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 026 | Train Loss: 0.5093, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5480, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 027 | Train Loss: 0.5109, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5417, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4975, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5396, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4935, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5389, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.4818, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5403, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5236, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5512, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 032 | Train Loss: 0.5077, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5374, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 033 | Train Loss: 0.4953, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5343, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 034 | Train Loss: 0.4915, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5347, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.4891, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5319, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 036 | Train Loss: 0.5025, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5325, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 037 | Train Loss: 0.4656, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5328, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running sage hidden=32, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7891, Acc: 0.6250, F1: 0.7667 | Val Loss: 0.6659, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6639, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6426, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6418, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6130, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6393, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6126, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6326, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6031, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6241, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6098, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6141, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5860, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6033, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5708, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5808, Acc: 0.6964, F1: 0.8111 | Val Loss: 0.5611, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5882, Acc: 0.6786, F1: 0.7955 | Val Loss: 0.5438, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5569, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.5670, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 012 | Train Loss: 0.5468, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5597, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 013 | Train Loss: 0.6190, Acc: 0.6964, F1: 0.8111 | Val Loss: 0.5531, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.5843, Acc: 0.6786, F1: 0.7353 | Val Loss: 0.5478, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.5396, Acc: 0.6875, F1: 0.7879 | Val Loss: 0.5628, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 016 | Train Loss: 0.5656, Acc: 0.7054, F1: 0.8156 | Val Loss: 0.5278, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 017 | Train Loss: 0.5387, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5246, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5017, Acc: 0.7232, F1: 0.8121 | Val Loss: 0.5246, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 019 | Train Loss: 0.5087, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5377, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4960, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5212, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 021 | Train Loss: 0.5326, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5214, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4669, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5186, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 023 | Train Loss: 0.4635, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5208, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.4733, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5488, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 025 | Train Loss: 0.5126, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5328, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4791, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5304, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4916, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5146, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4842, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5114, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4904, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5047, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 030 | Train Loss: 0.4769, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5405, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 031 | Train Loss: 0.4804, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5147, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4686, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5365, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 033 | Train Loss: 0.4913, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5314, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 034 | Train Loss: 0.5247, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5251, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 035 | Train Loss: 0.4860, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5253, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 036 | Train Loss: 0.4969, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5139, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 037 | Train Loss: 0.4523, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5316, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 038 | Train Loss: 0.4733, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5191, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 039 | Train Loss: 0.4848, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5315, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.5190, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5452, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 041 | Train Loss: 0.4668, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5217, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 042 | Train Loss: 0.5082, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5143, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 043 | Train Loss: 0.5019, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5562, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 044 | Train Loss: 0.4784, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5192, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 045 | Train Loss: 0.4545, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5276, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 046 | Train Loss: 0.5034, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5336, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4639, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5269, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 048 | Train Loss: 0.4772, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5143, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 049 | Train Loss: 0.4646, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5150, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 050 | Train Loss: 0.4571, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5176, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 50 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7177, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6931, Acc: 0.5000, F1: 0.5366\n",
      "Epoch 002 | Train Loss: 0.6881, Acc: 0.5446, F1: 0.6832 | Val Loss: 0.6630, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6605, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6423, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6465, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6277, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6359, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6199, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6348, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6162, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6329, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6144, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6317, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6128, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6301, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6109, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6286, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6095, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6258, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6076, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6246, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6061, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6046, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6190, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6025, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6170, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6002, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6145, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5976, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6117, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5958, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6089, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5931, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6083, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5915, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6025, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5875, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5989, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5834, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.5968, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5793, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 22 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6848, Acc: 0.5000, F1: 0.6364 | Val Loss: 0.6182, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6336, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6137, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6262, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5978, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6122, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5871, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5946, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5662, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5809, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5485, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.5618, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5332, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 008 | Train Loss: 0.5690, Acc: 0.6786, F1: 0.8000 | Val Loss: 0.5312, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.4927, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.6272, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 010 | Train Loss: 0.5705, Acc: 0.7143, F1: 0.7714 | Val Loss: 0.5171, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 011 | Train Loss: 0.5035, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5147, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 012 | Train Loss: 0.4951, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5306, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.4895, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5170, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 014 | Train Loss: 0.4919, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5312, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4977, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5225, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4709, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5180, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 017 | Train Loss: 0.4968, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5136, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 018 | Train Loss: 0.4682, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5241, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4678, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5092, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 020 | Train Loss: 0.4678, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5217, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.4616, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5093, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4632, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5102, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.5025, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5074, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 024 | Train Loss: 0.4698, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5136, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4780, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5064, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4572, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5110, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4540, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5093, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4534, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5034, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4543, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5092, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4575, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5312, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 031 | Train Loss: 0.4438, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5064, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4700, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5067, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4499, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5537, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 034 | Train Loss: 0.4597, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5157, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 035 | Train Loss: 0.4473, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5061, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4481, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5042, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4535, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5022, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4413, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5207, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 039 | Train Loss: 0.4471, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5144, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 040 | Train Loss: 0.4404, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4999, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4450, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5017, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4530, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5327, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 043 | Train Loss: 0.4462, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5102, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 044 | Train Loss: 0.4486, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5045, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 045 | Train Loss: 0.4433, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5115, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4406, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5194, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 047 | Train Loss: 0.4334, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5035, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4384, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5033, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4430, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5052, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 050 | Train Loss: 0.4294, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5332, Acc: 0.7895, F1: 0.8462\n",
      "\n",
      "Early stopping triggered after 50 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.0004, Acc: 0.6071, F1: 0.7412 | Val Loss: 0.6884, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 002 | Train Loss: 0.6599, Acc: 0.6339, F1: 0.7545 | Val Loss: 0.6140, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6289, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6086, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6211, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5909, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6027, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5674, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5981, Acc: 0.7054, F1: 0.8114 | Val Loss: 0.5981, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6060, Acc: 0.6518, F1: 0.7845 | Val Loss: 0.6049, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 008 | Train Loss: 0.5998, Acc: 0.7232, F1: 0.7832 | Val Loss: 0.5449, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5662, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5290, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 010 | Train Loss: 0.6139, Acc: 0.6786, F1: 0.7353 | Val Loss: 0.5448, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 011 | Train Loss: 0.5540, Acc: 0.7411, F1: 0.8284 | Val Loss: 0.5525, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5222, Acc: 0.7411, F1: 0.8242 | Val Loss: 0.5956, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 013 | Train Loss: 0.5501, Acc: 0.7232, F1: 0.7832 | Val Loss: 0.5204, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 014 | Train Loss: 0.5207, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.5264, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.5073, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5444, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.4904, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5243, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 017 | Train Loss: 0.4901, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5360, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4877, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5262, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 019 | Train Loss: 0.5020, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5311, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.5081, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5168, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.5082, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5123, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 022 | Train Loss: 0.4951, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5156, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.4793, Acc: 0.7500, F1: 0.8333 | Val Loss: 0.5500, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 024 | Train Loss: 0.4878, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5229, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.4704, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5243, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4753, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5212, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4478, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5230, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4461, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5149, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4609, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5251, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4594, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5740, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 031 | Train Loss: 0.4507, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5232, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 032 | Train Loss: 0.4535, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5191, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4435, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5273, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4448, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5193, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4404, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5454, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 036 | Train Loss: 0.4424, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5189, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4489, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5357, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 038 | Train Loss: 0.4464, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5125, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4482, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5181, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4444, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5129, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 041 | Train Loss: 0.4423, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5248, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4433, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5392, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 043 | Train Loss: 0.4425, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5184, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 044 | Train Loss: 0.4434, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5158, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4406, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5496, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 046 | Train Loss: 0.4426, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5179, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4384, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5163, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4330, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5246, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4332, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5356, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 050 | Train Loss: 0.4419, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5301, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 50 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7172, Acc: 0.3571, F1: 0.0769 | Val Loss: 0.6813, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6741, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6489, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6501, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6297, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6359, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6204, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6342, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6162, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6326, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6143, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6328, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6118, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6289, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6091, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6261, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6061, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6222, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6043, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6170, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6020, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6161, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6011, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6112, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5972, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6091, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5915, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6044, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5886, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5984, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5832, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5934, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5776, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5888, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5732, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5819, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5698, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5775, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5652, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 021 | Train Loss: 0.5714, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5604, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 022 | Train Loss: 0.5648, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5540, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 023 | Train Loss: 0.5575, Acc: 0.7232, F1: 0.8268 | Val Loss: 0.5479, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 024 | Train Loss: 0.5505, Acc: 0.7411, F1: 0.8362 | Val Loss: 0.5439, Acc: 0.7632, F1: 0.8525\n",
      "Epoch 025 | Train Loss: 0.5464, Acc: 0.7768, F1: 0.8521 | Val Loss: 0.5426, Acc: 0.7632, F1: 0.8475\n",
      "Epoch 026 | Train Loss: 0.5406, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5390, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 027 | Train Loss: 0.5340, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5327, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 028 | Train Loss: 0.5252, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5259, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 029 | Train Loss: 0.5206, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5213, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 030 | Train Loss: 0.5167, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5202, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 031 | Train Loss: 0.5106, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5161, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 032 | Train Loss: 0.5079, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5135, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 033 | Train Loss: 0.5002, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5171, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.5027, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5212, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 035 | Train Loss: 0.4944, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5095, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 036 | Train Loss: 0.4956, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5074, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 037 | Train Loss: 0.4880, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5104, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 038 | Train Loss: 0.4879, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5140, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 039 | Train Loss: 0.4862, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5069, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 040 | Train Loss: 0.4806, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5070, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 041 | Train Loss: 0.4794, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5073, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 042 | Train Loss: 0.4786, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5145, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 043 | Train Loss: 0.4781, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5037, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4761, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.4999, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 44 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6302, Acc: 0.6696, F1: 0.7483 | Val Loss: 0.6765, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6922, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6073, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6200, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6153, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6195, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6010, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6045, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5832, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5915, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5633, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.5735, Acc: 0.6696, F1: 0.7956 | Val Loss: 0.5436, Acc: 0.7632, F1: 0.8525\n",
      "Epoch 008 | Train Loss: 0.5637, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5366, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5162, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5915, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 010 | Train Loss: 0.5415, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5294, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 011 | Train Loss: 0.5253, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5245, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 012 | Train Loss: 0.5173, Acc: 0.7143, F1: 0.7808 | Val Loss: 0.5483, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 013 | Train Loss: 0.5093, Acc: 0.7143, F1: 0.7895 | Val Loss: 0.5264, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 014 | Train Loss: 0.5056, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5250, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.4917, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5421, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.5082, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5165, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 017 | Train Loss: 0.4857, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5241, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4839, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5108, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 019 | Train Loss: 0.4770, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5100, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 020 | Train Loss: 0.4926, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5166, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.4717, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5069, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.4710, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5111, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 023 | Train Loss: 0.4627, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5254, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.4625, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5050, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4559, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5047, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4710, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5096, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4573, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5043, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 27 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7632, Acc: 0.6339, F1: 0.7285 | Val Loss: 0.6635, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6674, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6039, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6388, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6177, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6365, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5913, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6090, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5812, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5926, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5639, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5614, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5593, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 008 | Train Loss: 0.5441, Acc: 0.7232, F1: 0.8187 | Val Loss: 0.7308, Acc: 0.6579, F1: 0.6829\n",
      "Epoch 009 | Train Loss: 0.6370, Acc: 0.6964, F1: 0.7639 | Val Loss: 0.5563, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 010 | Train Loss: 0.5424, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5949, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 011 | Train Loss: 0.5511, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5393, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 012 | Train Loss: 0.5291, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5341, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.5078, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5268, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 014 | Train Loss: 0.5074, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5323, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.5019, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5401, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4898, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5422, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.4915, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5371, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 018 | Train Loss: 0.5176, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5464, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4821, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5234, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.5185, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5473, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 021 | Train Loss: 0.5360, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5153, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 022 | Train Loss: 0.5071, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.5588, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 023 | Train Loss: 0.5084, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5213, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.4669, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5306, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 025 | Train Loss: 0.4591, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5468, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4654, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5370, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 027 | Train Loss: 0.4587, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5645, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 028 | Train Loss: 0.5122, Acc: 0.7589, F1: 0.8000 | Val Loss: 0.6900, Acc: 0.6842, F1: 0.8065\n",
      "Epoch 029 | Train Loss: 0.5532, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.6111, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 030 | Train Loss: 0.5242, Acc: 0.7768, F1: 0.8175 | Val Loss: 0.5291, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 031 | Train Loss: 0.4853, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5208, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 032 | Train Loss: 0.4368, Acc: 0.8036, F1: 0.8608 | Val Loss: 0.5676, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 033 | Train Loss: 0.4598, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5345, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4683, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5285, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 035 | Train Loss: 0.4368, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5629, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 036 | Train Loss: 0.4513, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5510, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 037 | Train Loss: 0.4325, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5324, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 038 | Train Loss: 0.4528, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5313, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4375, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5260, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.4467, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5179, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 041 | Train Loss: 0.4426, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5167, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 042 | Train Loss: 0.4437, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5301, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 043 | Train Loss: 0.4480, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5234, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4318, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5359, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 045 | Train Loss: 0.4410, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5269, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4349, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5191, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 047 | Train Loss: 0.4355, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5508, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 048 | Train Loss: 0.4566, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.5328, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 049 | Train Loss: 0.5209, Acc: 0.7500, F1: 0.8391 | Val Loss: 0.5475, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 050 | Train Loss: 0.4624, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5612, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 051 | Train Loss: 0.4505, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5317, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 052 | Train Loss: 0.4355, Acc: 0.8036, F1: 0.8608 | Val Loss: 0.5360, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 053 | Train Loss: 0.4477, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5366, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 054 | Train Loss: 0.4923, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5302, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 055 | Train Loss: 0.4286, Acc: 0.8393, F1: 0.8816 | Val Loss: 0.6017, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 056 | Train Loss: 0.4813, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.5332, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 56 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6644, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6384, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6442, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6216, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6341, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6154, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6338, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6123, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6297, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6103, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6278, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6087, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6257, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6067, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6235, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6046, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6212, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6024, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6183, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6014, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6147, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6005, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6178, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5998, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6101, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5953, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6092, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5893, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6071, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5859, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6030, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5829, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6001, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5816, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5930, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5769, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5900, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5715, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5853, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5675, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5827, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5628, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6998, Acc: 0.5000, F1: 0.6364 | Val Loss: 0.6125, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6335, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6012, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6210, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5913, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6040, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5838, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6007, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5737, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5800, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5596, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5836, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5521, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5639, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5433, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5426, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5404, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.5384, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5300, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 011 | Train Loss: 0.5353, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5266, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 012 | Train Loss: 0.5060, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5352, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.5348, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5335, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.4925, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5359, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 015 | Train Loss: 0.4949, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5527, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.5021, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5368, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4973, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5338, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4863, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5336, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.4930, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5332, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4844, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5296, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4775, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5265, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 022 | Train Loss: 0.4792, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5343, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4789, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5274, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4759, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5325, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4790, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5274, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 026 | Train Loss: 0.4873, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5365, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4748, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5251, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 028 | Train Loss: 0.4870, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5243, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 029 | Train Loss: 0.4658, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5397, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4557, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5244, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4687, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5278, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 032 | Train Loss: 0.4734, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5287, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 033 | Train Loss: 0.4471, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5367, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 034 | Train Loss: 0.4702, Acc: 0.8125, F1: 0.8662 | Val Loss: 0.5628, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 035 | Train Loss: 0.4714, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5263, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 036 | Train Loss: 0.4663, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5243, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4514, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5295, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4512, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5290, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4450, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5325, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 040 | Train Loss: 0.4558, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5209, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 041 | Train Loss: 0.4534, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5346, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 042 | Train Loss: 0.4510, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5192, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4446, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5244, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4439, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5405, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 045 | Train Loss: 0.4418, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5263, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4420, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5248, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4608, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5265, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4715, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5390, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 049 | Train Loss: 0.4809, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5446, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 050 | Train Loss: 0.4481, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5235, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 051 | Train Loss: 0.4390, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5237, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4694, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5322, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 053 | Train Loss: 0.4421, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5277, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 054 | Train Loss: 0.4305, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5364, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 055 | Train Loss: 0.4347, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5254, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 056 | Train Loss: 0.4443, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5250, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 057 | Train Loss: 0.4299, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5334, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 058 | Train Loss: 0.4403, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5432, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 059 | Train Loss: 0.4434, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5284, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 59 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.0113, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6700, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6567, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6225, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6388, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6153, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6368, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6002, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6141, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5785, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5759, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5504, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.6131, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.7725, Acc: 0.6316, F1: 0.6667\n",
      "Epoch 008 | Train Loss: 0.6761, Acc: 0.5982, F1: 0.6154 | Val Loss: 0.5519, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5810, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5378, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5528, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5456, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 011 | Train Loss: 0.5278, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5246, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 012 | Train Loss: 0.5278, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5344, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.5004, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5341, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.4871, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5450, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 015 | Train Loss: 0.4936, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5979, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 016 | Train Loss: 0.5226, Acc: 0.7411, F1: 0.7914 | Val Loss: 0.5230, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.5153, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5141, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 018 | Train Loss: 0.4629, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5789, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 019 | Train Loss: 0.5345, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5034, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 020 | Train Loss: 0.4708, Acc: 0.7857, F1: 0.8554 | Val Loss: 0.5098, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 021 | Train Loss: 0.4802, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5180, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4770, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5187, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.4823, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5347, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 024 | Train Loss: 0.4676, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5329, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4567, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5413, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4428, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5293, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4442, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5361, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4501, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5369, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4566, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5242, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4561, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5369, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 031 | Train Loss: 0.4398, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5178, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 032 | Train Loss: 0.4489, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5254, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4775, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5381, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 034 | Train Loss: 0.4524, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5091, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4480, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5118, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4432, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5093, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 037 | Train Loss: 0.4354, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5160, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4465, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5291, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 039 | Train Loss: 0.4511, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5230, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4387, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5333, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 041 | Train Loss: 0.4461, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5317, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4617, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5206, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4331, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5410, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 044 | Train Loss: 0.4482, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5284, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4447, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5207, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 046 | Train Loss: 0.4543, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5320, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4434, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5366, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 048 | Train Loss: 0.4426, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5153, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 049 | Train Loss: 0.4463, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5097, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 050 | Train Loss: 0.4476, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5112, Acc: 0.7368, F1: 0.8214\n",
      "\n",
      "Early stopping triggered after 50 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7136, Acc: 0.3304, F1: 0.0000 | Val Loss: 0.6850, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6769, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6463, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6525, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6249, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6355, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6170, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6326, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6141, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6356, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6127, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6322, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6105, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6305, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6094, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6267, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6092, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6238, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6094, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6242, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6075, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6063, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6183, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6027, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6165, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5979, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6115, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5937, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6071, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5904, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6059, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5872, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6014, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5839, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5981, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5808, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5935, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5763, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5884, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5719, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6722, Acc: 0.6071, F1: 0.7143 | Val Loss: 0.6111, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6410, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6414, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6358, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6001, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6160, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5932, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6067, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5847, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6125, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5940, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5856, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5654, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5905, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5622, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5754, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5673, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5716, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5450, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.5505, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5368, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5392, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5309, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 013 | Train Loss: 0.5341, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5322, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.5131, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5338, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 015 | Train Loss: 0.5330, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5420, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.5019, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5335, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.5172, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5409, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.5039, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5523, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 019 | Train Loss: 0.5086, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5376, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4961, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5431, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.5004, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5335, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.4998, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5404, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4921, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5336, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 024 | Train Loss: 0.4998, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5352, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.5063, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5314, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.4962, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5431, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.5044, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5317, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 028 | Train Loss: 0.4890, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5361, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4946, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5286, Acc: 0.7105, F1: 0.7925\n",
      "\n",
      "Early stopping triggered after 29 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.8456, Acc: 0.5357, F1: 0.6232 | Val Loss: 0.6697, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6372, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6223, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6299, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6126, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6212, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5875, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5938, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5676, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5829, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5617, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.6151, Acc: 0.7143, F1: 0.8161 | Val Loss: 0.5524, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5761, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5478, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5595, Acc: 0.7054, F1: 0.8136 | Val Loss: 0.5696, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 010 | Train Loss: 0.5471, Acc: 0.7232, F1: 0.8208 | Val Loss: 0.5414, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5258, Acc: 0.7589, F1: 0.8421 | Val Loss: 0.5319, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 012 | Train Loss: 0.5103, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5379, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.5352, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5801, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 014 | Train Loss: 0.5143, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5384, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.5048, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5511, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 016 | Train Loss: 0.5027, Acc: 0.8036, F1: 0.8608 | Val Loss: 0.5282, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.4838, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5355, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 018 | Train Loss: 0.5122, Acc: 0.8036, F1: 0.8625 | Val Loss: 0.5321, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.5356, Acc: 0.7768, F1: 0.8201 | Val Loss: 0.5311, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 020 | Train Loss: 0.5343, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5221, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.4896, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5448, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 022 | Train Loss: 0.4868, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5449, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.4821, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5385, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 024 | Train Loss: 0.4726, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5392, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.5146, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5378, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.4881, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5855, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 027 | Train Loss: 0.5405, Acc: 0.7411, F1: 0.7883 | Val Loss: 0.5313, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 028 | Train Loss: 0.4953, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5308, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4848, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5294, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 030 | Train Loss: 0.4717, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5261, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 031 | Train Loss: 0.4777, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5314, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 032 | Train Loss: 0.4809, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5360, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 033 | Train Loss: 0.4786, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5300, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 034 | Train Loss: 0.4640, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5399, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 035 | Train Loss: 0.4835, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5494, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 036 | Train Loss: 0.5417, Acc: 0.7143, F1: 0.7647 | Val Loss: 0.5305, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 037 | Train Loss: 0.4867, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5349, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 038 | Train Loss: 0.4925, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5235, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 039 | Train Loss: 0.5155, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5275, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 39 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6867, Acc: 0.5268, F1: 0.6581 | Val Loss: 0.6525, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6593, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6248, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6355, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6152, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6326, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6117, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6278, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6097, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6283, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6088, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6240, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6071, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6233, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6060, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6232, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6047, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6211, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6032, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6210, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6015, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6126, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5976, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6102, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5944, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6084, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5917, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6060, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5882, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6029, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5849, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6021, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5819, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5923, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5793, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5893, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5792, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5845, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5782, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 021 | Train Loss: 0.5856, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5737, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 022 | Train Loss: 0.5790, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5697, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 023 | Train Loss: 0.5748, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5641, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 024 | Train Loss: 0.5687, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5586, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 025 | Train Loss: 0.5639, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5549, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 026 | Train Loss: 0.5624, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5529, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 027 | Train Loss: 0.5535, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5497, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 028 | Train Loss: 0.5535, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.5488, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 029 | Train Loss: 0.5475, Acc: 0.7232, F1: 0.8144 | Val Loss: 0.5461, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 030 | Train Loss: 0.5455, Acc: 0.7232, F1: 0.8098 | Val Loss: 0.5414, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 031 | Train Loss: 0.5400, Acc: 0.7054, F1: 0.8092 | Val Loss: 0.5411, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 032 | Train Loss: 0.5351, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.5369, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 033 | Train Loss: 0.5321, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5357, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 034 | Train Loss: 0.5246, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5361, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 035 | Train Loss: 0.5212, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5350, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 036 | Train Loss: 0.5159, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5333, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 037 | Train Loss: 0.5193, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5320, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 038 | Train Loss: 0.5244, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5307, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 039 | Train Loss: 0.5078, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5328, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 040 | Train Loss: 0.5174, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5358, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 041 | Train Loss: 0.5115, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5292, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 042 | Train Loss: 0.5144, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5284, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 043 | Train Loss: 0.5129, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5286, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 044 | Train Loss: 0.4986, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5284, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 045 | Train Loss: 0.4936, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5269, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 046 | Train Loss: 0.5017, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5263, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 047 | Train Loss: 0.5020, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5289, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 048 | Train Loss: 0.4914, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5263, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 48 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6591, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6114, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6242, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6172, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6218, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5950, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6194, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5881, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6053, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5880, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6026, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5743, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5806, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5574, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5639, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5446, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5511, Acc: 0.7321, F1: 0.8214 | Val Loss: 0.5337, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 010 | Train Loss: 0.5303, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5245, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 011 | Train Loss: 0.5121, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5280, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.5336, Acc: 0.7232, F1: 0.8050 | Val Loss: 0.5274, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.5107, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5349, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.5042, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5267, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.4944, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5387, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.4889, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5198, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 017 | Train Loss: 0.4840, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5261, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4864, Acc: 0.7321, F1: 0.7887 | Val Loss: 0.5227, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4985, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5126, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.5066, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5483, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 021 | Train Loss: 0.4858, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5196, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 022 | Train Loss: 0.5186, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5154, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 023 | Train Loss: 0.5412, Acc: 0.7411, F1: 0.7914 | Val Loss: 0.5412, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 024 | Train Loss: 0.4672, Acc: 0.8214, F1: 0.8701 | Val Loss: 0.5404, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 025 | Train Loss: 0.5152, Acc: 0.7857, F1: 0.8554 | Val Loss: 0.5089, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4716, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5297, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4850, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5132, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4733, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5098, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4731, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5096, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 030 | Train Loss: 0.4577, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5414, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 031 | Train Loss: 0.4881, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5169, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4730, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5195, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 033 | Train Loss: 0.4687, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5243, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 034 | Train Loss: 0.4662, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5159, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4440, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5127, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 036 | Train Loss: 0.4546, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5192, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 037 | Train Loss: 0.4522, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5204, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4537, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5207, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 039 | Train Loss: 0.4549, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5235, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4473, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5248, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4500, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5158, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4321, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5189, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 42 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.0438, Acc: 0.5000, F1: 0.5942 | Val Loss: 0.6812, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6381, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6229, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6515, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6028, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6314, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5901, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6174, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6093, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6031, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5576, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5574, Acc: 0.6607, F1: 0.7912 | Val Loss: 0.6092, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 008 | Train Loss: 0.5599, Acc: 0.7232, F1: 0.8000 | Val Loss: 0.5454, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 009 | Train Loss: 0.5422, Acc: 0.7232, F1: 0.7801 | Val Loss: 0.5420, Acc: 0.6579, F1: 0.7719\n",
      "Epoch 010 | Train Loss: 0.5397, Acc: 0.7232, F1: 0.8229 | Val Loss: 0.5358, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 011 | Train Loss: 0.5290, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5447, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 012 | Train Loss: 0.5294, Acc: 0.7143, F1: 0.8000 | Val Loss: 0.5323, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.5334, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5316, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.5191, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5406, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 015 | Train Loss: 0.5369, Acc: 0.7143, F1: 0.7949 | Val Loss: 0.5348, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4879, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5172, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 017 | Train Loss: 0.4874, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.5158, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 018 | Train Loss: 0.4662, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5380, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 019 | Train Loss: 0.4737, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5492, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4822, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5436, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.5104, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5276, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 022 | Train Loss: 0.4853, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5230, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4920, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5390, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 024 | Train Loss: 0.4612, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5213, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4888, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5272, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4725, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5207, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4646, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5422, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4708, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5193, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4611, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5187, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 030 | Train Loss: 0.4697, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5261, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4363, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5252, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 032 | Train Loss: 0.4402, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5336, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4512, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5442, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4522, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5298, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 035 | Train Loss: 0.4639, Acc: 0.8214, F1: 0.8734 | Val Loss: 0.5542, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 036 | Train Loss: 0.4755, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5183, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4619, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5125, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 038 | Train Loss: 0.4598, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5274, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 039 | Train Loss: 0.4455, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5199, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.4575, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5175, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 041 | Train Loss: 0.4289, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5209, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4544, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5227, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 043 | Train Loss: 0.4441, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5408, Acc: 0.7895, F1: 0.8400\n",
      "\n",
      "Early stopping triggered after 43 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6680, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6424, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6461, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6278, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6375, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6183, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6355, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6142, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6319, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6121, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6340, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6103, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6324, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6094, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6233, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6086, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6289, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6081, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6220, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6059, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6201, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6027, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6229, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6004, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6231, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5981, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6153, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5947, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6051, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5914, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6129, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5880, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6011, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5845, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5970, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5817, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5969, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5780, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5897, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5741, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5890, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5699, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6580, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6166, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6270, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6054, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6325, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5970, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6195, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5958, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6026, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5797, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5932, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5640, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5983, Acc: 0.6607, F1: 0.7791 | Val Loss: 0.5507, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5438, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5485, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5455, Acc: 0.7143, F1: 0.8161 | Val Loss: 0.5415, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 010 | Train Loss: 0.5281, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5343, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 011 | Train Loss: 0.5217, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5826, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 012 | Train Loss: 0.5313, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5368, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.4985, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5328, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.4887, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5255, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 015 | Train Loss: 0.5329, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5256, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4948, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5235, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.4864, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5193, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 018 | Train Loss: 0.4805, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5373, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4998, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5205, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.4903, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5242, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 021 | Train Loss: 0.4700, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5309, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4740, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5370, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4723, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5159, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.4886, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5195, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4597, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5190, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4547, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5272, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4856, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5164, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4458, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5181, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4733, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5239, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 030 | Train Loss: 0.4535, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5218, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4760, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5254, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 032 | Train Loss: 0.4662, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5324, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4597, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5324, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4589, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5293, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4644, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5410, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4409, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5333, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 037 | Train Loss: 0.4541, Acc: 0.8214, F1: 0.8701 | Val Loss: 0.5290, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 038 | Train Loss: 0.4513, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5301, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 039 | Train Loss: 0.4307, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5415, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4489, Acc: 0.8304, F1: 0.8742 | Val Loss: 0.5286, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4609, Acc: 0.8036, F1: 0.8625 | Val Loss: 0.5304, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4274, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5935, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 043 | Train Loss: 0.4673, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.5280, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 044 | Train Loss: 0.4749, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5275, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 045 | Train Loss: 0.4684, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5721, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 046 | Train Loss: 0.4634, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5174, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 047 | Train Loss: 0.4746, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5184, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 048 | Train Loss: 0.4250, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5258, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4437, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5342, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 050 | Train Loss: 0.4441, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5295, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 051 | Train Loss: 0.4449, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5327, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 052 | Train Loss: 0.4852, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5311, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 053 | Train Loss: 0.4331, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5461, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 054 | Train Loss: 0.4717, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5307, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 055 | Train Loss: 0.4713, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5351, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 056 | Train Loss: 0.4683, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5265, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 057 | Train Loss: 0.4721, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5238, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 058 | Train Loss: 0.4478, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.5523, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 059 | Train Loss: 0.4516, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5236, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 060 | Train Loss: 0.4473, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5231, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 061 | Train Loss: 0.4483, Acc: 0.8125, F1: 0.8662 | Val Loss: 0.5346, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 062 | Train Loss: 0.4475, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5240, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 62 epochs.\n",
      "\n",
      ">>> Running sage hidden=64, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.8088, Acc: 0.4911, F1: 0.4673 | Val Loss: 0.6580, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6485, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6087, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6270, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5997, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6087, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5936, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6036, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5733, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5776, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5510, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 007 | Train Loss: 0.5325, Acc: 0.7054, F1: 0.8136 | Val Loss: 0.6576, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 008 | Train Loss: 0.6155, Acc: 0.6875, F1: 0.7799 | Val Loss: 0.5408, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 009 | Train Loss: 0.5297, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5571, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 010 | Train Loss: 0.5177, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5267, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 011 | Train Loss: 0.5128, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5366, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 012 | Train Loss: 0.5119, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5314, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.4986, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5163, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 014 | Train Loss: 0.4881, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5175, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 015 | Train Loss: 0.4992, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5295, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 016 | Train Loss: 0.4777, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5386, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4669, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5280, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 018 | Train Loss: 0.5026, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5482, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 019 | Train Loss: 0.4824, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5182, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 020 | Train Loss: 0.5038, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.5404, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 021 | Train Loss: 0.4701, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5331, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 022 | Train Loss: 0.4700, Acc: 0.7946, F1: 0.8589 | Val Loss: 0.5230, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.4908, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5315, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 024 | Train Loss: 0.4791, Acc: 0.7857, F1: 0.8571 | Val Loss: 0.6572, Acc: 0.6579, F1: 0.6977\n",
      "Epoch 025 | Train Loss: 0.6554, Acc: 0.5625, F1: 0.5586 | Val Loss: 0.5187, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 026 | Train Loss: 0.4932, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5324, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 027 | Train Loss: 0.4802, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5479, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 028 | Train Loss: 0.5171, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5219, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4578, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5328, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 030 | Train Loss: 0.5068, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5188, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 031 | Train Loss: 0.4448, Acc: 0.8125, F1: 0.8696 | Val Loss: 0.5189, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 032 | Train Loss: 0.4862, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.5310, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 033 | Train Loss: 0.4553, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5282, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 034 | Train Loss: 0.4712, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5283, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 035 | Train Loss: 0.4504, Acc: 0.8125, F1: 0.8662 | Val Loss: 0.5323, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 036 | Train Loss: 0.4853, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5292, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4592, Acc: 0.8036, F1: 0.8608 | Val Loss: 0.5191, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 038 | Train Loss: 0.4602, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5178, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4621, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5189, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 040 | Train Loss: 0.4895, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5043, Acc: 0.7895, F1: 0.8519\n",
      "\n",
      "Early stopping triggered after 40 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6776, Acc: 0.5982, F1: 0.6853 | Val Loss: 0.6255, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6421, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6155, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6411, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6132, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6329, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6074, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6232, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6056, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6207, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6061, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6169, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6023, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6136, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5958, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6036, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5864, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5992, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5816, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5960, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5761, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5879, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5711, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5821, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5706, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5735, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5614, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5647, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5536, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 016 | Train Loss: 0.5577, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5477, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 017 | Train Loss: 0.5500, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5427, Acc: 0.7632, F1: 0.8525\n",
      "Epoch 018 | Train Loss: 0.5394, Acc: 0.7321, F1: 0.8256 | Val Loss: 0.5425, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 019 | Train Loss: 0.5380, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5396, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 020 | Train Loss: 0.5275, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5311, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 021 | Train Loss: 0.5255, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.5284, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 022 | Train Loss: 0.5162, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5319, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 023 | Train Loss: 0.5130, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5310, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.5082, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5225, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 025 | Train Loss: 0.5073, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5208, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 026 | Train Loss: 0.5066, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5302, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.5002, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5233, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4967, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5170, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 029 | Train Loss: 0.4948, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5184, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4874, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5153, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 031 | Train Loss: 0.4852, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5130, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 032 | Train Loss: 0.4837, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5140, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 033 | Train Loss: 0.4797, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5122, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 034 | Train Loss: 0.4784, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5099, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4772, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5112, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 036 | Train Loss: 0.4765, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5085, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4771, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5092, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.8282, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6178, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.7263, Acc: 0.3929, F1: 0.3585 | Val Loss: 0.6625, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 003 | Train Loss: 0.6340, Acc: 0.6786, F1: 0.8000 | Val Loss: 0.5957, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6141, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5874, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6048, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5742, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5803, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5507, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 007 | Train Loss: 0.5499, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.5550, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 008 | Train Loss: 0.5653, Acc: 0.7143, F1: 0.7714 | Val Loss: 0.5219, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 009 | Train Loss: 0.5829, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5200, Acc: 0.7632, F1: 0.8525\n",
      "Epoch 010 | Train Loss: 0.5418, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5998, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 011 | Train Loss: 0.5454, Acc: 0.7143, F1: 0.7838 | Val Loss: 0.5156, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 012 | Train Loss: 0.5190, Acc: 0.7857, F1: 0.8588 | Val Loss: 0.5040, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 013 | Train Loss: 0.4860, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5460, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 014 | Train Loss: 0.5064, Acc: 0.7321, F1: 0.7887 | Val Loss: 0.5059, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 015 | Train Loss: 0.4949, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5078, Acc: 0.7632, F1: 0.8421\n",
      "Epoch 016 | Train Loss: 0.4947, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5134, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4734, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5072, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 018 | Train Loss: 0.4717, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5106, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 019 | Train Loss: 0.4663, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5024, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 020 | Train Loss: 0.4643, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5031, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 021 | Train Loss: 0.4675, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5195, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4630, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5022, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.4570, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5064, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 024 | Train Loss: 0.4507, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5055, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.4518, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5084, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4449, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5005, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4518, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5023, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4466, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5108, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4422, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5055, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 29 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.8584, Acc: 0.4821, F1: 0.5246 | Val Loss: 0.6431, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6318, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6522, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6588, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6264, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6438, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6282, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6362, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6062, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6238, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6048, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.6367, Acc: 0.6429, F1: 0.7647 | Val Loss: 0.5713, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5880, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5592, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5617, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5350, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5433, Acc: 0.6964, F1: 0.8000 | Val Loss: 0.6008, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 011 | Train Loss: 0.6143, Acc: 0.6875, F1: 0.7368 | Val Loss: 0.5408, Acc: 0.6316, F1: 0.7500\n",
      "Epoch 012 | Train Loss: 0.5862, Acc: 0.7232, F1: 0.8249 | Val Loss: 0.5335, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.5743, Acc: 0.7143, F1: 0.7612 | Val Loss: 0.5494, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.5140, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5313, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 015 | Train Loss: 0.5329, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5257, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.5076, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5258, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4891, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5224, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 018 | Train Loss: 0.4915, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5294, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4752, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5302, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 020 | Train Loss: 0.4644, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5288, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 021 | Train Loss: 0.4779, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5391, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4555, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.5509, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 023 | Train Loss: 0.4397, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5242, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 024 | Train Loss: 0.4719, Acc: 0.7321, F1: 0.8235 | Val Loss: 0.5259, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 025 | Train Loss: 0.4407, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5477, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 026 | Train Loss: 0.4582, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5280, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4635, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5412, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 028 | Train Loss: 0.4544, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5191, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 029 | Train Loss: 0.4558, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5330, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4514, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5230, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 031 | Train Loss: 0.4616, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5532, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 032 | Train Loss: 0.4436, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5539, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4378, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5574, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4646, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5428, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 035 | Train Loss: 0.4616, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5246, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 036 | Train Loss: 0.4353, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5458, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 037 | Train Loss: 0.4781, Acc: 0.8304, F1: 0.8742 | Val Loss: 0.5327, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 038 | Train Loss: 0.4428, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5223, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 039 | Train Loss: 0.4381, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5428, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4475, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5419, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4589, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5281, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 042 | Train Loss: 0.4488, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5684, Acc: 0.7895, F1: 0.8400\n",
      "\n",
      "Early stopping triggered after 42 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6480, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6177, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6365, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6153, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6343, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6097, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6251, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6067, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6189, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6044, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6159, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6000, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6156, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5988, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6074, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5929, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6006, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5863, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5957, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5811, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5919, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5779, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 012 | Train Loss: 0.5826, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5710, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 013 | Train Loss: 0.5777, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5659, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5676, Acc: 0.6964, F1: 0.8111 | Val Loss: 0.5644, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 015 | Train Loss: 0.5641, Acc: 0.7054, F1: 0.8024 | Val Loss: 0.5608, Acc: 0.6053, F1: 0.7273\n",
      "Epoch 016 | Train Loss: 0.5576, Acc: 0.7054, F1: 0.8024 | Val Loss: 0.5538, Acc: 0.6053, F1: 0.7273\n",
      "Epoch 017 | Train Loss: 0.5479, Acc: 0.7054, F1: 0.8024 | Val Loss: 0.5485, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 018 | Train Loss: 0.5437, Acc: 0.7143, F1: 0.8095 | Val Loss: 0.5451, Acc: 0.6053, F1: 0.7273\n",
      "Epoch 019 | Train Loss: 0.5377, Acc: 0.6875, F1: 0.7879 | Val Loss: 0.5449, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 020 | Train Loss: 0.5309, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5443, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 021 | Train Loss: 0.5273, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5378, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 022 | Train Loss: 0.5254, Acc: 0.7054, F1: 0.8000 | Val Loss: 0.5368, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 023 | Train Loss: 0.5217, Acc: 0.7143, F1: 0.8049 | Val Loss: 0.5348, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 024 | Train Loss: 0.5234, Acc: 0.7143, F1: 0.7895 | Val Loss: 0.5457, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 025 | Train Loss: 0.5111, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5292, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 026 | Train Loss: 0.5168, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5284, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 027 | Train Loss: 0.5179, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5381, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 028 | Train Loss: 0.5079, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5244, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 029 | Train Loss: 0.5055, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5257, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.4992, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5231, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 031 | Train Loss: 0.4974, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5221, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 31 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6552, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5977, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6115, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5780, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.5910, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5646, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 004 | Train Loss: 0.5623, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5386, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 005 | Train Loss: 0.5312, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5422, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 006 | Train Loss: 0.5008, Acc: 0.7143, F1: 0.7895 | Val Loss: 0.5730, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 007 | Train Loss: 0.5058, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.8996, Acc: 0.5526, F1: 0.5143\n",
      "Epoch 008 | Train Loss: 0.7042, Acc: 0.6607, F1: 0.7206 | Val Loss: 0.6119, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5655, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5675, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 010 | Train Loss: 0.5186, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5170, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 011 | Train Loss: 0.5026, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5166, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 012 | Train Loss: 0.4964, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5234, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.5055, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5261, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4911, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5158, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 015 | Train Loss: 0.4924, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5178, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.4827, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5182, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 017 | Train Loss: 0.4765, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5310, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4770, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5192, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 019 | Train Loss: 0.4727, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5176, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.4701, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5151, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 021 | Train Loss: 0.4652, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5234, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.4595, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5182, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4550, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5332, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 024 | Train Loss: 0.4534, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5185, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4800, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5166, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4617, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5168, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4724, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5248, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4399, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5233, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4680, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5203, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4427, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5180, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4514, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5171, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4616, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5198, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 033 | Train Loss: 0.4383, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5518, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 034 | Train Loss: 0.4617, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5228, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4411, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5164, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4398, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5185, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 037 | Train Loss: 0.4366, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5288, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 038 | Train Loss: 0.4448, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5207, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4459, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5220, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4452, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5279, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4549, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5203, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 042 | Train Loss: 0.4348, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5452, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 043 | Train Loss: 0.4434, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5219, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 044 | Train Loss: 0.4639, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5218, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 045 | Train Loss: 0.4368, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5338, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 046 | Train Loss: 0.4421, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5202, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4360, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5180, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 048 | Train Loss: 0.4513, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5197, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4995, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5420, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 050 | Train Loss: 0.4481, Acc: 0.8304, F1: 0.8758 | Val Loss: 0.5349, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 051 | Train Loss: 0.4565, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5135, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4568, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5529, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 053 | Train Loss: 0.4446, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5150, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 53 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.3845, Acc: 0.3929, F1: 0.3585 | Val Loss: 0.6648, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6430, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6090, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6439, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5903, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6200, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5880, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5903, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5570, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5532, Acc: 0.6786, F1: 0.8000 | Val Loss: 0.7623, Acc: 0.6579, F1: 0.6977\n",
      "Epoch 007 | Train Loss: 0.6389, Acc: 0.6786, F1: 0.7568 | Val Loss: 0.5469, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 008 | Train Loss: 0.5209, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5792, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 009 | Train Loss: 0.5603, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5381, Acc: 0.6053, F1: 0.7170\n",
      "Epoch 010 | Train Loss: 0.5372, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5508, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 011 | Train Loss: 0.5417, Acc: 0.7232, F1: 0.8098 | Val Loss: 0.5346, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 012 | Train Loss: 0.4903, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.6875, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 013 | Train Loss: 0.5927, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5235, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 014 | Train Loss: 0.4965, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5434, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.4957, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5197, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 016 | Train Loss: 0.4839, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5336, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4993, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5345, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.5367, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5115, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 019 | Train Loss: 0.4974, Acc: 0.7857, F1: 0.8571 | Val Loss: 0.5118, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 020 | Train Loss: 0.4653, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5532, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4662, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5258, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 022 | Train Loss: 0.5014, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5655, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4521, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5202, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.4605, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5132, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4489, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5457, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 026 | Train Loss: 0.4543, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5153, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 027 | Train Loss: 0.4640, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5315, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4781, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5454, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4589, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5475, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4487, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5344, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4485, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5175, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4537, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5287, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4436, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5156, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 034 | Train Loss: 0.4430, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5316, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4445, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5443, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 036 | Train Loss: 0.4445, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5286, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 037 | Train Loss: 0.4386, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5175, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 038 | Train Loss: 0.4568, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5318, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4656, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5432, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4367, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5465, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4477, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5458, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4419, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5314, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4429, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5277, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4523, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5244, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4584, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5271, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 046 | Train Loss: 0.4576, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5598, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 047 | Train Loss: 0.4583, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5403, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4385, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5383, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 049 | Train Loss: 0.4574, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5446, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 050 | Train Loss: 0.4363, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5413, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 051 | Train Loss: 0.4495, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5278, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4425, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5084, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 053 | Train Loss: 0.4450, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5264, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 054 | Train Loss: 0.4459, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5210, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 055 | Train Loss: 0.4399, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5242, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 55 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6541, Acc: 0.6607, F1: 0.7654 | Val Loss: 0.6163, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6388, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6166, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6375, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6098, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6284, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6066, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6219, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6040, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6176, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6048, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6165, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6008, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6103, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5918, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6057, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5866, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6004, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5814, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5953, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5762, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5857, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5736, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 013 | Train Loss: 0.5826, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.5689, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5739, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5601, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5685, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5547, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 016 | Train Loss: 0.5676, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5501, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 017 | Train Loss: 0.5549, Acc: 0.6607, F1: 0.7841 | Val Loss: 0.5504, Acc: 0.6053, F1: 0.7273\n",
      "Epoch 018 | Train Loss: 0.5492, Acc: 0.7054, F1: 0.7975 | Val Loss: 0.5509, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 019 | Train Loss: 0.5461, Acc: 0.6964, F1: 0.7901 | Val Loss: 0.5389, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 020 | Train Loss: 0.5414, Acc: 0.7143, F1: 0.8095 | Val Loss: 0.5356, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 021 | Train Loss: 0.5345, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5390, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.5345, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5364, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.5264, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5293, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 024 | Train Loss: 0.5204, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5269, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 025 | Train Loss: 0.5286, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5350, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 026 | Train Loss: 0.5170, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5232, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 027 | Train Loss: 0.5160, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5248, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 028 | Train Loss: 0.5152, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5196, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 029 | Train Loss: 0.5132, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5345, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 030 | Train Loss: 0.5095, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5199, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 031 | Train Loss: 0.4977, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5159, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 032 | Train Loss: 0.5093, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5147, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 32 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7298, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.6358, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6451, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6150, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6284, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5981, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6166, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5898, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6044, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5755, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5866, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5589, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5623, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5539, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 008 | Train Loss: 0.5420, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5333, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5230, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5519, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 010 | Train Loss: 0.5160, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5467, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 011 | Train Loss: 0.5105, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5416, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 012 | Train Loss: 0.5466, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5265, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.5467, Acc: 0.7143, F1: 0.7714 | Val Loss: 0.5550, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 014 | Train Loss: 0.4951, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5293, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 015 | Train Loss: 0.5408, Acc: 0.7411, F1: 0.8343 | Val Loss: 0.5169, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 016 | Train Loss: 0.4895, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5909, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 017 | Train Loss: 0.5421, Acc: 0.7321, F1: 0.7887 | Val Loss: 0.5197, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 018 | Train Loss: 0.5019, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5203, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 019 | Train Loss: 0.4802, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.5504, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 020 | Train Loss: 0.5257, Acc: 0.7321, F1: 0.7826 | Val Loss: 0.5149, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 021 | Train Loss: 0.5076, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5270, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.4898, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5476, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 023 | Train Loss: 0.4879, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5157, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.4749, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5185, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4687, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.5166, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 026 | Train Loss: 0.4789, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5387, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.4693, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5124, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4611, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5173, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4560, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5356, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 030 | Train Loss: 0.4539, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5173, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4503, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5188, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4467, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5187, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4405, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5230, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4448, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5257, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4420, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5190, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4440, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5176, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 037 | Train Loss: 0.4440, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5257, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4428, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5173, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4426, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5224, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4361, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5171, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4394, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5250, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4362, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5166, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 043 | Train Loss: 0.4420, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5239, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 044 | Train Loss: 0.4311, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5178, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4401, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5177, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4412, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5285, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 047 | Train Loss: 0.4357, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5185, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4401, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5170, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4373, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5275, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 050 | Train Loss: 0.4415, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5171, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 051 | Train Loss: 0.4343, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5223, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 052 | Train Loss: 0.4381, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5218, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 053 | Train Loss: 0.4353, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5172, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 054 | Train Loss: 0.4304, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5140, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 055 | Train Loss: 0.4308, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5177, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 056 | Train Loss: 0.4386, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5177, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 057 | Train Loss: 0.4492, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5170, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 058 | Train Loss: 0.4433, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5275, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 059 | Train Loss: 0.4341, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5146, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 060 | Train Loss: 0.4382, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5189, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 061 | Train Loss: 0.4430, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5152, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 062 | Train Loss: 0.4283, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5342, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 063 | Train Loss: 0.4330, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5248, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 064 | Train Loss: 0.4301, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5177, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 065 | Train Loss: 0.4303, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5220, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 066 | Train Loss: 0.4293, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5171, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 067 | Train Loss: 0.4256, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5195, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 068 | Train Loss: 0.4351, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5198, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 069 | Train Loss: 0.4314, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5230, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 070 | Train Loss: 0.4394, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5123, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 071 | Train Loss: 0.4311, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5222, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 072 | Train Loss: 0.4356, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5145, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 073 | Train Loss: 0.4313, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5146, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 074 | Train Loss: 0.4328, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5261, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 075 | Train Loss: 0.4419, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5175, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 076 | Train Loss: 0.4307, Acc: 0.8125, F1: 0.8679 | Val Loss: 0.5204, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 077 | Train Loss: 0.4359, Acc: 0.8036, F1: 0.8608 | Val Loss: 0.5126, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 078 | Train Loss: 0.4310, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5305, Acc: 0.7895, F1: 0.8400\n",
      "\n",
      "Early stopping triggered after 78 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 2.6347, Acc: 0.4286, F1: 0.5362 | Val Loss: 0.6229, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6316, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6113, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6330, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6023, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6182, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5965, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6165, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5715, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5944, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5398, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 007 | Train Loss: 0.5363, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.5986, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 008 | Train Loss: 0.5541, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5306, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 009 | Train Loss: 0.5042, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5208, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 010 | Train Loss: 0.5006, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5185, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 011 | Train Loss: 0.4854, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5133, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 012 | Train Loss: 0.4804, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5094, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 013 | Train Loss: 0.4602, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5643, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 014 | Train Loss: 0.4642, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5179, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 015 | Train Loss: 0.4572, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5358, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 016 | Train Loss: 0.4957, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5136, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 017 | Train Loss: 0.4583, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5461, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 018 | Train Loss: 0.4631, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5196, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 019 | Train Loss: 0.4693, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5352, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 020 | Train Loss: 0.4671, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5116, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 021 | Train Loss: 0.4465, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5350, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 022 | Train Loss: 0.4633, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5210, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 023 | Train Loss: 0.4483, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5225, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 024 | Train Loss: 0.4483, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5311, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.4877, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5261, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4703, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5754, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 027 | Train Loss: 0.4571, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5266, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 028 | Train Loss: 0.4670, Acc: 0.7411, F1: 0.8284 | Val Loss: 0.5196, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4487, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5336, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 030 | Train Loss: 0.4413, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5185, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4764, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5149, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 032 | Train Loss: 0.4692, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5569, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 033 | Train Loss: 0.4389, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5123, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 034 | Train Loss: 0.4953, Acc: 0.7500, F1: 0.8372 | Val Loss: 0.5053, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 035 | Train Loss: 0.4473, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5141, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 036 | Train Loss: 0.4352, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5369, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 037 | Train Loss: 0.4439, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5493, Acc: 0.7895, F1: 0.8400\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6593, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6239, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6412, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6193, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6400, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6158, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6333, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6129, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6300, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6098, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6262, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6069, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6227, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6090, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6196, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6036, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6176, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5963, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6062, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5934, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6032, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5888, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5987, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5851, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5921, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5789, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5823, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5704, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5804, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5640, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 016 | Train Loss: 0.5715, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5632, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 017 | Train Loss: 0.5613, Acc: 0.7054, F1: 0.8047 | Val Loss: 0.5583, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 018 | Train Loss: 0.5563, Acc: 0.7054, F1: 0.8024 | Val Loss: 0.5486, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 019 | Train Loss: 0.5464, Acc: 0.7321, F1: 0.8295 | Val Loss: 0.5478, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 020 | Train Loss: 0.5573, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5419, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 021 | Train Loss: 0.5444, Acc: 0.7143, F1: 0.8095 | Val Loss: 0.5471, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 022 | Train Loss: 0.5400, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5422, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 023 | Train Loss: 0.5239, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5314, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 024 | Train Loss: 0.5242, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5289, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 025 | Train Loss: 0.5185, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5315, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 026 | Train Loss: 0.5181, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5294, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.5081, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5251, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 028 | Train Loss: 0.5092, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5222, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 029 | Train Loss: 0.5015, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5222, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.4971, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5247, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.4935, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5254, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.5005, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5178, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.4890, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5180, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.4848, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5158, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 035 | Train Loss: 0.4794, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5132, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 036 | Train Loss: 0.4966, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5113, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 037 | Train Loss: 0.4986, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5316, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 038 | Train Loss: 0.4915, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5093, Acc: 0.7105, F1: 0.7843\n",
      "\n",
      "Early stopping triggered after 38 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6671, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6353, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6303, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5908, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6052, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5796, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5887, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5595, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5620, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5430, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 006 | Train Loss: 0.5404, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5630, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 007 | Train Loss: 0.5696, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5412, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 008 | Train Loss: 0.6218, Acc: 0.6875, F1: 0.7368 | Val Loss: 0.5294, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 009 | Train Loss: 0.5483, Acc: 0.7232, F1: 0.8144 | Val Loss: 0.5392, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 010 | Train Loss: 0.5222, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5380, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 011 | Train Loss: 0.5086, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5233, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 012 | Train Loss: 0.4988, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5257, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 013 | Train Loss: 0.4934, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5268, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 014 | Train Loss: 0.4948, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5485, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 015 | Train Loss: 0.4984, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5307, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4789, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5348, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 017 | Train Loss: 0.5073, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5243, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4893, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5212, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 019 | Train Loss: 0.4796, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5305, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4751, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5185, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 021 | Train Loss: 0.4768, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5191, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 022 | Train Loss: 0.4632, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5245, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4763, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5270, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4626, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5233, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 025 | Train Loss: 0.4650, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5128, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4625, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5135, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 027 | Train Loss: 0.4637, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5248, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4516, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5255, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4622, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5227, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 030 | Train Loss: 0.4542, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5290, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4670, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5180, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4409, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5158, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 033 | Train Loss: 0.4487, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5184, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4553, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5169, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4504, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5193, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4361, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5229, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4438, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5319, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 038 | Train Loss: 0.4438, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5254, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4383, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5190, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4541, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5265, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 041 | Train Loss: 0.4634, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5358, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 042 | Train Loss: 0.4533, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5202, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4347, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5302, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 044 | Train Loss: 0.4475, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.5297, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 045 | Train Loss: 0.4321, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5255, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 046 | Train Loss: 0.4455, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5240, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4385, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5254, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4329, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5234, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4404, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5211, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 050 | Train Loss: 0.4476, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5255, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 051 | Train Loss: 0.4268, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5213, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 052 | Train Loss: 0.4411, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5218, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 053 | Train Loss: 0.4271, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5457, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 054 | Train Loss: 0.4474, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5338, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 055 | Train Loss: 0.4540, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5283, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 056 | Train Loss: 0.4460, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5485, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 057 | Train Loss: 0.4462, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5206, Acc: 0.7105, F1: 0.8070\n",
      "\n",
      "Early stopping triggered after 57 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.7413, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6142, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6295, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6315, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6343, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6006, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6201, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6011, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6323, Acc: 0.6875, F1: 0.7879 | Val Loss: 0.5858, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6182, Acc: 0.6429, F1: 0.7778 | Val Loss: 0.5699, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 007 | Train Loss: 0.5480, Acc: 0.7321, F1: 0.8315 | Val Loss: 0.5148, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 008 | Train Loss: 0.5005, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5107, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 009 | Train Loss: 0.4903, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.6355, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 010 | Train Loss: 0.5301, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5295, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 011 | Train Loss: 0.4711, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5348, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 012 | Train Loss: 0.4495, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5427, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 013 | Train Loss: 0.4957, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5394, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 014 | Train Loss: 0.4683, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5202, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 015 | Train Loss: 0.4614, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5352, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 016 | Train Loss: 0.4928, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.5176, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 017 | Train Loss: 0.4769, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5082, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 018 | Train Loss: 0.4382, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5422, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 019 | Train Loss: 0.4746, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5263, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 020 | Train Loss: 0.4942, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5385, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 021 | Train Loss: 0.4619, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5342, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4401, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5183, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.5047, Acc: 0.7232, F1: 0.8121 | Val Loss: 0.5152, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.4665, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.5863, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 025 | Train Loss: 0.4576, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.5150, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 026 | Train Loss: 0.4735, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5186, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4374, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5432, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4633, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.5317, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 029 | Train Loss: 0.4527, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5284, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4418, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5471, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 031 | Train Loss: 0.4533, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5300, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 032 | Train Loss: 0.4510, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5346, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 033 | Train Loss: 0.4485, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5287, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 33 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6629, Acc: 0.5982, F1: 0.7097 | Val Loss: 0.6160, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6477, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6179, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6363, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6100, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6283, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6106, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6263, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6156, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6291, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6152, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6277, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6068, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6176, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5988, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6095, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5926, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6088, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5876, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6043, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5834, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5979, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5788, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5940, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5749, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5869, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5709, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5795, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5643, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 016 | Train Loss: 0.5699, Acc: 0.7321, F1: 0.8295 | Val Loss: 0.5641, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 017 | Train Loss: 0.5674, Acc: 0.7054, F1: 0.8047 | Val Loss: 0.5542, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 018 | Train Loss: 0.5526, Acc: 0.6964, F1: 0.8000 | Val Loss: 0.5489, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 019 | Train Loss: 0.5484, Acc: 0.7232, F1: 0.8166 | Val Loss: 0.5457, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 020 | Train Loss: 0.5437, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5449, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 021 | Train Loss: 0.5439, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5370, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 022 | Train Loss: 0.5326, Acc: 0.7143, F1: 0.8049 | Val Loss: 0.5349, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 023 | Train Loss: 0.5256, Acc: 0.7232, F1: 0.8098 | Val Loss: 0.5324, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 024 | Train Loss: 0.5221, Acc: 0.7411, F1: 0.8242 | Val Loss: 0.5307, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 025 | Train Loss: 0.5260, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5350, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.5230, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5287, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 027 | Train Loss: 0.5133, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5265, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 028 | Train Loss: 0.5050, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5248, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 029 | Train Loss: 0.5099, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5233, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 030 | Train Loss: 0.5003, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5250, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.5173, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5186, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 032 | Train Loss: 0.5050, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5324, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 033 | Train Loss: 0.5036, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5154, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 034 | Train Loss: 0.4973, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5145, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 035 | Train Loss: 0.4809, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5186, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 036 | Train Loss: 0.5110, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5210, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 037 | Train Loss: 0.4990, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5165, Acc: 0.6842, F1: 0.7857\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7240, Acc: 0.5357, F1: 0.6623 | Val Loss: 0.6371, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6421, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6071, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6171, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5927, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6106, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5841, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5971, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5862, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5933, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5665, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5788, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5560, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5772, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5460, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5554, Acc: 0.7054, F1: 0.8136 | Val Loss: 0.5613, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 010 | Train Loss: 0.5512, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5329, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 011 | Train Loss: 0.5249, Acc: 0.7321, F1: 0.8256 | Val Loss: 0.5276, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 012 | Train Loss: 0.5189, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5371, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.5110, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5339, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.5236, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5408, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.5555, Acc: 0.7232, F1: 0.7669 | Val Loss: 0.5563, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 016 | Train Loss: 0.4890, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5537, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 017 | Train Loss: 0.5241, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5383, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.5040, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5376, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.5033, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5320, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 020 | Train Loss: 0.5021, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5323, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4924, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5286, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.4911, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5381, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.5106, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5455, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4850, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5342, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4979, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5310, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.5070, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5401, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4996, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5372, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 028 | Train Loss: 0.5009, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5346, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4874, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5262, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4707, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5251, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 031 | Train Loss: 0.4783, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5246, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.4983, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5339, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4728, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5333, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 034 | Train Loss: 0.4787, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5348, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4747, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5204, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 036 | Train Loss: 0.4887, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5279, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 037 | Train Loss: 0.4611, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5384, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 038 | Train Loss: 0.4822, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5151, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4860, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5177, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 040 | Train Loss: 0.4640, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5341, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 041 | Train Loss: 0.4684, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.5132, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4510, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5224, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 043 | Train Loss: 0.4591, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5253, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4795, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.5216, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4375, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5180, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 046 | Train Loss: 0.4603, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5209, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4479, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5146, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 048 | Train Loss: 0.4370, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5312, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 049 | Train Loss: 0.5172, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5128, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 050 | Train Loss: 0.4455, Acc: 0.8125, F1: 0.8712 | Val Loss: 0.5483, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 051 | Train Loss: 0.4902, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5082, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4599, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5224, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 053 | Train Loss: 0.4438, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5107, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 054 | Train Loss: 0.4557, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5141, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 055 | Train Loss: 0.4425, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5174, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 056 | Train Loss: 0.4345, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5186, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 057 | Train Loss: 0.4306, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5184, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 058 | Train Loss: 0.4452, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5223, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 059 | Train Loss: 0.4452, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5230, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 060 | Train Loss: 0.4225, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5404, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 061 | Train Loss: 0.4573, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5279, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 062 | Train Loss: 0.4438, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5290, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 063 | Train Loss: 0.4388, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5237, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 064 | Train Loss: 0.4408, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5410, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 065 | Train Loss: 0.4352, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5234, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 066 | Train Loss: 0.4348, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5235, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 067 | Train Loss: 0.4392, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5212, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 068 | Train Loss: 0.4350, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5242, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 68 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 2.1918, Acc: 0.6250, F1: 0.7529 | Val Loss: 0.7389, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.6593, Acc: 0.6071, F1: 0.7143 | Val Loss: 0.6190, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6341, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6135, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6271, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5918, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5912, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5972, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 006 | Train Loss: 0.6310, Acc: 0.6161, F1: 0.6815 | Val Loss: 0.6478, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6222, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.6028, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 008 | Train Loss: 0.5470, Acc: 0.7143, F1: 0.7838 | Val Loss: 0.5215, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 009 | Train Loss: 0.5168, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5326, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 010 | Train Loss: 0.5022, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5078, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 011 | Train Loss: 0.4990, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.6317, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 012 | Train Loss: 0.5379, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5052, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 013 | Train Loss: 0.4710, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5097, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 014 | Train Loss: 0.4753, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5111, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 015 | Train Loss: 0.4811, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5450, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 016 | Train Loss: 0.4619, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5143, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 017 | Train Loss: 0.4518, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5512, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 018 | Train Loss: 0.4674, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5185, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 019 | Train Loss: 0.4547, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5245, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 020 | Train Loss: 0.4696, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5169, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.4739, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5527, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 022 | Train Loss: 0.4807, Acc: 0.8393, F1: 0.8784 | Val Loss: 0.5231, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4488, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5134, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 024 | Train Loss: 0.4756, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5381, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 025 | Train Loss: 0.4653, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5159, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4841, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5261, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4518, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5072, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 028 | Train Loss: 0.4459, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5245, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4479, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5145, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 030 | Train Loss: 0.4535, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5290, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4635, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5424, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4487, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5292, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 033 | Train Loss: 0.4410, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5465, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 034 | Train Loss: 0.4591, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5391, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 035 | Train Loss: 0.4548, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5134, Acc: 0.6842, F1: 0.7857\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6827, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6303, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6373, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6147, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6419, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6113, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6259, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6080, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6239, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6117, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6255, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6125, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6196, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6051, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6134, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5952, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6059, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5875, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6063, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5828, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6007, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5787, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6010, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5722, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5899, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5680, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5766, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.5757, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 015 | Train Loss: 0.5817, Acc: 0.6964, F1: 0.8000 | Val Loss: 0.5659, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 016 | Train Loss: 0.5745, Acc: 0.6964, F1: 0.8068 | Val Loss: 0.5496, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 017 | Train Loss: 0.5550, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5445, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 018 | Train Loss: 0.5633, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.5410, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 019 | Train Loss: 0.5454, Acc: 0.7054, F1: 0.8156 | Val Loss: 0.5340, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 020 | Train Loss: 0.5549, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5477, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 021 | Train Loss: 0.5393, Acc: 0.7143, F1: 0.7949 | Val Loss: 0.5290, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 022 | Train Loss: 0.5256, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5250, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 023 | Train Loss: 0.5296, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5223, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 024 | Train Loss: 0.5189, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5248, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 025 | Train Loss: 0.5191, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5199, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 026 | Train Loss: 0.5133, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5258, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.5122, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5226, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.5084, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5193, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.5033, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5177, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 030 | Train Loss: 0.5159, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5171, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.5012, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5194, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.5019, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5196, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4978, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5150, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 034 | Train Loss: 0.5079, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5243, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 035 | Train Loss: 0.4924, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5183, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7088, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6573, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6607, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6355, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6294, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6008, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6306, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5882, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5994, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5848, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6047, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5817, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5830, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5644, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5778, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5535, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5658, Acc: 0.6964, F1: 0.8046 | Val Loss: 0.5578, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.5522, Acc: 0.7143, F1: 0.8202 | Val Loss: 0.5433, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.5548, Acc: 0.7232, F1: 0.8208 | Val Loss: 0.5399, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 012 | Train Loss: 0.5255, Acc: 0.7500, F1: 0.8333 | Val Loss: 0.5307, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 013 | Train Loss: 0.5300, Acc: 0.7321, F1: 0.8235 | Val Loss: 0.5436, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.5316, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5297, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.5798, Acc: 0.7321, F1: 0.8235 | Val Loss: 0.5326, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 016 | Train Loss: 0.5227, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5745, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 017 | Train Loss: 0.5142, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5384, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 018 | Train Loss: 0.5179, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5299, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.4924, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5314, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4983, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5339, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.4966, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5360, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.4948, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5389, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4789, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5457, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 024 | Train Loss: 0.4990, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5400, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.5007, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5397, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4830, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5362, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4807, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5335, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 028 | Train Loss: 0.4882, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5343, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running sage hidden=128, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 3.3589, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6887, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6861, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6428, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6309, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6114, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6358, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6193, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6448, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6060, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6365, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6130, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6310, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5949, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6168, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5819, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6082, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5794, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6110, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5760, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6094, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5582, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5754, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5486, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 013 | Train Loss: 0.5733, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5837, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 014 | Train Loss: 0.5773, Acc: 0.7768, F1: 0.8538 | Val Loss: 0.5449, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5369, Acc: 0.6696, F1: 0.7978 | Val Loss: 0.5585, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 016 | Train Loss: 0.5717, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5401, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 017 | Train Loss: 0.5493, Acc: 0.7232, F1: 0.8229 | Val Loss: 0.5349, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 018 | Train Loss: 0.5327, Acc: 0.7232, F1: 0.8187 | Val Loss: 0.5304, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 019 | Train Loss: 0.5256, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5238, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.5155, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5873, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 021 | Train Loss: 0.5686, Acc: 0.6964, F1: 0.7703 | Val Loss: 0.5373, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.5567, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5219, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 023 | Train Loss: 0.5207, Acc: 0.7143, F1: 0.8140 | Val Loss: 0.5187, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 024 | Train Loss: 0.5677, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5322, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.5216, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5206, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 026 | Train Loss: 0.5022, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5271, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4883, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5841, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 028 | Train Loss: 0.5770, Acc: 0.6875, F1: 0.7445 | Val Loss: 0.5844, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 029 | Train Loss: 0.6454, Acc: 0.7321, F1: 0.8315 | Val Loss: 0.5474, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 030 | Train Loss: 0.5953, Acc: 0.6964, F1: 0.7385 | Val Loss: 0.5610, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 031 | Train Loss: 0.5336, Acc: 0.7143, F1: 0.7949 | Val Loss: 0.5530, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 032 | Train Loss: 0.5430, Acc: 0.7411, F1: 0.8284 | Val Loss: 0.5324, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4954, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5144, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 034 | Train Loss: 0.4973, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5303, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 035 | Train Loss: 0.5101, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5189, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 036 | Train Loss: 0.5060, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5212, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 037 | Train Loss: 0.4751, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5404, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 038 | Train Loss: 0.4658, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5325, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4672, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5325, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.4716, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5319, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 041 | Train Loss: 0.4945, Acc: 0.7232, F1: 0.8050 | Val Loss: 0.5292, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4983, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5268, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.5045, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5429, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 044 | Train Loss: 0.5077, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5340, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 045 | Train Loss: 0.4539, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5386, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 046 | Train Loss: 0.4966, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5394, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 047 | Train Loss: 0.4415, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5577, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 048 | Train Loss: 0.4673, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5500, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 049 | Train Loss: 0.4999, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5551, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 050 | Train Loss: 0.4365, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5386, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 051 | Train Loss: 0.5015, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5357, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4547, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5348, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 053 | Train Loss: 0.4914, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5270, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 53 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6518, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6129, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6278, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6044, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6196, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5995, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6101, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5874, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6009, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5750, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5888, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5668, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.5771, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5593, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5690, Acc: 0.6786, F1: 0.7931 | Val Loss: 0.5591, Acc: 0.6053, F1: 0.7273\n",
      "Epoch 009 | Train Loss: 0.5576, Acc: 0.7143, F1: 0.8072 | Val Loss: 0.5488, Acc: 0.6316, F1: 0.7407\n",
      "Epoch 010 | Train Loss: 0.5477, Acc: 0.6964, F1: 0.7976 | Val Loss: 0.5366, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 011 | Train Loss: 0.5398, Acc: 0.7143, F1: 0.8095 | Val Loss: 0.5322, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 012 | Train Loss: 0.5284, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5272, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 013 | Train Loss: 0.5231, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5289, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.5178, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5361, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.5130, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5215, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 016 | Train Loss: 0.5153, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5182, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 017 | Train Loss: 0.5105, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5199, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4979, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5193, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.5117, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5134, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 020 | Train Loss: 0.4980, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5287, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 021 | Train Loss: 0.4952, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5099, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 022 | Train Loss: 0.4886, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5105, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4829, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5179, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.5048, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5161, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4836, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5059, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.5029, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5120, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 26 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 1.1599, Acc: 0.6071, F1: 0.7412 | Val Loss: 0.8943, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7644, Acc: 0.5089, F1: 0.5378 | Val Loss: 0.6022, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6208, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5962, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6127, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5880, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6014, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5701, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5795, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5504, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.5721, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5340, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 008 | Train Loss: 0.5552, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5221, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 009 | Train Loss: 0.5161, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5330, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 010 | Train Loss: 0.4985, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5167, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 011 | Train Loss: 0.4864, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5315, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.4846, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5147, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 013 | Train Loss: 0.4784, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5177, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 014 | Train Loss: 0.4744, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5125, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 015 | Train Loss: 0.5028, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5253, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.4717, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5047, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 017 | Train Loss: 0.4673, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5254, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 018 | Train Loss: 0.4649, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5059, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 019 | Train Loss: 0.4593, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5128, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 020 | Train Loss: 0.4516, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5074, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.4614, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5114, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4604, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5025, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4623, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5048, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 024 | Train Loss: 0.4580, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5186, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 025 | Train Loss: 0.4614, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.4993, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4558, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5106, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 027 | Train Loss: 0.4473, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5144, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 028 | Train Loss: 0.4569, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5041, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4526, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.4999, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 030 | Train Loss: 0.4597, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5086, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 031 | Train Loss: 0.4443, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5044, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 032 | Train Loss: 0.4539, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5198, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 033 | Train Loss: 0.4557, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5047, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 034 | Train Loss: 0.4822, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5016, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4388, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5566, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 036 | Train Loss: 0.4757, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5199, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 037 | Train Loss: 0.4577, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4936, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 5.7926, Acc: 0.6429, F1: 0.7403 | Val Loss: 0.6094, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6454, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6253, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6301, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6086, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 004 | Train Loss: 0.6785, Acc: 0.5179, F1: 0.6029 | Val Loss: 0.5932, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6308, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5760, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6141, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5737, Acc: 0.6842, F1: 0.8065\n",
      "Epoch 007 | Train Loss: 0.5449, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5131, Acc: 0.7632, F1: 0.8475\n",
      "Epoch 008 | Train Loss: 0.5378, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5438, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 009 | Train Loss: 0.5238, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.5880, Acc: 0.7632, F1: 0.8475\n",
      "Epoch 010 | Train Loss: 0.6066, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.5274, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 011 | Train Loss: 0.4670, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5139, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 012 | Train Loss: 0.4895, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5141, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 013 | Train Loss: 0.4708, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5119, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 014 | Train Loss: 0.4617, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5118, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 015 | Train Loss: 0.4595, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5289, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 016 | Train Loss: 0.4572, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5192, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 017 | Train Loss: 0.4802, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.5110, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 018 | Train Loss: 0.5504, Acc: 0.7500, F1: 0.8333 | Val Loss: 0.5515, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 019 | Train Loss: 0.5216, Acc: 0.7946, F1: 0.8345 | Val Loss: 0.5697, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 020 | Train Loss: 0.4650, Acc: 0.8214, F1: 0.8718 | Val Loss: 0.5071, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 021 | Train Loss: 0.4465, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5468, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 022 | Train Loss: 0.4601, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5255, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4393, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5369, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 024 | Train Loss: 0.4682, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5237, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4884, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5142, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 026 | Train Loss: 0.4604, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5419, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 027 | Train Loss: 0.4540, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5165, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 028 | Train Loss: 0.4428, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5371, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4946, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.5568, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 030 | Train Loss: 0.4436, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5257, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 031 | Train Loss: 0.4658, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5195, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4686, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.5304, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 033 | Train Loss: 0.4483, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5400, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 034 | Train Loss: 0.4487, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5412, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 035 | Train Loss: 0.4701, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5273, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 036 | Train Loss: 0.4613, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5206, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 037 | Train Loss: 0.4544, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5479, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 038 | Train Loss: 0.4501, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5109, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 38 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6469, Acc: 0.5893, F1: 0.7013 | Val Loss: 0.6368, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6596, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6060, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6243, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6060, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6161, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5935, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6033, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5848, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5947, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5756, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5830, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5682, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5724, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5555, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5598, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5472, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5486, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.5412, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 011 | Train Loss: 0.5455, Acc: 0.7143, F1: 0.8049 | Val Loss: 0.5374, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 012 | Train Loss: 0.5315, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5298, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 013 | Train Loss: 0.5263, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5274, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 014 | Train Loss: 0.5200, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5254, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 015 | Train Loss: 0.5082, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5342, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.5114, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5183, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.5033, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5171, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.4983, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5140, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 019 | Train Loss: 0.4984, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5190, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4968, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5104, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.4840, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5126, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4964, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5060, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4793, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5090, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4768, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5083, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4738, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5015, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4740, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.4986, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4692, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5001, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4633, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5048, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 029 | Train Loss: 0.4614, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5009, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4762, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4990, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4669, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5060, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 032 | Train Loss: 0.4805, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5065, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4688, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5037, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4892, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.4992, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 035 | Train Loss: 0.4586, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5169, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 036 | Train Loss: 0.4671, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5080, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 037 | Train Loss: 0.4615, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5001, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4597, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4962, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4499, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5083, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4544, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5036, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4532, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4956, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4656, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5013, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 043 | Train Loss: 0.4493, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.4978, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4558, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4988, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4532, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5109, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4504, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4999, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4483, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5047, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4542, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5101, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4496, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4996, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 050 | Train Loss: 0.4512, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5036, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 051 | Train Loss: 0.4470, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5059, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4519, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5053, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 053 | Train Loss: 0.4515, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5050, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 054 | Train Loss: 0.4394, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5003, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 055 | Train Loss: 0.4630, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.4998, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 55 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7890, Acc: 0.5268, F1: 0.6667 | Val Loss: 0.5859, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6625, Acc: 0.5446, F1: 0.6107 | Val Loss: 0.6253, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6476, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6061, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6232, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6082, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6083, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5796, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5905, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5611, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5678, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5368, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 008 | Train Loss: 0.5530, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5239, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 009 | Train Loss: 0.5129, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5849, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 010 | Train Loss: 0.5246, Acc: 0.7143, F1: 0.7714 | Val Loss: 0.6322, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5988, Acc: 0.7143, F1: 0.8140 | Val Loss: 0.5300, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.5165, Acc: 0.7232, F1: 0.7832 | Val Loss: 0.5158, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.5164, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5114, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 014 | Train Loss: 0.4967, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5213, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4897, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5106, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.4767, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5109, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 017 | Train Loss: 0.4665, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5140, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 018 | Train Loss: 0.4736, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5181, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 019 | Train Loss: 0.4681, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5475, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 020 | Train Loss: 0.4608, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5225, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 021 | Train Loss: 0.4497, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5176, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4775, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5142, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.4543, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5089, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 024 | Train Loss: 0.4553, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5149, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.4477, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5094, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4493, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5123, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4462, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5165, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4422, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5155, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4657, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5219, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4668, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5270, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4329, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5283, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 032 | Train Loss: 0.4686, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5141, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 033 | Train Loss: 0.4433, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5298, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 034 | Train Loss: 0.4400, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5167, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4400, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5204, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4416, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5233, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 037 | Train Loss: 0.4380, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5146, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 038 | Train Loss: 0.4377, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5252, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 039 | Train Loss: 0.4368, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5273, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 040 | Train Loss: 0.4413, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5162, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 041 | Train Loss: 0.4346, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5241, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4367, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5295, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 043 | Train Loss: 0.4338, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5234, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4351, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5298, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4336, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5207, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4331, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5266, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 047 | Train Loss: 0.4364, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5222, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4348, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5201, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4353, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5356, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 050 | Train Loss: 0.4424, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5177, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 051 | Train Loss: 0.4334, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5219, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4366, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5270, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 053 | Train Loss: 0.4357, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5283, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 53 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 5.7159, Acc: 0.5893, F1: 0.7013 | Val Loss: 0.6397, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6435, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6356, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6462, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6256, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6621, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6077, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6267, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5935, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6025, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5514, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 007 | Train Loss: 0.5641, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.6595, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 008 | Train Loss: 0.5697, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5208, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 009 | Train Loss: 0.5448, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5329, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 010 | Train Loss: 0.5162, Acc: 0.7768, F1: 0.8521 | Val Loss: 0.5315, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 011 | Train Loss: 0.4786, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5141, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 012 | Train Loss: 0.4840, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5257, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 013 | Train Loss: 0.4895, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5234, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 014 | Train Loss: 0.4568, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5088, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 015 | Train Loss: 0.4606, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5128, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 016 | Train Loss: 0.4810, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5678, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 017 | Train Loss: 0.4893, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.5298, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 018 | Train Loss: 0.4957, Acc: 0.7411, F1: 0.8284 | Val Loss: 0.5312, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 019 | Train Loss: 0.4628, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.5445, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 020 | Train Loss: 0.4586, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5259, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.4574, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5367, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4911, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5338, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.4878, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.5454, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 024 | Train Loss: 0.5545, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5318, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.4601, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.5666, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 026 | Train Loss: 0.4606, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5146, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 027 | Train Loss: 0.4526, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5356, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4349, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5171, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 029 | Train Loss: 0.4401, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5283, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4464, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5103, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4443, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5121, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4385, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5361, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 033 | Train Loss: 0.4692, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5127, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4348, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5162, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 035 | Train Loss: 0.4613, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5216, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4420, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5388, Acc: 0.7895, F1: 0.8400\n",
      "\n",
      "Early stopping triggered after 36 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6513, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6123, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6283, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6072, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6166, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5941, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6059, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5836, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5972, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5740, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5914, Acc: 0.6696, F1: 0.7933 | Val Loss: 0.5787, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 007 | Train Loss: 0.5804, Acc: 0.6786, F1: 0.7978 | Val Loss: 0.5583, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5656, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5524, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 009 | Train Loss: 0.5597, Acc: 0.7232, F1: 0.8249 | Val Loss: 0.5469, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.5471, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5440, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 011 | Train Loss: 0.5448, Acc: 0.7143, F1: 0.7949 | Val Loss: 0.5339, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 012 | Train Loss: 0.5420, Acc: 0.6964, F1: 0.8068 | Val Loss: 0.5453, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 013 | Train Loss: 0.5419, Acc: 0.7143, F1: 0.8095 | Val Loss: 0.5411, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.5233, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5322, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.5272, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5222, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 016 | Train Loss: 0.5077, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5239, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.5026, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5210, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4985, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5176, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 019 | Train Loss: 0.4990, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5172, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 020 | Train Loss: 0.4960, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5237, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4901, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5150, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4854, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5153, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4950, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5211, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4839, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5124, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 025 | Train Loss: 0.4845, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5216, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4763, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5098, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4891, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5086, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 028 | Train Loss: 0.4755, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5399, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4762, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5068, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4822, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5052, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 031 | Train Loss: 0.4644, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5227, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.5128, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5180, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4995, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5200, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 034 | Train Loss: 0.4976, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5123, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 035 | Train Loss: 0.4675, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5058, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4613, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5047, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 037 | Train Loss: 0.4602, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5018, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4600, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5059, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4641, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5032, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4576, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5019, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4572, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5004, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4594, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5048, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 043 | Train Loss: 0.4533, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5053, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4520, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5013, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4516, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5025, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4583, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5030, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4564, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5226, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 048 | Train Loss: 0.4668, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5059, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4588, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5056, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 050 | Train Loss: 0.4605, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5074, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 051 | Train Loss: 0.4505, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5378, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 052 | Train Loss: 0.4561, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5027, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 053 | Train Loss: 0.4565, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5029, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 054 | Train Loss: 0.4555, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5057, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 055 | Train Loss: 0.4452, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5121, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 056 | Train Loss: 0.4535, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5087, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 057 | Train Loss: 0.4559, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5048, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 058 | Train Loss: 0.4479, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5160, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 059 | Train Loss: 0.4537, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5196, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 060 | Train Loss: 0.4550, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5053, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 061 | Train Loss: 0.4449, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5163, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 062 | Train Loss: 0.4695, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5098, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 063 | Train Loss: 0.4573, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5045, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 064 | Train Loss: 0.4573, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5124, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 065 | Train Loss: 0.4492, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5043, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 066 | Train Loss: 0.4480, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5112, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 067 | Train Loss: 0.4470, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5041, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 67 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 1.0077, Acc: 0.5000, F1: 0.5942 | Val Loss: 0.8811, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7302, Acc: 0.5268, F1: 0.5470 | Val Loss: 0.6076, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6604, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6149, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6136, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6269, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6396, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6300, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6238, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5882, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6002, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5795, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5915, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5671, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5893, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5516, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5623, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5309, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5196, Acc: 0.7679, F1: 0.8471 | Val Loss: 0.5625, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 012 | Train Loss: 0.5204, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5169, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 013 | Train Loss: 0.5049, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5319, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4966, Acc: 0.7321, F1: 0.7917 | Val Loss: 0.5253, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 015 | Train Loss: 0.5219, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5196, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.5182, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5204, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4788, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5165, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 018 | Train Loss: 0.4783, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5289, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4730, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5053, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 020 | Train Loss: 0.4690, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5052, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 021 | Train Loss: 0.4690, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5216, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 022 | Train Loss: 0.4674, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5073, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4507, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5461, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 024 | Train Loss: 0.4904, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5037, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.4639, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5016, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4629, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5246, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 027 | Train Loss: 0.4753, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4980, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4533, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5250, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 029 | Train Loss: 0.4577, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5001, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4512, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5002, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 031 | Train Loss: 0.4643, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5169, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 032 | Train Loss: 0.4470, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5039, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4574, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4997, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4458, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5025, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4391, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5137, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 036 | Train Loss: 0.4438, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5121, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 037 | Train Loss: 0.4448, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5080, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4409, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5194, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 039 | Train Loss: 0.4543, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5073, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4402, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5287, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 041 | Train Loss: 0.4485, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5142, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4513, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5083, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 043 | Train Loss: 0.4491, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5360, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 044 | Train Loss: 0.4402, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5047, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4396, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5003, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 046 | Train Loss: 0.4503, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5019, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 46 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 8.9858, Acc: 0.5893, F1: 0.7294 | Val Loss: 2.8730, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 1.3807, Acc: 0.4821, F1: 0.5246 | Val Loss: 0.6310, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6346, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6334, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6338, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5950, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6141, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5939, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5957, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6046, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6037, Acc: 0.6339, F1: 0.7515 | Val Loss: 0.5396, Acc: 0.6053, F1: 0.7170\n",
      "Epoch 008 | Train Loss: 0.6218, Acc: 0.6607, F1: 0.7889 | Val Loss: 0.5358, Acc: 0.6316, F1: 0.7500\n",
      "Epoch 009 | Train Loss: 0.5306, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5628, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 010 | Train Loss: 0.5566, Acc: 0.7232, F1: 0.7891 | Val Loss: 0.6999, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.6587, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5690, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 012 | Train Loss: 0.6483, Acc: 0.6339, F1: 0.6555 | Val Loss: 0.5653, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 013 | Train Loss: 0.6743, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5471, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5781, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5442, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 015 | Train Loss: 0.5241, Acc: 0.7589, F1: 0.8457 | Val Loss: 0.5147, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 016 | Train Loss: 0.4991, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5324, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 017 | Train Loss: 0.5079, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5408, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4921, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5346, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 019 | Train Loss: 0.4956, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5285, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 020 | Train Loss: 0.4654, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5386, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 021 | Train Loss: 0.4688, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5224, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4616, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5246, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.4535, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5190, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.4543, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5256, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4428, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5267, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4547, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5480, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 027 | Train Loss: 0.4578, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5235, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 028 | Train Loss: 0.4789, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5246, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4485, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5325, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4490, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5257, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4381, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5155, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 032 | Train Loss: 0.4549, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5308, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 033 | Train Loss: 0.4397, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5529, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 034 | Train Loss: 0.4564, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5361, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4415, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5288, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4267, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5557, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 037 | Train Loss: 0.4480, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5318, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4506, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5261, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 039 | Train Loss: 0.4518, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5262, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 040 | Train Loss: 0.4473, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5240, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 041 | Train Loss: 0.4367, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5357, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4465, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5329, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4225, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5586, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 044 | Train Loss: 0.4422, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5424, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 045 | Train Loss: 0.4618, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5443, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 046 | Train Loss: 0.4436, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5381, Acc: 0.7895, F1: 0.8400\n",
      "\n",
      "Early stopping triggered after 46 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6623, Acc: 0.5893, F1: 0.7013 | Val Loss: 0.6254, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6563, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6159, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6379, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6197, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6306, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6140, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6162, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5984, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6125, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5962, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6167, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5907, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6056, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5820, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5929, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5806, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5851, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5693, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.5746, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5584, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 012 | Train Loss: 0.5617, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5529, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 013 | Train Loss: 0.5511, Acc: 0.7321, F1: 0.8235 | Val Loss: 0.5456, Acc: 0.6316, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.5546, Acc: 0.7232, F1: 0.8208 | Val Loss: 0.5371, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 015 | Train Loss: 0.5331, Acc: 0.7143, F1: 0.8095 | Val Loss: 0.5424, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 016 | Train Loss: 0.5358, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5328, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 017 | Train Loss: 0.5192, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5235, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 018 | Train Loss: 0.5194, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5232, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.5102, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5239, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.5032, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5155, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 021 | Train Loss: 0.5033, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5140, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4930, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5128, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4911, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5126, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4886, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5166, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4908, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5164, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.5155, Acc: 0.7232, F1: 0.8000 | Val Loss: 0.5072, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4663, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5397, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 028 | Train Loss: 0.4970, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5220, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4693, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5035, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 030 | Train Loss: 0.4790, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5025, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4789, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5048, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 032 | Train Loss: 0.4804, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5244, Acc: 0.7368, F1: 0.7917\n",
      "\n",
      "Early stopping triggered after 32 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 1.0745, Acc: 0.5000, F1: 0.5942 | Val Loss: 0.8204, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7154, Acc: 0.5179, F1: 0.5000 | Val Loss: 0.6091, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6318, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6013, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6130, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5964, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6047, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5718, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5867, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5554, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5732, Acc: 0.6875, F1: 0.7977 | Val Loss: 0.5499, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 008 | Train Loss: 0.5445, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5269, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 009 | Train Loss: 0.5237, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5199, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 010 | Train Loss: 0.5084, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5507, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 011 | Train Loss: 0.5144, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5193, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 012 | Train Loss: 0.4862, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5399, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 013 | Train Loss: 0.5220, Acc: 0.6964, F1: 0.7763 | Val Loss: 0.5160, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4786, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5393, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 015 | Train Loss: 0.4774, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5095, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.4729, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5162, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 017 | Train Loss: 0.4766, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5108, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 018 | Train Loss: 0.4780, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5212, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 019 | Train Loss: 0.5126, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5089, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 020 | Train Loss: 0.4829, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5030, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.5143, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5040, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4745, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5366, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 023 | Train Loss: 0.4637, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5050, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.4626, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5073, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.4502, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5135, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4520, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5125, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4528, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5144, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4463, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5118, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4605, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5074, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4488, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5163, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 031 | Train Loss: 0.4503, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5111, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4724, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5059, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 033 | Train Loss: 0.4351, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5390, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 034 | Train Loss: 0.4567, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5141, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4459, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5067, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4476, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5068, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 037 | Train Loss: 0.4422, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5092, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4417, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5189, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4440, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5229, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4376, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5303, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 041 | Train Loss: 0.4506, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5126, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 042 | Train Loss: 0.4558, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5146, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 42 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 5.6021, Acc: 0.5357, F1: 0.6232 | Val Loss: 0.6054, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6354, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6826, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 003 | Train Loss: 0.6702, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.6068, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6229, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6142, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 005 | Train Loss: 0.6058, Acc: 0.7411, F1: 0.8284 | Val Loss: 0.5519, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 006 | Train Loss: 0.5891, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5666, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 007 | Train Loss: 0.5272, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5229, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 008 | Train Loss: 0.4843, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5803, Acc: 0.7632, F1: 0.8475\n",
      "Epoch 009 | Train Loss: 0.5583, Acc: 0.7232, F1: 0.8025 | Val Loss: 0.5368, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 010 | Train Loss: 0.4953, Acc: 0.7946, F1: 0.8589 | Val Loss: 0.5352, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 011 | Train Loss: 0.4710, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5387, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 012 | Train Loss: 0.4451, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5110, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 013 | Train Loss: 0.4794, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5447, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 014 | Train Loss: 0.4556, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5092, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 015 | Train Loss: 0.5153, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5151, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 016 | Train Loss: 0.4672, Acc: 0.7857, F1: 0.8554 | Val Loss: 0.5112, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 017 | Train Loss: 0.4666, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5182, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 018 | Train Loss: 0.4894, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5228, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 019 | Train Loss: 0.4779, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5210, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 020 | Train Loss: 0.4438, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5260, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.4504, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5503, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.4596, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5345, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4566, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5278, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 024 | Train Loss: 0.4471, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5133, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4499, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5096, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4383, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5218, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 027 | Train Loss: 0.4741, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5711, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 028 | Train Loss: 0.4880, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.5243, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 029 | Train Loss: 0.5162, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.4993, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 030 | Train Loss: 0.4598, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5210, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4565, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5246, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 032 | Train Loss: 0.4630, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5347, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4877, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5144, Acc: 0.7105, F1: 0.8070\n",
      "\n",
      "Early stopping triggered after 33 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6425, Acc: 0.6161, F1: 0.7296 | Val Loss: 0.6290, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6374, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6061, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6194, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6094, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6186, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5874, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6005, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5774, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5889, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5691, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.5831, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5597, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5861, Acc: 0.6964, F1: 0.8000 | Val Loss: 0.5675, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 009 | Train Loss: 0.5615, Acc: 0.6964, F1: 0.7952 | Val Loss: 0.5474, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5534, Acc: 0.7054, F1: 0.8156 | Val Loss: 0.5398, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5494, Acc: 0.7143, F1: 0.8025 | Val Loss: 0.5382, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 012 | Train Loss: 0.5475, Acc: 0.7232, F1: 0.8121 | Val Loss: 0.5324, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 013 | Train Loss: 0.5346, Acc: 0.7143, F1: 0.7949 | Val Loss: 0.5388, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.5226, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5245, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 015 | Train Loss: 0.5284, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.5237, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 016 | Train Loss: 0.5117, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5337, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 017 | Train Loss: 0.5165, Acc: 0.7143, F1: 0.7808 | Val Loss: 0.5183, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5065, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5204, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 019 | Train Loss: 0.5208, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5217, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.5029, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5147, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.4914, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5118, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 022 | Train Loss: 0.4967, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5106, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.4868, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5208, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4813, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5081, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4918, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5070, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4841, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5295, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4852, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5041, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 028 | Train Loss: 0.4939, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5064, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 029 | Train Loss: 0.4783, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5216, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4792, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5030, Acc: 0.7368, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 1.8517, Acc: 0.4286, F1: 0.4754 | Val Loss: 0.6194, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6538, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6225, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6461, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6169, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6297, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6146, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6227, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6003, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6187, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5883, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6011, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5810, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5815, Acc: 0.6786, F1: 0.8000 | Val Loss: 0.5522, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5647, Acc: 0.6518, F1: 0.7771 | Val Loss: 0.5308, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 010 | Train Loss: 0.5246, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5291, Acc: 0.7895, F1: 0.8667\n",
      "Epoch 011 | Train Loss: 0.6176, Acc: 0.6696, F1: 0.7483 | Val Loss: 0.5330, Acc: 0.7632, F1: 0.8525\n",
      "Epoch 012 | Train Loss: 0.5816, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5228, Acc: 0.7632, F1: 0.8475\n",
      "Epoch 013 | Train Loss: 0.5613, Acc: 0.7054, F1: 0.7724 | Val Loss: 0.5268, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.5085, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5264, Acc: 0.7632, F1: 0.8475\n",
      "Epoch 015 | Train Loss: 0.5043, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5482, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 016 | Train Loss: 0.5209, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.5086, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 017 | Train Loss: 0.4808, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5317, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4943, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5108, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 019 | Train Loss: 0.4841, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5074, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 020 | Train Loss: 0.4755, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5052, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.4613, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5436, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 022 | Train Loss: 0.4654, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5098, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.4741, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5121, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 024 | Train Loss: 0.4816, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5091, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.5634, Acc: 0.7589, F1: 0.8402 | Val Loss: 0.5143, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 026 | Train Loss: 0.5080, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5635, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.4741, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4979, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4630, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.4932, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4634, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5001, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4571, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5019, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 9.8020, Acc: 0.5179, F1: 0.6087 | Val Loss: 1.2725, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.8063, Acc: 0.5000, F1: 0.5410 | Val Loss: 0.6830, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6570, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6111, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6192, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5821, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5951, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5955, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6096, Acc: 0.6786, F1: 0.7907 | Val Loss: 0.6249, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6338, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5823, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 008 | Train Loss: 0.5580, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5727, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5576, Acc: 0.6696, F1: 0.7910 | Val Loss: 0.5599, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 010 | Train Loss: 0.5675, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.7066, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 011 | Train Loss: 0.5494, Acc: 0.7054, F1: 0.7692 | Val Loss: 0.5445, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 012 | Train Loss: 0.5428, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5224, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.4889, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5185, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 014 | Train Loss: 0.4986, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5104, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 015 | Train Loss: 0.4757, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5127, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 016 | Train Loss: 0.4881, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5260, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 017 | Train Loss: 0.5235, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5180, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 018 | Train Loss: 0.4506, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5171, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 019 | Train Loss: 0.4502, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5188, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.4552, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5687, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 021 | Train Loss: 0.4515, Acc: 0.8304, F1: 0.8758 | Val Loss: 0.5501, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 022 | Train Loss: 0.5214, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5377, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 023 | Train Loss: 0.4516, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5217, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.4818, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5346, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 025 | Train Loss: 0.4646, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5425, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 026 | Train Loss: 0.4831, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5098, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 027 | Train Loss: 0.4427, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5212, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4410, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5151, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 029 | Train Loss: 0.4411, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5281, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 030 | Train Loss: 0.4464, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5266, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4600, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5645, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 032 | Train Loss: 0.5079, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.5033, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 033 | Train Loss: 0.4518, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5047, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 034 | Train Loss: 0.4506, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5208, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 035 | Train Loss: 0.4455, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5064, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 036 | Train Loss: 0.4671, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5233, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 037 | Train Loss: 0.4550, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5099, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 038 | Train Loss: 0.4514, Acc: 0.8036, F1: 0.8608 | Val Loss: 0.4984, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 039 | Train Loss: 0.4539, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5048, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 040 | Train Loss: 0.4513, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5021, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 041 | Train Loss: 0.4427, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5122, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.4600, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5203, Acc: 0.7895, F1: 0.8462\n",
      "\n",
      "Early stopping triggered after 42 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6921, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.6193, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6377, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6125, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6273, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6175, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6248, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6084, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6170, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5928, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6092, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5828, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5988, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5732, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5892, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5646, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.5887, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5581, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5672, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5530, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 011 | Train Loss: 0.5525, Acc: 0.7232, F1: 0.8144 | Val Loss: 0.5420, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5426, Acc: 0.7232, F1: 0.8166 | Val Loss: 0.5358, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 013 | Train Loss: 0.5532, Acc: 0.6786, F1: 0.7931 | Val Loss: 0.5381, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5251, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.5369, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.5206, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5280, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 016 | Train Loss: 0.5194, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5221, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 017 | Train Loss: 0.5166, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5204, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.5162, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5168, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 019 | Train Loss: 0.4949, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5252, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.5014, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.5133, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 021 | Train Loss: 0.5016, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5115, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 022 | Train Loss: 0.4868, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5111, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 023 | Train Loss: 0.5027, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5200, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4853, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5096, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 025 | Train Loss: 0.4922, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5047, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4889, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5218, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.5055, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5071, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 028 | Train Loss: 0.4805, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5080, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 029 | Train Loss: 0.4926, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5029, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 030 | Train Loss: 0.4957, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5189, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4948, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5022, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 032 | Train Loss: 0.4582, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.4995, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 033 | Train Loss: 0.4631, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5188, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 034 | Train Loss: 0.4744, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5000, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4703, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5023, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 036 | Train Loss: 0.4653, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5083, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 037 | Train Loss: 0.4643, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5005, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 038 | Train Loss: 0.4784, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5006, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4569, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5025, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 040 | Train Loss: 0.4710, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5000, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 041 | Train Loss: 0.4554, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5239, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4541, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5035, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 043 | Train Loss: 0.4546, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5027, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 044 | Train Loss: 0.4753, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5016, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 045 | Train Loss: 0.4484, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5032, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 046 | Train Loss: 0.4484, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5155, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 047 | Train Loss: 0.4577, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5179, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4682, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5046, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 049 | Train Loss: 0.4483, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5051, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 050 | Train Loss: 0.4569, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5068, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 051 | Train Loss: 0.4467, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5091, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4497, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5049, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 053 | Train Loss: 0.4334, Acc: 0.8214, F1: 0.8701 | Val Loss: 0.5116, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 054 | Train Loss: 0.4467, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5062, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 055 | Train Loss: 0.4365, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5206, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 056 | Train Loss: 0.4517, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5090, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 057 | Train Loss: 0.4445, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5229, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 058 | Train Loss: 0.4458, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.5076, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 059 | Train Loss: 0.4716, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5543, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 060 | Train Loss: 0.4614, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5079, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 061 | Train Loss: 0.4339, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5247, Acc: 0.6579, F1: 0.7719\n",
      "\n",
      "Early stopping triggered after 61 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.8643, Acc: 0.5893, F1: 0.7294 | Val Loss: 0.6644, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 002 | Train Loss: 0.6328, Acc: 0.6786, F1: 0.8000 | Val Loss: 0.5969, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6234, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5909, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6171, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5976, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6046, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5785, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5958, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5678, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5831, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5645, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5620, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5397, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 009 | Train Loss: 0.5469, Acc: 0.7143, F1: 0.8202 | Val Loss: 0.5619, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 010 | Train Loss: 0.5203, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5307, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 011 | Train Loss: 0.5067, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5400, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 012 | Train Loss: 0.4973, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5525, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 013 | Train Loss: 0.5051, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5415, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.5642, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5375, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4564, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5333, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 016 | Train Loss: 0.4966, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5353, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4905, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5167, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 018 | Train Loss: 0.4978, Acc: 0.7946, F1: 0.8606 | Val Loss: 0.5274, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4959, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5205, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 020 | Train Loss: 0.4852, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5259, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.4671, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5231, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 022 | Train Loss: 0.4762, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5531, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 023 | Train Loss: 0.4864, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5183, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 024 | Train Loss: 0.4662, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5172, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 025 | Train Loss: 0.4825, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5268, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 026 | Train Loss: 0.5068, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5714, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 027 | Train Loss: 0.5590, Acc: 0.7589, F1: 0.8029 | Val Loss: 0.5188, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 028 | Train Loss: 0.4762, Acc: 0.8036, F1: 0.8625 | Val Loss: 0.5153, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 029 | Train Loss: 0.4997, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5298, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 030 | Train Loss: 0.4668, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5077, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 031 | Train Loss: 0.4429, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5171, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4569, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5212, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4403, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.5251, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4547, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5256, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4700, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.5236, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 036 | Train Loss: 0.4180, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5545, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 037 | Train Loss: 0.4979, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.5226, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4369, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5385, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 039 | Train Loss: 0.4544, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5199, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 040 | Train Loss: 0.4520, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5265, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 041 | Train Loss: 0.4430, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5223, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 042 | Train Loss: 0.4448, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5131, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 043 | Train Loss: 0.4681, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5402, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 044 | Train Loss: 0.4634, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.5175, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 045 | Train Loss: 0.4479, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5197, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 046 | Train Loss: 0.4431, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5504, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 047 | Train Loss: 0.4432, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5212, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 048 | Train Loss: 0.4298, Acc: 0.8125, F1: 0.8645 | Val Loss: 0.5251, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 049 | Train Loss: 0.4750, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5217, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 49 epochs.\n",
      "\n",
      ">>> Running sage hidden=256, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 5.7829, Acc: 0.5536, F1: 0.6377 | Val Loss: 1.2682, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.8547, Acc: 0.4821, F1: 0.5246 | Val Loss: 0.6335, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6722, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6253, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6659, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6255, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6441, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6384, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6408, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5964, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6066, Acc: 0.6786, F1: 0.8022 | Val Loss: 0.6100, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6354, Acc: 0.6607, F1: 0.7889 | Val Loss: 0.6089, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 009 | Train Loss: 0.5833, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5722, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 010 | Train Loss: 0.5275, Acc: 0.7143, F1: 0.8000 | Val Loss: 0.5446, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 011 | Train Loss: 0.5108, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.6038, Acc: 0.7632, F1: 0.8525\n",
      "Epoch 012 | Train Loss: 0.5751, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.6042, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 013 | Train Loss: 0.4886, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5568, Acc: 0.7632, F1: 0.8475\n",
      "Epoch 014 | Train Loss: 0.4966, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5957, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 015 | Train Loss: 0.5199, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5098, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.4641, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.5305, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 017 | Train Loss: 0.4898, Acc: 0.7143, F1: 0.7975 | Val Loss: 0.4947, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 018 | Train Loss: 0.4811, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5097, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 019 | Train Loss: 0.4726, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4928, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 020 | Train Loss: 0.4820, Acc: 0.7143, F1: 0.8025 | Val Loss: 0.4979, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 021 | Train Loss: 0.4436, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5271, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 022 | Train Loss: 0.4750, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4997, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 023 | Train Loss: 0.4548, Acc: 0.8036, F1: 0.8608 | Val Loss: 0.5038, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.4394, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5133, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4440, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.5348, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 026 | Train Loss: 0.4587, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4990, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 027 | Train Loss: 0.4456, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.4983, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4452, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5012, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4782, Acc: 0.8214, F1: 0.8701 | Val Loss: 0.4994, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 030 | Train Loss: 0.4777, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.4901, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 031 | Train Loss: 0.4573, Acc: 0.8036, F1: 0.8625 | Val Loss: 0.5025, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4889, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4969, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4538, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.4992, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4259, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5009, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4586, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5086, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 036 | Train Loss: 0.4711, Acc: 0.8214, F1: 0.8718 | Val Loss: 0.5049, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 36 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7537, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7497, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7376, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7342, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.7248, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7215, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 004 | Train Loss: 0.7147, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7115, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 005 | Train Loss: 0.7062, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7029, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 006 | Train Loss: 0.6992, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6942, Acc: 0.3947, F1: 0.3429\n",
      "Epoch 007 | Train Loss: 0.6912, Acc: 0.6161, F1: 0.7296 | Val Loss: 0.6842, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6834, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6736, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6726, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6634, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6643, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6527, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6567, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6416, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6465, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6312, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6387, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6212, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6301, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6132, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6256, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6074, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6230, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6037, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6217, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6014, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5997, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5982, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6199, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5973, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6180, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5965, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6176, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5968, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6160, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5960, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 024 | Train Loss: 0.6142, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5942, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 025 | Train Loss: 0.6128, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5925, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 026 | Train Loss: 0.6117, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5903, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 027 | Train Loss: 0.6095, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5888, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 27 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6447, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6154, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6332, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6080, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6304, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6064, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6250, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6051, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6203, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6017, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6171, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5945, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6031, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5746, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5860, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5531, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5564, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5374, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 010 | Train Loss: 0.5346, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5065, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 011 | Train Loss: 0.4943, Acc: 0.7411, F1: 0.8242 | Val Loss: 0.4995, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 012 | Train Loss: 0.4746, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5341, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 013 | Train Loss: 0.4814, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5152, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4594, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5175, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 015 | Train Loss: 0.4506, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.4989, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4522, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.4958, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 017 | Train Loss: 0.4705, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4841, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4451, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.4828, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4678, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4792, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4481, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4861, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 021 | Train Loss: 0.4487, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4825, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 022 | Train Loss: 0.4448, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4778, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4388, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4761, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4321, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4879, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 025 | Train Loss: 0.4369, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4907, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4298, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4938, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 027 | Train Loss: 0.4257, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4827, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4661, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.4966, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 029 | Train Loss: 0.4925, Acc: 0.7589, F1: 0.8029 | Val Loss: 0.4873, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 030 | Train Loss: 0.4452, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4711, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4288, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4930, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 032 | Train Loss: 0.4335, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4787, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4347, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4827, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 034 | Train Loss: 0.4225, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4786, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4226, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4769, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 036 | Train Loss: 0.4399, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4761, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 037 | Train Loss: 0.4198, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4865, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 038 | Train Loss: 0.4366, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4805, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 039 | Train Loss: 0.4392, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4711, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 040 | Train Loss: 0.4143, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4762, Acc: 0.7368, F1: 0.7826\n",
      "\n",
      "Early stopping triggered after 40 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6642, Acc: 0.6071, F1: 0.7143 | Val Loss: 0.6658, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6683, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6144, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6148, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5721, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6110, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5645, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5863, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5442, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5357, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5108, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5879, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.5792, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5725, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5423, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 009 | Train Loss: 0.5428, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5126, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.4962, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.4970, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 011 | Train Loss: 0.4881, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4948, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 012 | Train Loss: 0.5135, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.4932, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.5595, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.5384, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 014 | Train Loss: 0.4915, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5482, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 015 | Train Loss: 0.4856, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5035, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 016 | Train Loss: 0.5253, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4882, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 017 | Train Loss: 0.4713, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5145, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 018 | Train Loss: 0.4472, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5051, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 019 | Train Loss: 0.4823, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.5000, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 020 | Train Loss: 0.4625, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5010, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 021 | Train Loss: 0.5003, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5424, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.4991, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.4952, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 023 | Train Loss: 0.4703, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.5005, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 024 | Train Loss: 0.4752, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5037, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 025 | Train Loss: 0.4810, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5134, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 026 | Train Loss: 0.4648, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4949, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.4402, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5015, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 028 | Train Loss: 0.4373, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5044, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4447, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5034, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 030 | Train Loss: 0.4399, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5044, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 031 | Train Loss: 0.4431, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5022, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 032 | Train Loss: 0.4479, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.5020, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 033 | Train Loss: 0.4618, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.5009, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 034 | Train Loss: 0.4331, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5087, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 035 | Train Loss: 0.4465, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4993, Acc: 0.7368, F1: 0.7826\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6634, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6530, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6573, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6483, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6544, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6441, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6501, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6400, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6491, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6361, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6470, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6321, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6415, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6285, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6398, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6246, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6374, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6208, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6353, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6180, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6334, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6150, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6322, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6126, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6291, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6113, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6305, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6093, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6254, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6065, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6255, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6039, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6236, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6020, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6248, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6000, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6198, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5982, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6175, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5970, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6150, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5951, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7477, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6990, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.6782, Acc: 0.5179, F1: 0.6494 | Val Loss: 0.6127, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6370, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6064, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6322, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5968, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6271, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6009, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6173, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5888, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6065, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5798, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6017, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5690, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5843, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5639, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5711, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5436, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5431, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5184, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5250, Acc: 0.6696, F1: 0.7956 | Val Loss: 0.5030, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 013 | Train Loss: 0.4958, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.4838, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 014 | Train Loss: 0.4627, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4815, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4796, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4914, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 016 | Train Loss: 0.4436, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4875, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4362, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4819, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4231, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5344, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 019 | Train Loss: 0.4533, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4860, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.4429, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4800, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 021 | Train Loss: 0.4194, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4700, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4251, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4760, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 023 | Train Loss: 0.4344, Acc: 0.7946, F1: 0.8345 | Val Loss: 0.5304, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 024 | Train Loss: 0.5235, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.4576, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 025 | Train Loss: 0.4390, Acc: 0.7768, F1: 0.8175 | Val Loss: 0.4647, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 026 | Train Loss: 0.4190, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4797, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 027 | Train Loss: 0.4334, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4676, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 028 | Train Loss: 0.4169, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4608, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4061, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4554, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4191, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4511, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 031 | Train Loss: 0.4014, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4465, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.3995, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4629, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 033 | Train Loss: 0.4134, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.4508, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 034 | Train Loss: 0.3962, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.5257, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 035 | Train Loss: 0.4320, Acc: 0.7679, F1: 0.8088 | Val Loss: 0.4897, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 036 | Train Loss: 0.4324, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4904, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 037 | Train Loss: 0.4125, Acc: 0.8125, F1: 0.8467 | Val Loss: 0.4563, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 038 | Train Loss: 0.4240, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4983, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 039 | Train Loss: 0.4895, Acc: 0.7857, F1: 0.8125 | Val Loss: 0.4592, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 040 | Train Loss: 0.4751, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.4727, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 041 | Train Loss: 0.3843, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5434, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 042 | Train Loss: 0.4335, Acc: 0.8214, F1: 0.8507 | Val Loss: 0.4514, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 043 | Train Loss: 0.4264, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4469, Acc: 0.7895, F1: 0.8400\n",
      "\n",
      "Early stopping triggered after 43 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6604, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6291, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.8328, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6060, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6294, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6569, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6656, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6653, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6669, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6524, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6488, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6016, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6290, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5962, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6108, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6129, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6256, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6073, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6090, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5714, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6067, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5566, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5664, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5396, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5362, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5462, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.5388, Acc: 0.6842, F1: 0.8065\n",
      "Epoch 015 | Train Loss: 0.5475, Acc: 0.7768, F1: 0.8485 | Val Loss: 0.5932, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 016 | Train Loss: 0.5530, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4796, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 017 | Train Loss: 0.5078, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5146, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 018 | Train Loss: 0.4886, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.4866, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 019 | Train Loss: 0.4858, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5036, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 020 | Train Loss: 0.4700, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.4774, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 021 | Train Loss: 0.4683, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.4867, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 022 | Train Loss: 0.4818, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.4933, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 023 | Train Loss: 0.5631, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.5006, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 024 | Train Loss: 0.4917, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.4759, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.5549, Acc: 0.7411, F1: 0.8242 | Val Loss: 0.4759, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.5088, Acc: 0.7500, F1: 0.7941 | Val Loss: 0.5498, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 027 | Train Loss: 0.4640, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5365, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 028 | Train Loss: 0.5052, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.4868, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 029 | Train Loss: 0.4898, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4839, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4880, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4789, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4867, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.4969, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 032 | Train Loss: 0.4595, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.4844, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 033 | Train Loss: 0.4610, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.4905, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 034 | Train Loss: 0.4709, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4822, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4531, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.4720, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 036 | Train Loss: 0.4451, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.4804, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 037 | Train Loss: 0.4368, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.4723, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 038 | Train Loss: 0.4786, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.4770, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 039 | Train Loss: 0.4338, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4665, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 040 | Train Loss: 0.4605, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4657, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 041 | Train Loss: 0.4349, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4680, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 042 | Train Loss: 0.4315, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4611, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 043 | Train Loss: 0.4319, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4527, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 044 | Train Loss: 0.4223, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4528, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 045 | Train Loss: 0.4218, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4477, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 046 | Train Loss: 0.4445, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4807, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 047 | Train Loss: 0.4240, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4310, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 048 | Train Loss: 0.4035, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.5943, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 049 | Train Loss: 0.5568, Acc: 0.7411, F1: 0.7717 | Val Loss: 0.4412, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 050 | Train Loss: 0.4629, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.4192, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 051 | Train Loss: 0.4303, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4696, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 052 | Train Loss: 0.4320, Acc: 0.7857, F1: 0.8261 | Val Loss: 0.4149, Acc: 0.7895, F1: 0.8400\n",
      "\n",
      "Early stopping triggered after 52 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7055, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7033, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7008, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6986, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.6969, Acc: 0.3393, F1: 0.0513 | Val Loss: 0.6941, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 004 | Train Loss: 0.6920, Acc: 0.6161, F1: 0.7152 | Val Loss: 0.6896, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6884, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.6847, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6844, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6803, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6810, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6761, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6781, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6720, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6739, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6679, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6704, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6634, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6670, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6583, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6615, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6522, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6537, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6454, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6514, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6385, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6445, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6321, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6410, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6270, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6353, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6232, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6308, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6193, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6278, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6161, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6251, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6124, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6316, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6097, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6237, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6079, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6243, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6065, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 024 | Train Loss: 0.6246, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6051, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 24 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6588, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6302, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6364, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6146, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6385, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6104, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6317, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6081, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6290, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6097, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6237, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6057, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6180, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5959, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6069, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5820, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6006, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5706, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5835, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5504, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5595, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5322, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5299, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5066, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 013 | Train Loss: 0.4996, Acc: 0.7054, F1: 0.8024 | Val Loss: 0.5043, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 014 | Train Loss: 0.4928, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.6096, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 015 | Train Loss: 0.5497, Acc: 0.7143, F1: 0.8118 | Val Loss: 0.5095, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 016 | Train Loss: 0.5016, Acc: 0.7589, F1: 0.7970 | Val Loss: 0.5033, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 017 | Train Loss: 0.4563, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5101, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 018 | Train Loss: 0.4812, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.4842, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4999, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.4964, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 020 | Train Loss: 0.4944, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.4948, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.4646, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.4852, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4522, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4817, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4615, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4811, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4539, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4803, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 025 | Train Loss: 0.4285, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4842, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 026 | Train Loss: 0.4407, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4753, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4186, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4774, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4471, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4738, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4332, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4696, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4454, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4681, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 031 | Train Loss: 0.4272, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4777, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4155, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4706, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4167, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4903, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 034 | Train Loss: 0.4321, Acc: 0.7857, F1: 0.8286 | Val Loss: 0.4801, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4113, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.5070, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 036 | Train Loss: 0.4546, Acc: 0.8125, F1: 0.8421 | Val Loss: 0.5127, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 037 | Train Loss: 0.5684, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.4842, Acc: 0.7632, F1: 0.8235\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6765, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6816, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6689, Acc: 0.6607, F1: 0.7841 | Val Loss: 0.5910, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6437, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6070, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6500, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6502, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6471, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6038, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6175, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5805, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5937, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5641, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5887, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5404, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5621, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5270, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5589, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5149, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.6444, Acc: 0.5982, F1: 0.6281 | Val Loss: 0.5944, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 012 | Train Loss: 0.5588, Acc: 0.6964, F1: 0.7976 | Val Loss: 0.5419, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 013 | Train Loss: 0.5470, Acc: 0.7232, F1: 0.8229 | Val Loss: 0.5612, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 014 | Train Loss: 0.5358, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5126, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.5426, Acc: 0.6696, F1: 0.7978 | Val Loss: 0.5008, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 016 | Train Loss: 0.4971, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.4916, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 017 | Train Loss: 0.4879, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.4830, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 018 | Train Loss: 0.4959, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.4987, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 019 | Train Loss: 0.4685, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4656, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 020 | Train Loss: 0.4715, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5978, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 021 | Train Loss: 0.4963, Acc: 0.7679, F1: 0.8088 | Val Loss: 0.5235, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 022 | Train Loss: 0.5544, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.4597, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 023 | Train Loss: 0.4794, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.4610, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 024 | Train Loss: 0.4884, Acc: 0.7857, F1: 0.8286 | Val Loss: 0.4560, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4810, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4720, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 026 | Train Loss: 0.4744, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4718, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 027 | Train Loss: 0.4992, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5470, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 028 | Train Loss: 0.4610, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4985, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 029 | Train Loss: 0.4436, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5119, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 030 | Train Loss: 0.4468, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4568, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 031 | Train Loss: 0.4067, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5967, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 032 | Train Loss: 0.5126, Acc: 0.7768, F1: 0.8092 | Val Loss: 0.5056, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 033 | Train Loss: 0.5004, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5446, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 034 | Train Loss: 0.4912, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4383, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 035 | Train Loss: 0.4789, Acc: 0.7857, F1: 0.8209 | Val Loss: 0.4398, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 036 | Train Loss: 0.4867, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4455, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 037 | Train Loss: 0.4212, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4404, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 038 | Train Loss: 0.4150, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4375, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 039 | Train Loss: 0.4353, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.4313, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 39 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6711, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6582, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6602, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6487, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6499, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6387, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6410, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6293, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6345, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6203, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6303, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6124, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6237, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6064, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6289, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6023, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6176, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5986, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6217, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5959, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6154, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5938, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6184, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5920, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5973, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5905, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6149, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5893, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6006, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5878, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6129, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5860, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6034, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5834, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6036, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5814, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6159, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5804, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5955, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5776, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5933, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5751, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6430, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6083, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6338, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6039, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5924, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6060, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5848, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5942, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5746, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5873, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5640, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5754, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5509, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5526, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5450, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 009 | Train Loss: 0.5416, Acc: 0.7054, F1: 0.8156 | Val Loss: 0.5221, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 010 | Train Loss: 0.5086, Acc: 0.7143, F1: 0.8072 | Val Loss: 0.5178, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 011 | Train Loss: 0.5068, Acc: 0.7143, F1: 0.8095 | Val Loss: 0.5285, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 012 | Train Loss: 0.4680, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5937, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 013 | Train Loss: 0.5155, Acc: 0.7411, F1: 0.7820 | Val Loss: 0.5294, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 014 | Train Loss: 0.5317, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5196, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 015 | Train Loss: 0.4561, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5672, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 016 | Train Loss: 0.5176, Acc: 0.7411, F1: 0.7852 | Val Loss: 0.5024, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4974, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5535, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 018 | Train Loss: 0.4945, Acc: 0.7946, F1: 0.8606 | Val Loss: 0.5004, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4773, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5190, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 020 | Train Loss: 0.4633, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.4987, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 021 | Train Loss: 0.4641, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5057, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4780, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5180, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 023 | Train Loss: 0.4580, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4997, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4596, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4976, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4677, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4912, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4418, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4913, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.4319, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4972, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 028 | Train Loss: 0.4450, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4882, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4310, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4884, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4344, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4915, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 031 | Train Loss: 0.4471, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4895, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.4162, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4853, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 033 | Train Loss: 0.4371, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4820, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 034 | Train Loss: 0.4410, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4826, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4559, Acc: 0.7857, F1: 0.8286 | Val Loss: 0.4837, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 036 | Train Loss: 0.4907, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5020, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 037 | Train Loss: 0.4553, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5175, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 038 | Train Loss: 0.4482, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.4807, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 039 | Train Loss: 0.4288, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4891, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 040 | Train Loss: 0.4526, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4876, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 041 | Train Loss: 0.4478, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4736, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 042 | Train Loss: 0.4470, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4726, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 043 | Train Loss: 0.4349, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4744, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 044 | Train Loss: 0.4607, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4858, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 045 | Train Loss: 0.4472, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4805, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 046 | Train Loss: 0.4255, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4744, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 047 | Train Loss: 0.4301, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4720, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 048 | Train Loss: 0.4413, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.4730, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 049 | Train Loss: 0.4238, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4785, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 050 | Train Loss: 0.4245, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4771, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 051 | Train Loss: 0.4241, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4744, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 052 | Train Loss: 0.4353, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4705, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 053 | Train Loss: 0.4292, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4672, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 054 | Train Loss: 0.4168, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4699, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 055 | Train Loss: 0.4385, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.5032, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 056 | Train Loss: 0.4594, Acc: 0.7946, F1: 0.8321 | Val Loss: 0.4899, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 057 | Train Loss: 0.4464, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4954, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 058 | Train Loss: 0.4219, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5112, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 059 | Train Loss: 0.4400, Acc: 0.7857, F1: 0.8209 | Val Loss: 0.4605, Acc: 0.7368, F1: 0.7917\n",
      "\n",
      "Early stopping triggered after 59 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6758, Acc: 0.5357, F1: 0.6623 | Val Loss: 0.6797, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6781, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6428, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6349, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6238, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6503, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6210, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6401, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6348, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6444, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6318, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6506, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6259, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6473, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6258, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6403, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6234, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6363, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6209, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6407, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6203, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6359, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6206, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6193, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6046, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6099, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5869, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5865, Acc: 0.6518, F1: 0.7797 | Val Loss: 0.5670, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 016 | Train Loss: 0.5475, Acc: 0.7054, F1: 0.8136 | Val Loss: 0.5421, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 017 | Train Loss: 0.5509, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5449, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 018 | Train Loss: 0.6330, Acc: 0.6607, F1: 0.7912 | Val Loss: 0.5679, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 019 | Train Loss: 0.5375, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5730, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 020 | Train Loss: 0.5424, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5196, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.5263, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5168, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 022 | Train Loss: 0.4729, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5157, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.5051, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5198, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.5252, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5173, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.5028, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5170, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.5084, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5156, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 027 | Train Loss: 0.4728, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5338, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 028 | Train Loss: 0.5374, Acc: 0.7143, F1: 0.7949 | Val Loss: 0.5455, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 029 | Train Loss: 0.5306, Acc: 0.7232, F1: 0.8098 | Val Loss: 0.5423, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.5112, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5372, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 031 | Train Loss: 0.4990, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.5287, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 032 | Train Loss: 0.5074, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5293, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 033 | Train Loss: 0.4784, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5335, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 034 | Train Loss: 0.4583, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5404, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 035 | Train Loss: 0.4972, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5419, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 036 | Train Loss: 0.4787, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5468, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 037 | Train Loss: 0.4775, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5473, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 038 | Train Loss: 0.4661, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5431, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 039 | Train Loss: 0.4957, Acc: 0.7054, F1: 0.7755 | Val Loss: 0.5414, Acc: 0.6579, F1: 0.7451\n",
      "\n",
      "Early stopping triggered after 39 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7077, Acc: 0.3929, F1: 0.4603 | Val Loss: 0.6805, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6743, Acc: 0.6339, F1: 0.7602 | Val Loss: 0.6668, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6670, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6577, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6618, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6509, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6587, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6454, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6521, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6402, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6553, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6360, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6452, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6330, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6462, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6300, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6421, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6277, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6406, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6257, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6417, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6240, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6403, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6225, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6347, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6208, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6386, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6194, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6355, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6181, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6415, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6167, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6313, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6158, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6312, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6145, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6368, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6138, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6285, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6131, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6685, Acc: 0.6429, F1: 0.7468 | Val Loss: 0.6018, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6161, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5982, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6220, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5988, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6295, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6176, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6139, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5758, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5873, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5794, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6111, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5664, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5815, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5899, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5756, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5349, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5364, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5195, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5428, Acc: 0.6696, F1: 0.7861 | Val Loss: 0.5132, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.4706, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.4952, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 013 | Train Loss: 0.4876, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5132, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 014 | Train Loss: 0.5020, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.4885, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4894, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5628, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 016 | Train Loss: 0.5520, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4846, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4562, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5107, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 018 | Train Loss: 0.4592, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4824, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 019 | Train Loss: 0.4733, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.4929, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 020 | Train Loss: 0.4761, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5425, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 021 | Train Loss: 0.4994, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.4807, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.4868, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5022, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4595, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.4864, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4714, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4922, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 025 | Train Loss: 0.4760, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4878, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 026 | Train Loss: 0.4483, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4835, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4488, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4839, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 028 | Train Loss: 0.5018, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.4778, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4700, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4841, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 030 | Train Loss: 0.4664, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4725, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 031 | Train Loss: 0.4478, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4719, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.4395, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4877, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 033 | Train Loss: 0.4575, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4747, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 034 | Train Loss: 0.4441, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4747, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4047, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.4726, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.4458, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4699, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 037 | Train Loss: 0.4552, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4684, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.4470, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4710, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 039 | Train Loss: 0.4177, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4672, Acc: 0.7368, F1: 0.7826\n",
      "\n",
      "Early stopping triggered after 39 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6979, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6263, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6514, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.6302, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6408, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6065, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6258, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5667, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5912, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5863, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6558, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.6364, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5908, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5804, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5777, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5457, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5454, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5056, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 010 | Train Loss: 0.5228, Acc: 0.7679, F1: 0.8471 | Val Loss: 0.4923, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 011 | Train Loss: 0.4893, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.4974, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 012 | Train Loss: 0.4739, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5274, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 013 | Train Loss: 0.4937, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5430, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 014 | Train Loss: 0.5365, Acc: 0.7589, F1: 0.8058 | Val Loss: 0.4906, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 015 | Train Loss: 0.4825, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5296, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.5475, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5420, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 017 | Train Loss: 0.4835, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.5112, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 018 | Train Loss: 0.4726, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4927, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 019 | Train Loss: 0.4624, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4929, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 020 | Train Loss: 0.5006, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5128, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 021 | Train Loss: 0.4440, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5132, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 022 | Train Loss: 0.4905, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5003, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 023 | Train Loss: 0.4686, Acc: 0.7679, F1: 0.8060 | Val Loss: 0.4990, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 024 | Train Loss: 0.5191, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4884, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 025 | Train Loss: 0.4977, Acc: 0.7321, F1: 0.7887 | Val Loss: 0.4949, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 026 | Train Loss: 0.5004, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.5628, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 027 | Train Loss: 0.5009, Acc: 0.7232, F1: 0.8000 | Val Loss: 0.5195, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 028 | Train Loss: 0.4997, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4880, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4334, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4885, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4900, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4865, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 031 | Train Loss: 0.4819, Acc: 0.7411, F1: 0.7883 | Val Loss: 0.4928, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4353, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.5448, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 033 | Train Loss: 0.4666, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4965, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 034 | Train Loss: 0.4889, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4913, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 035 | Train Loss: 0.4774, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.4900, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.5126, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4834, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 037 | Train Loss: 0.4536, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.4805, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.5258, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4953, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 38 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6929, Acc: 0.5089, F1: 0.6154 | Val Loss: 0.6799, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6734, Acc: 0.6429, F1: 0.7531 | Val Loss: 0.6685, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6732, Acc: 0.6429, F1: 0.7826 | Val Loss: 0.6580, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6699, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6490, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6518, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6419, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6544, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6360, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6409, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6298, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6345, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6233, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6432, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6171, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6373, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6116, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6470, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6090, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6244, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6064, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6318, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6049, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6387, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6043, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6104, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6033, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6285, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6021, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6303, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6008, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6285, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5995, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6380, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5987, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6244, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5984, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6341, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5986, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6649, Acc: 0.6429, F1: 0.7674 | Val Loss: 0.6154, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6287, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6090, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6442, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6034, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6422, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6029, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6344, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5990, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5994, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5794, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5985, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5637, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5837, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5520, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5599, Acc: 0.6696, F1: 0.7956 | Val Loss: 0.5233, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5889, Acc: 0.6964, F1: 0.7952 | Val Loss: 0.5062, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 011 | Train Loss: 0.5230, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.5138, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 012 | Train Loss: 0.5104, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5850, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 013 | Train Loss: 0.5206, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.4968, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 014 | Train Loss: 0.4910, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.4988, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 015 | Train Loss: 0.5322, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5264, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 016 | Train Loss: 0.5443, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.4906, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 017 | Train Loss: 0.4793, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5012, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 018 | Train Loss: 0.4891, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.4853, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 019 | Train Loss: 0.5532, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5080, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 020 | Train Loss: 0.4943, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4827, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.4789, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5011, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 022 | Train Loss: 0.5474, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5421, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 023 | Train Loss: 0.4772, Acc: 0.8036, F1: 0.8429 | Val Loss: 0.4886, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4627, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5103, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.5558, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.4905, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.5086, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5149, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 027 | Train Loss: 0.5081, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4934, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 028 | Train Loss: 0.4749, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4839, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 029 | Train Loss: 0.4602, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4884, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 030 | Train Loss: 0.4804, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.4820, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 031 | Train Loss: 0.4893, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4916, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4952, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4808, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4786, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4811, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 034 | Train Loss: 0.4460, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4793, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4643, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4786, Acc: 0.7368, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running gin hidden=16, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6695, Acc: 0.5625, F1: 0.6797 | Val Loss: 0.6175, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6274, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5869, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6274, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5753, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6487, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6420, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6490, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6281, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6355, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6230, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6389, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6244, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6428, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6245, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6412, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6244, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6409, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6252, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6376, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6187, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6345, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6173, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6332, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6134, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6368, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6035, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6349, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5977, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6111, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5899, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6272, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5800, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6123, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6572, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6533, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6593, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6600, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6475, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6325, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5611, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6613, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6388, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6403, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6181, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6244, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6023, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6156, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5936, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6092, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5881, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6057, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5846, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6027, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5809, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5994, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5780, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5960, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5767, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5915, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5733, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5891, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5689, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5855, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5656, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5811, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5626, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5756, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5614, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5721, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5588, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5665, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5543, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5622, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5490, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5546, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5441, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5519, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5393, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5466, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5419, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 021 | Train Loss: 0.5384, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5323, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 022 | Train Loss: 0.5275, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5271, Acc: 0.6579, F1: 0.7869\n",
      "Epoch 023 | Train Loss: 0.5202, Acc: 0.7054, F1: 0.8156 | Val Loss: 0.5234, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.5127, Acc: 0.7232, F1: 0.8144 | Val Loss: 0.5201, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 025 | Train Loss: 0.5051, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5152, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4995, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5170, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 027 | Train Loss: 0.4884, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5103, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 028 | Train Loss: 0.4843, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5101, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 029 | Train Loss: 0.4732, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5209, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4853, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5137, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4655, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5112, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 032 | Train Loss: 0.4678, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5114, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 033 | Train Loss: 0.4635, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5144, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 034 | Train Loss: 0.4605, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5166, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 035 | Train Loss: 0.4623, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5146, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 036 | Train Loss: 0.4567, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5157, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 037 | Train Loss: 0.4573, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5150, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 038 | Train Loss: 0.4556, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5134, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 039 | Train Loss: 0.4593, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5153, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 040 | Train Loss: 0.4505, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5130, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 40 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6197, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6287, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6406, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6147, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6124, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5795, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6005, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5717, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5694, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5781, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 006 | Train Loss: 0.5633, Acc: 0.7232, F1: 0.8229 | Val Loss: 0.5316, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.5190, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.5387, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 008 | Train Loss: 0.5357, Acc: 0.7321, F1: 0.7826 | Val Loss: 0.5400, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 009 | Train Loss: 0.5844, Acc: 0.7054, F1: 0.8070 | Val Loss: 0.5152, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 010 | Train Loss: 0.5061, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5599, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 011 | Train Loss: 0.4962, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5138, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 012 | Train Loss: 0.4943, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5025, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 013 | Train Loss: 0.4700, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5083, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4728, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4983, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 015 | Train Loss: 0.4593, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5015, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4575, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5009, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4481, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4993, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4482, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5043, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 019 | Train Loss: 0.4560, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5055, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4519, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4979, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 021 | Train Loss: 0.4605, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4919, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4528, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4890, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 023 | Train Loss: 0.4336, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4934, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 024 | Train Loss: 0.4479, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4905, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 025 | Train Loss: 0.4445, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.4875, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 026 | Train Loss: 0.4282, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4978, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 027 | Train Loss: 0.4355, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4848, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 028 | Train Loss: 0.4354, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4858, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4730, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4822, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 030 | Train Loss: 0.5059, Acc: 0.7500, F1: 0.7941 | Val Loss: 0.4937, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 031 | Train Loss: 0.4991, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5275, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 032 | Train Loss: 0.4576, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5395, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 033 | Train Loss: 0.4787, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.4908, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 034 | Train Loss: 0.4495, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5035, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 035 | Train Loss: 0.4477, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4976, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 036 | Train Loss: 0.4376, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5106, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 037 | Train Loss: 0.4508, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5020, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 038 | Train Loss: 0.4407, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5028, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 039 | Train Loss: 0.4442, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5070, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 040 | Train Loss: 0.4334, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5137, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 041 | Train Loss: 0.4531, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5032, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 042 | Train Loss: 0.4360, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4985, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 043 | Train Loss: 0.4342, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4904, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 044 | Train Loss: 0.4410, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4935, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 045 | Train Loss: 0.4254, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4918, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 046 | Train Loss: 0.4402, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4911, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 047 | Train Loss: 0.4291, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4888, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 048 | Train Loss: 0.4311, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4877, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 049 | Train Loss: 0.4223, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4919, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 050 | Train Loss: 0.4263, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4930, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 051 | Train Loss: 0.4232, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.4994, Acc: 0.7368, F1: 0.7826\n",
      "\n",
      "Early stopping triggered after 51 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.9144, Acc: 0.6339, F1: 0.7760 | Val Loss: 0.6723, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6500, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6161, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6600, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6381, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6376, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5935, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6120, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5733, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5878, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5440, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5778, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5303, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5636, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6771, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 009 | Train Loss: 0.7046, Acc: 0.5000, F1: 0.3913 | Val Loss: 0.7276, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 010 | Train Loss: 0.7156, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.6852, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6731, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6522, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6553, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6318, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6447, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6245, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6429, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6237, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6433, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6241, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6442, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6240, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6434, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6237, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6426, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6240, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6405, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6244, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6433, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6260, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6410, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6257, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6407, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6246, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6412, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6239, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 024 | Train Loss: 0.6411, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6241, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 025 | Train Loss: 0.6440, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6252, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 026 | Train Loss: 0.6409, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6250, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 027 | Train Loss: 0.6407, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6251, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 028 | Train Loss: 0.6411, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6260, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6802, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6710, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6691, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6601, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6614, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6482, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6503, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6345, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6381, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6220, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6261, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6116, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6217, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6058, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6173, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6139, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5997, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6131, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5976, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6092, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5955, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6085, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5928, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6050, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5907, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6021, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5884, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5985, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5856, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5937, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5814, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5941, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5788, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5875, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5756, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5797, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5729, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5754, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5690, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5714, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5640, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6997, Acc: 0.5000, F1: 0.5410 | Val Loss: 0.6109, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6317, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6035, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6309, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5985, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6165, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5832, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5960, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5676, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5796, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5629, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5697, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5303, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5422, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.5229, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 009 | Train Loss: 0.4955, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.5096, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 010 | Train Loss: 0.4949, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5124, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 011 | Train Loss: 0.5042, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4912, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.4983, Acc: 0.7411, F1: 0.7914 | Val Loss: 0.4862, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.4919, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.4841, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4548, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5152, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 015 | Train Loss: 0.4566, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.5013, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 016 | Train Loss: 0.4691, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4933, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4434, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5052, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 018 | Train Loss: 0.4472, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.5035, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4675, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4806, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4290, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4861, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 021 | Train Loss: 0.4393, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.5010, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 022 | Train Loss: 0.5039, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4864, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4499, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4977, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 024 | Train Loss: 0.4466, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5107, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 025 | Train Loss: 0.4669, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5096, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 026 | Train Loss: 0.4375, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4914, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4608, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4849, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4329, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4948, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 029 | Train Loss: 0.4413, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4780, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4384, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4731, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 30 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7330, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6225, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6955, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6302, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6701, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6608, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6581, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6405, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6504, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6204, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6346, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6182, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6372, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6114, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6338, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6041, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6142, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5805, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5976, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5499, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5397, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5059, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 012 | Train Loss: 0.4920, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.9673, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 013 | Train Loss: 0.6499, Acc: 0.7232, F1: 0.7704 | Val Loss: 0.5244, Acc: 0.6316, F1: 0.7308\n",
      "Epoch 014 | Train Loss: 0.4761, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5266, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.4849, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5023, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.4801, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.4991, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 017 | Train Loss: 0.4715, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5098, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4616, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5110, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 019 | Train Loss: 0.4848, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5047, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 020 | Train Loss: 0.4507, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5032, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 021 | Train Loss: 0.4569, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5042, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 022 | Train Loss: 0.4625, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5076, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 023 | Train Loss: 0.4532, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5250, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 024 | Train Loss: 0.4499, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.4990, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 025 | Train Loss: 0.4596, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5000, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 026 | Train Loss: 0.4454, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5016, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 027 | Train Loss: 0.4790, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5007, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 028 | Train Loss: 0.4517, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4958, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 029 | Train Loss: 0.4135, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5364, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4730, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4854, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 031 | Train Loss: 0.4509, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.4986, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4324, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5097, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 033 | Train Loss: 0.4322, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4876, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 034 | Train Loss: 0.4812, Acc: 0.7768, F1: 0.8175 | Val Loss: 0.4950, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 035 | Train Loss: 0.4726, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5404, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 036 | Train Loss: 0.5029, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5009, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 037 | Train Loss: 0.4232, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4874, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4475, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4789, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 039 | Train Loss: 0.4559, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4782, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 040 | Train Loss: 0.4318, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5291, Acc: 0.7368, F1: 0.7826\n",
      "\n",
      "Early stopping triggered after 40 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7266, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7115, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7026, Acc: 0.3661, F1: 0.0779 | Val Loss: 0.6871, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 003 | Train Loss: 0.6809, Acc: 0.6964, F1: 0.8090 | Val Loss: 0.6642, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6585, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6417, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6409, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6220, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6311, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6068, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6300, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6015, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6207, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6000, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6237, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5985, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6151, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5973, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6169, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5961, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6186, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5958, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6092, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5948, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6149, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5917, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6180, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5895, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6096, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5870, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6028, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5845, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6024, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5821, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6032, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5790, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5976, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5758, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5969, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5730, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.5892, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5706, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 22 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6649, Acc: 0.6250, F1: 0.7273 | Val Loss: 0.6038, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6343, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5996, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6184, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6163, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6256, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5977, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6080, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5793, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5908, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5695, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5884, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5525, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5627, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5325, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5243, Acc: 0.6964, F1: 0.8090 | Val Loss: 0.5229, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 010 | Train Loss: 0.4892, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5148, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.4626, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5063, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.4501, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5464, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.4714, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5551, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 014 | Train Loss: 0.5574, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4889, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.5784, Acc: 0.7321, F1: 0.7794 | Val Loss: 0.4997, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 016 | Train Loss: 0.5122, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5324, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 017 | Train Loss: 0.4908, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5043, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.5046, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5151, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4910, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4909, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 020 | Train Loss: 0.4668, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.4899, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.4576, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4816, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 022 | Train Loss: 0.4516, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4808, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 023 | Train Loss: 0.4362, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4866, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 024 | Train Loss: 0.4631, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4842, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4375, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4805, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 026 | Train Loss: 0.4530, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4848, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4638, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4872, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4707, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4924, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 029 | Train Loss: 0.4486, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4994, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4477, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4926, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 031 | Train Loss: 0.4423, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.4782, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.4271, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4717, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 033 | Train Loss: 0.4307, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4684, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 034 | Train Loss: 0.4557, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4704, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4341, Acc: 0.8214, F1: 0.8592 | Val Loss: 0.4657, Acc: 0.7368, F1: 0.7826\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7283, Acc: 0.5893, F1: 0.7013 | Val Loss: 0.6655, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6517, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6111, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6371, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6026, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6218, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5838, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5927, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5429, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5246, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.7262, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 007 | Train Loss: 0.6456, Acc: 0.5714, F1: 0.5789 | Val Loss: 0.5618, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5945, Acc: 0.7143, F1: 0.8118 | Val Loss: 0.5343, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 009 | Train Loss: 0.5014, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.5165, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 010 | Train Loss: 0.5000, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.4824, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 011 | Train Loss: 0.4654, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4851, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.5044, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.6178, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 013 | Train Loss: 0.4938, Acc: 0.7857, F1: 0.8286 | Val Loss: 0.5293, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 014 | Train Loss: 0.4942, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.4823, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 015 | Train Loss: 0.4559, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4839, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 016 | Train Loss: 0.4348, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4995, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 017 | Train Loss: 0.4861, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.4871, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 018 | Train Loss: 0.4473, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4916, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 019 | Train Loss: 0.4486, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.5084, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 020 | Train Loss: 0.4904, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.4906, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 021 | Train Loss: 0.5809, Acc: 0.7143, F1: 0.7500 | Val Loss: 0.4919, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 022 | Train Loss: 0.4871, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5343, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 023 | Train Loss: 0.4886, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5110, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 024 | Train Loss: 0.4690, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4962, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 025 | Train Loss: 0.4902, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5535, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 026 | Train Loss: 0.4680, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5857, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 027 | Train Loss: 0.4567, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5692, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 028 | Train Loss: 0.4796, Acc: 0.7857, F1: 0.8286 | Val Loss: 0.5065, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 029 | Train Loss: 0.4804, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.4925, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 030 | Train Loss: 0.4845, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.4887, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 031 | Train Loss: 0.4407, Acc: 0.7946, F1: 0.8369 | Val Loss: 0.5074, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4523, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.5182, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 033 | Train Loss: 0.4534, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5168, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 034 | Train Loss: 0.5115, Acc: 0.7946, F1: 0.8296 | Val Loss: 0.5027, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.5484, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.4970, Acc: 0.7368, F1: 0.7826\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6922, Acc: 0.5089, F1: 0.6043 | Val Loss: 0.6557, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6562, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6303, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6328, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6113, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6223, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6021, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6238, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5983, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6325, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5956, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6225, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5942, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6169, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5925, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6088, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5911, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6108, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5896, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6131, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5872, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6014, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5844, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6028, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5821, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6014, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5790, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5941, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5758, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5962, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5720, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5830, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5670, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5836, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5626, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5775, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5574, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5726, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5522, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5661, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5485, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6633, Acc: 0.6250, F1: 0.7308 | Val Loss: 0.6197, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6442, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6383, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6517, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6417, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6514, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6058, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6172, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5873, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6201, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5756, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6033, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5851, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5878, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5516, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5557, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5275, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5438, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5084, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 011 | Train Loss: 0.5088, Acc: 0.7500, F1: 0.8333 | Val Loss: 0.4904, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 012 | Train Loss: 0.4946, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.4917, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 013 | Train Loss: 0.5631, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.4769, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4896, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4788, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4628, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5210, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 016 | Train Loss: 0.4911, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.4911, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4472, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4893, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4376, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4946, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4469, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4942, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4482, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4888, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4452, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4889, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 022 | Train Loss: 0.4365, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4773, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4313, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4775, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 024 | Train Loss: 0.4351, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4776, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 025 | Train Loss: 0.4316, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4668, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 026 | Train Loss: 0.4534, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.4763, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 027 | Train Loss: 0.4752, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4621, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 028 | Train Loss: 0.4825, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4643, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 029 | Train Loss: 0.4322, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4722, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4500, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4701, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 031 | Train Loss: 0.4362, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.4598, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 032 | Train Loss: 0.4329, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4582, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 033 | Train Loss: 0.4088, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4675, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 034 | Train Loss: 0.3981, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.4668, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 035 | Train Loss: 0.4393, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4596, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 036 | Train Loss: 0.3927, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4511, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 037 | Train Loss: 0.4077, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4415, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.3952, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4404, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 039 | Train Loss: 0.4422, Acc: 0.7768, F1: 0.8201 | Val Loss: 0.4431, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 040 | Train Loss: 0.4078, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5336, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 041 | Train Loss: 0.4380, Acc: 0.8036, F1: 0.8406 | Val Loss: 0.4514, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 042 | Train Loss: 0.4006, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4606, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 043 | Train Loss: 0.3895, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4290, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 044 | Train Loss: 0.3822, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4365, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 045 | Train Loss: 0.3889, Acc: 0.8214, F1: 0.8592 | Val Loss: 0.4408, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 046 | Train Loss: 0.3770, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4434, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 047 | Train Loss: 0.4252, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4964, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 048 | Train Loss: 0.4124, Acc: 0.8214, F1: 0.8551 | Val Loss: 0.4292, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 049 | Train Loss: 0.3999, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4325, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 050 | Train Loss: 0.3787, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4237, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 051 | Train Loss: 0.3801, Acc: 0.8214, F1: 0.8571 | Val Loss: 0.4165, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 052 | Train Loss: 0.3981, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4295, Acc: 0.7368, F1: 0.7727\n",
      "\n",
      "Early stopping triggered after 52 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.8840, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6536, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6595, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6131, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.7014, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6353, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6502, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6417, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6494, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6341, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6440, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6278, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6415, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6247, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6411, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6241, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6409, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6243, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6420, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6253, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6433, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6245, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6413, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6262, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6407, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6266, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6413, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6262, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6419, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6257, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6415, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6263, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6413, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6258, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6404, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6243, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6408, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6240, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6422, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6237, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6424, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6237, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6671, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6481, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6504, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6383, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6401, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6264, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6377, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6179, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6243, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6130, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6280, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6104, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6347, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6086, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6359, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6071, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6226, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6065, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6275, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6060, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6350, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6048, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6195, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6023, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6231, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5981, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6255, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5941, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6076, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5900, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6095, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5856, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5964, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5837, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6051, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5786, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6028, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5736, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5762, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5688, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5715, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5622, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6491, Acc: 0.6339, F1: 0.7389 | Val Loss: 0.6086, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6564, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6047, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6511, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6309, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6317, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5876, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5979, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5754, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5944, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5620, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5782, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5453, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5500, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5314, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5257, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5134, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 010 | Train Loss: 0.4980, Acc: 0.7143, F1: 0.8202 | Val Loss: 0.4948, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.4775, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.4962, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 012 | Train Loss: 0.5490, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5053, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.4624, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4881, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4769, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4889, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 015 | Train Loss: 0.4854, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4831, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4727, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4823, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 017 | Train Loss: 0.4798, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4858, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4718, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5000, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 019 | Train Loss: 0.4523, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4795, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 020 | Train Loss: 0.4789, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4811, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 021 | Train Loss: 0.4675, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4675, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4692, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4846, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 023 | Train Loss: 0.5088, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.4762, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 024 | Train Loss: 0.4893, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4973, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 025 | Train Loss: 0.4707, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5136, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 026 | Train Loss: 0.4587, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4869, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4692, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5029, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 028 | Train Loss: 0.4585, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4880, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4552, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4899, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4640, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4943, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 031 | Train Loss: 0.4759, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.4924, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.4695, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.4906, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 033 | Train Loss: 0.4404, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4824, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 034 | Train Loss: 0.4500, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4800, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4325, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4912, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 036 | Train Loss: 0.4722, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.4733, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 037 | Train Loss: 0.4188, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4769, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 038 | Train Loss: 0.4256, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4705, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 039 | Train Loss: 0.4432, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5063, Acc: 0.6842, F1: 0.7273\n",
      "\n",
      "Early stopping triggered after 39 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.6922, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6462, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6471, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6083, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6413, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5894, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6022, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.7019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6711, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6569, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6630, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6498, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6534, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6276, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6470, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5997, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6235, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5710, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5746, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5723, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5880, Acc: 0.6786, F1: 0.8022 | Val Loss: 0.4873, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 012 | Train Loss: 0.4875, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5710, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 013 | Train Loss: 0.5644, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.5451, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 014 | Train Loss: 0.5675, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5482, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 015 | Train Loss: 0.5529, Acc: 0.7232, F1: 0.8098 | Val Loss: 0.4884, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 016 | Train Loss: 0.5243, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5338, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.5314, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.4794, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 018 | Train Loss: 0.4655, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4833, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.5154, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.6211, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 020 | Train Loss: 0.6096, Acc: 0.6964, F1: 0.7463 | Val Loss: 0.4811, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.6122, Acc: 0.7232, F1: 0.8187 | Val Loss: 0.4860, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 022 | Train Loss: 0.5601, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.5942, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 023 | Train Loss: 0.5354, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4955, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 024 | Train Loss: 0.5142, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.5136, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 025 | Train Loss: 0.5295, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5311, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 026 | Train Loss: 0.5209, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4838, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 027 | Train Loss: 0.4803, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4874, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 028 | Train Loss: 0.4838, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.4823, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 029 | Train Loss: 0.4664, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.4841, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4800, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5163, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 031 | Train Loss: 0.4873, Acc: 0.7946, F1: 0.8296 | Val Loss: 0.4862, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.4552, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4824, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 033 | Train Loss: 0.4868, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.4857, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 034 | Train Loss: 0.4875, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.4759, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 035 | Train Loss: 0.4739, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4786, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 036 | Train Loss: 0.4641, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.4743, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 037 | Train Loss: 0.5026, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4988, Acc: 0.6842, F1: 0.7273\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7671, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7517, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.7407, Acc: 0.3482, F1: 0.0267 | Val Loss: 0.7255, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.7126, Acc: 0.3929, F1: 0.2444 | Val Loss: 0.7032, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 004 | Train Loss: 0.7006, Acc: 0.4732, F1: 0.5124 | Val Loss: 0.6804, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 005 | Train Loss: 0.6722, Acc: 0.5625, F1: 0.6755 | Val Loss: 0.6589, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6519, Acc: 0.6607, F1: 0.7841 | Val Loss: 0.6381, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6368, Acc: 0.6518, F1: 0.7869 | Val Loss: 0.6218, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6372, Acc: 0.6518, F1: 0.7845 | Val Loss: 0.6116, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6328, Acc: 0.6339, F1: 0.7760 | Val Loss: 0.6052, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6320, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.6016, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6567, Acc: 0.6429, F1: 0.7826 | Val Loss: 0.6002, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6402, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.6007, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6384, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.6013, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6508, Acc: 0.6518, F1: 0.7869 | Val Loss: 0.6008, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6235, Acc: 0.6429, F1: 0.7802 | Val Loss: 0.5994, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6366, Acc: 0.6429, F1: 0.7826 | Val Loss: 0.5961, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6265, Acc: 0.6696, F1: 0.7978 | Val Loss: 0.5937, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5964, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5898, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6086, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5862, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6304, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.5859, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5851, Acc: 0.6429, F1: 0.7802 | Val Loss: 0.5833, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6274, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.5818, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6320, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.5802, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 024 | Train Loss: 0.5877, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5779, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 24 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6928, Acc: 0.5536, F1: 0.6324 | Val Loss: 0.5997, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6367, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6038, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6659, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6379, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6498, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.6165, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6314, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5807, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5997, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5726, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5926, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5662, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5820, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5454, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5780, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5515, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5717, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5209, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5270, Acc: 0.6786, F1: 0.8022 | Val Loss: 0.5089, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 012 | Train Loss: 0.5089, Acc: 0.7321, F1: 0.8295 | Val Loss: 0.4920, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 013 | Train Loss: 0.4958, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.4987, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 014 | Train Loss: 0.5154, Acc: 0.7143, F1: 0.7975 | Val Loss: 0.4998, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 015 | Train Loss: 0.4909, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.4775, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.5197, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5084, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 017 | Train Loss: 0.5770, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.4763, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4908, Acc: 0.7232, F1: 0.7770 | Val Loss: 0.5458, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 019 | Train Loss: 0.5054, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4728, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 020 | Train Loss: 0.5010, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4726, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 021 | Train Loss: 0.4624, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.4701, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 022 | Train Loss: 0.4381, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4758, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.5030, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4734, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.5169, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.4667, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 025 | Train Loss: 0.4662, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5067, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 026 | Train Loss: 0.4852, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4712, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 027 | Train Loss: 0.4875, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.4683, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 028 | Train Loss: 0.4485, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4956, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 029 | Train Loss: 0.4604, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4723, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 030 | Train Loss: 0.4666, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.4655, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4506, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5344, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 032 | Train Loss: 0.4085, Acc: 0.8125, F1: 0.8531 | Val Loss: 0.4967, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 033 | Train Loss: 0.6017, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.4622, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 034 | Train Loss: 0.4733, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4809, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 035 | Train Loss: 0.4286, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4777, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 036 | Train Loss: 0.4854, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4617, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 037 | Train Loss: 0.4307, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4584, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 038 | Train Loss: 0.4639, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4754, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 039 | Train Loss: 0.4575, Acc: 0.7946, F1: 0.8369 | Val Loss: 0.4660, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 040 | Train Loss: 0.4233, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4667, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 041 | Train Loss: 0.4160, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.4582, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 042 | Train Loss: 0.4715, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4614, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 043 | Train Loss: 0.4864, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4539, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 044 | Train Loss: 0.4414, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4929, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 045 | Train Loss: 0.4622, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.4485, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 046 | Train Loss: 0.4206, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4602, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 047 | Train Loss: 0.4578, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4589, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 048 | Train Loss: 0.4290, Acc: 0.8393, F1: 0.8750 | Val Loss: 0.4387, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 049 | Train Loss: 0.4723, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.4451, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 050 | Train Loss: 0.4524, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4476, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 051 | Train Loss: 0.3991, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4546, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 052 | Train Loss: 0.4738, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4644, Acc: 0.7105, F1: 0.7442\n",
      "\n",
      "Early stopping triggered after 52 epochs.\n",
      "\n",
      ">>> Running gin hidden=32, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 0.7408, Acc: 0.5268, F1: 0.5760 | Val Loss: 0.6688, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6196, Acc: 0.6518, F1: 0.7869 | Val Loss: 1.1371, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.7976, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6616, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6668, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6540, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6538, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6382, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6513, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6263, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6392, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6241, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6403, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6237, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6417, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6238, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6440, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6237, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6453, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6238, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6414, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6240, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6417, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6242, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6410, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6253, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6419, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6260, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6424, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6258, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6415, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6264, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6408, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6261, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6408, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6257, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6411, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6252, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6407, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6250, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6668, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6338, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6360, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6011, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6127, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5918, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6114, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5894, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6118, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5852, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6056, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5824, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5990, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5827, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5984, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5808, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5935, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5724, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5870, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5657, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5794, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5620, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5751, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5575, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5715, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5492, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5586, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5431, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5482, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5506, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 016 | Train Loss: 0.5436, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5329, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 017 | Train Loss: 0.5309, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5228, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 018 | Train Loss: 0.5188, Acc: 0.7143, F1: 0.8161 | Val Loss: 0.5207, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 019 | Train Loss: 0.5064, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5143, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 020 | Train Loss: 0.4937, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5137, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 021 | Train Loss: 0.4848, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5084, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 022 | Train Loss: 0.4763, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5091, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4654, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5097, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4648, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5146, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4637, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5166, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 026 | Train Loss: 0.4584, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5269, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 027 | Train Loss: 0.4538, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.5133, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4532, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5115, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4484, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5129, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4542, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.5041, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4988, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5162, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 032 | Train Loss: 0.4588, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5382, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 033 | Train Loss: 0.4736, Acc: 0.7857, F1: 0.8286 | Val Loss: 0.4950, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 034 | Train Loss: 0.4525, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5080, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 035 | Train Loss: 0.4631, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.4910, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 036 | Train Loss: 0.4479, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5112, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 037 | Train Loss: 0.4531, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4922, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 038 | Train Loss: 0.4427, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4948, Acc: 0.7368, F1: 0.8077\n",
      "\n",
      "Early stopping triggered after 38 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6526, Acc: 0.6161, F1: 0.7624 | Val Loss: 0.5964, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6251, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5862, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6117, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5743, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5946, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5778, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5586, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5685, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5628, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.5159, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 007 | Train Loss: 0.4966, Acc: 0.7232, F1: 0.8208 | Val Loss: 0.4911, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 008 | Train Loss: 0.4696, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5033, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 009 | Train Loss: 0.4507, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4927, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 010 | Train Loss: 0.4488, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5726, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 011 | Train Loss: 0.5831, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.4602, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 012 | Train Loss: 0.4956, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4668, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 013 | Train Loss: 0.5891, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5096, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 014 | Train Loss: 0.4692, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.6065, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 015 | Train Loss: 0.5509, Acc: 0.7768, F1: 0.8148 | Val Loss: 0.5133, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 016 | Train Loss: 0.4834, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5156, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 017 | Train Loss: 0.4956, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.4996, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 018 | Train Loss: 0.4631, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5008, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4609, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5044, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4597, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5138, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 021 | Train Loss: 0.4586, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5018, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 022 | Train Loss: 0.4505, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5068, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 023 | Train Loss: 0.4535, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4945, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 024 | Train Loss: 0.4460, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4922, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 025 | Train Loss: 0.4435, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4963, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 026 | Train Loss: 0.4469, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4979, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.4375, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5136, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 028 | Train Loss: 0.4428, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5022, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4397, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4938, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4329, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4932, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 031 | Train Loss: 0.4387, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4822, Acc: 0.7368, F1: 0.7917\n",
      "\n",
      "Early stopping triggered after 31 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 2.7791, Acc: 0.4643, F1: 0.5082 | Val Loss: 0.7243, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.8071, Acc: 0.4107, F1: 0.3774 | Val Loss: 0.6991, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.6865, Acc: 0.6071, F1: 0.7143 | Val Loss: 0.6404, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6594, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6146, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6639, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6322, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6462, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6145, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6329, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6105, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6303, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5869, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6030, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5667, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5762, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5934, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 011 | Train Loss: 0.5683, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5140, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 012 | Train Loss: 0.5090, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.6081, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 013 | Train Loss: 0.6297, Acc: 0.6607, F1: 0.7164 | Val Loss: 0.5064, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 014 | Train Loss: 0.4967, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4904, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 015 | Train Loss: 0.4727, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.4758, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 016 | Train Loss: 0.4489, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.4749, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 017 | Train Loss: 0.4407, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5625, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 018 | Train Loss: 0.4556, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.8101, Acc: 0.3684, F1: 0.1429\n",
      "Epoch 019 | Train Loss: 0.6564, Acc: 0.6071, F1: 0.6812 | Val Loss: 0.5100, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 020 | Train Loss: 0.4733, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.4630, Acc: 0.8158, F1: 0.8679\n",
      "Epoch 021 | Train Loss: 0.4557, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4757, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 022 | Train Loss: 0.4453, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4938, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 023 | Train Loss: 0.4621, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4900, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 024 | Train Loss: 0.4712, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4687, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 025 | Train Loss: 0.5070, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.6276, Acc: 0.7105, F1: 0.7317\n",
      "Epoch 026 | Train Loss: 0.5433, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.5138, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 027 | Train Loss: 0.4811, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5215, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 028 | Train Loss: 0.4574, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5122, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 029 | Train Loss: 0.5047, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4883, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4528, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4833, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 031 | Train Loss: 0.4411, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4727, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.4306, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4604, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4247, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4673, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 034 | Train Loss: 0.4377, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4551, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4047, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4486, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 036 | Train Loss: 0.4014, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4453, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 037 | Train Loss: 0.3944, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.5992, Acc: 0.7105, F1: 0.7317\n",
      "Epoch 038 | Train Loss: 0.5219, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5134, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 039 | Train Loss: 0.5029, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.5281, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 040 | Train Loss: 0.4309, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.4274, Acc: 0.7632, F1: 0.8085\n",
      "\n",
      "Early stopping triggered after 40 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6931, Acc: 0.4286, F1: 0.4839 | Val Loss: 0.6560, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6429, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6149, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6135, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5951, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6174, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5965, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6215, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5932, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6121, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5884, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6038, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5895, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6045, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5887, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5993, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5831, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5948, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5768, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5912, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5708, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5854, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5656, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5774, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5644, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5758, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5642, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5670, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5484, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5549, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5424, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5488, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5328, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 018 | Train Loss: 0.5344, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5320, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 019 | Train Loss: 0.5246, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5222, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 020 | Train Loss: 0.5069, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5100, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.4991, Acc: 0.6964, F1: 0.8000 | Val Loss: 0.5031, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 022 | Train Loss: 0.4914, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5039, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4790, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.4990, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 024 | Train Loss: 0.4749, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5013, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 025 | Train Loss: 0.4720, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5188, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 026 | Train Loss: 0.4621, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5053, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4575, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5095, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4512, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5150, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4555, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5070, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4481, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5114, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 031 | Train Loss: 0.4882, Acc: 0.7589, F1: 0.8058 | Val Loss: 0.5294, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 032 | Train Loss: 0.4693, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5119, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 033 | Train Loss: 0.4680, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.4970, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 034 | Train Loss: 0.4439, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4947, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 035 | Train Loss: 0.4403, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.4913, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 036 | Train Loss: 0.4459, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4903, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 037 | Train Loss: 0.4426, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4898, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 038 | Train Loss: 0.4470, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4914, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 039 | Train Loss: 0.4347, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4879, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 040 | Train Loss: 0.4388, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4873, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 041 | Train Loss: 0.4385, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4870, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 042 | Train Loss: 0.4468, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4977, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 043 | Train Loss: 0.4360, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4857, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 044 | Train Loss: 0.4393, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4861, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 045 | Train Loss: 0.4617, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.5133, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 046 | Train Loss: 0.4474, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4846, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 047 | Train Loss: 0.4318, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4825, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 048 | Train Loss: 0.4327, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4801, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 049 | Train Loss: 0.4466, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4974, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 050 | Train Loss: 0.4393, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4787, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 051 | Train Loss: 0.4306, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4789, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 052 | Train Loss: 0.4327, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4765, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 053 | Train Loss: 0.4244, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4803, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 054 | Train Loss: 0.4250, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4760, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 055 | Train Loss: 0.4235, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4743, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 056 | Train Loss: 0.4255, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4737, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 057 | Train Loss: 0.4241, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4749, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 058 | Train Loss: 0.4191, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4842, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 059 | Train Loss: 0.4190, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4720, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 060 | Train Loss: 0.4311, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4736, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 061 | Train Loss: 0.4215, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4797, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 062 | Train Loss: 0.4164, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4674, Acc: 0.7368, F1: 0.7917\n",
      "\n",
      "Early stopping triggered after 62 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6624, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6429, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6710, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6489, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6322, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5977, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6241, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5856, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6028, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5722, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5804, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5501, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5550, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5328, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5467, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5420, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 009 | Train Loss: 0.5142, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.4906, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.4595, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5118, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 011 | Train Loss: 0.4597, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.6240, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 012 | Train Loss: 0.5380, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.6412, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 013 | Train Loss: 0.5348, Acc: 0.7411, F1: 0.7717 | Val Loss: 0.5057, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.5514, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5053, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 015 | Train Loss: 0.5154, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.6162, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 016 | Train Loss: 0.5760, Acc: 0.7679, F1: 0.8030 | Val Loss: 0.5303, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 017 | Train Loss: 0.4753, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5072, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 018 | Train Loss: 0.4956, Acc: 0.7589, F1: 0.8402 | Val Loss: 0.5226, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 019 | Train Loss: 0.4801, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.4938, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4522, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4985, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 021 | Train Loss: 0.4531, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5081, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 022 | Train Loss: 0.4452, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5086, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 023 | Train Loss: 0.4466, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5002, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 024 | Train Loss: 0.4350, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4821, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 025 | Train Loss: 0.4474, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4739, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4893, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.4684, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 027 | Train Loss: 0.4630, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4718, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 028 | Train Loss: 0.4217, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4910, Acc: 0.7368, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.6584, Acc: 0.6429, F1: 0.7403 | Val Loss: 0.6065, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6550, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6112, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6254, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5958, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6456, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6138, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5933, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6283, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6023, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6550, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6882, Acc: 0.3929, F1: 0.3585 | Val Loss: 0.6945, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 008 | Train Loss: 0.6897, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6604, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6877, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6356, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6522, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6318, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6419, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6239, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6521, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6226, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6418, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6219, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6423, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6188, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6385, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6134, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6334, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5979, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6045, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5871, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5889, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5487, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5130, Acc: 0.7411, F1: 0.8343 | Val Loss: 0.7228, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 020 | Train Loss: 0.6732, Acc: 0.7054, F1: 0.7925 | Val Loss: 0.5718, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 021 | Train Loss: 0.5196, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.5016, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 022 | Train Loss: 0.4933, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4974, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4821, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.4795, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 024 | Train Loss: 0.4693, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.6126, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 025 | Train Loss: 0.5398, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.6035, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 026 | Train Loss: 0.4933, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4959, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 027 | Train Loss: 0.4835, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5025, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4593, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.4859, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 029 | Train Loss: 0.5124, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5110, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 030 | Train Loss: 0.4680, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4804, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4393, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4892, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4508, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.4920, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 033 | Train Loss: 0.4331, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4865, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 034 | Train Loss: 0.4305, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4937, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 035 | Train Loss: 0.4661, Acc: 0.8036, F1: 0.8429 | Val Loss: 0.4850, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 036 | Train Loss: 0.4735, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.4983, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 037 | Train Loss: 0.4927, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4908, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.4045, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.5952, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 039 | Train Loss: 0.5296, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5128, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 040 | Train Loss: 0.4595, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.4963, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 041 | Train Loss: 0.4595, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4956, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 042 | Train Loss: 0.4629, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4938, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 043 | Train Loss: 0.4367, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4947, Acc: 0.7368, F1: 0.7917\n",
      "\n",
      "Early stopping triggered after 43 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7064, Acc: 0.4107, F1: 0.2667 | Val Loss: 0.6597, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6455, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6078, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6132, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5883, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6096, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5878, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6100, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5830, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6026, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5789, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5966, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5814, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5981, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5814, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5905, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5709, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5828, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5666, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5764, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5580, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5639, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5529, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5621, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5481, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5551, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5426, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5439, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5320, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5325, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5241, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 017 | Train Loss: 0.5274, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5170, Acc: 0.6579, F1: 0.7869\n",
      "Epoch 018 | Train Loss: 0.5109, Acc: 0.7321, F1: 0.8214 | Val Loss: 0.5154, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 019 | Train Loss: 0.5035, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5053, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 020 | Train Loss: 0.4990, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5018, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 021 | Train Loss: 0.4846, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5126, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4712, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5023, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 023 | Train Loss: 0.4649, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5044, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4642, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5123, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4592, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5068, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4429, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5036, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4560, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.5116, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 028 | Train Loss: 0.4464, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.5020, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4643, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.4978, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4509, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4952, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4477, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4944, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.4448, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4936, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 033 | Train Loss: 0.4475, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.4910, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 034 | Train Loss: 0.4516, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4926, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4270, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4946, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 036 | Train Loss: 0.4726, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4911, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 037 | Train Loss: 0.4443, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5036, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 038 | Train Loss: 0.4424, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4809, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 039 | Train Loss: 0.4410, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4784, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 040 | Train Loss: 0.4398, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4858, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 041 | Train Loss: 0.4375, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4754, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 042 | Train Loss: 0.4279, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4764, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 043 | Train Loss: 0.4365, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4756, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 044 | Train Loss: 0.4285, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4805, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 045 | Train Loss: 0.4298, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4729, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 046 | Train Loss: 0.4199, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4784, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 047 | Train Loss: 0.4306, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4707, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 048 | Train Loss: 0.4214, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4709, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 049 | Train Loss: 0.4154, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4697, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 050 | Train Loss: 0.4165, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4683, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 051 | Train Loss: 0.4182, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4632, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 052 | Train Loss: 0.4150, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4618, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 053 | Train Loss: 0.4359, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4602, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 054 | Train Loss: 0.4156, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4801, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 055 | Train Loss: 0.4288, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4688, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 056 | Train Loss: 0.4137, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4764, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 057 | Train Loss: 0.4057, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4622, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 058 | Train Loss: 0.4175, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4631, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 059 | Train Loss: 0.4136, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4768, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 060 | Train Loss: 0.4115, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.4548, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 061 | Train Loss: 0.4107, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4543, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 062 | Train Loss: 0.3872, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4858, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 063 | Train Loss: 0.4147, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4562, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 064 | Train Loss: 0.3953, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4604, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 065 | Train Loss: 0.3953, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4606, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 066 | Train Loss: 0.3975, Acc: 0.8125, F1: 0.8531 | Val Loss: 0.4826, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 067 | Train Loss: 0.3962, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4483, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 068 | Train Loss: 0.3893, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4474, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 069 | Train Loss: 0.3873, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4468, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 070 | Train Loss: 0.3828, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4471, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 071 | Train Loss: 0.3855, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4453, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 072 | Train Loss: 0.3840, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4447, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 073 | Train Loss: 0.4060, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4493, Acc: 0.7632, F1: 0.8085\n",
      "\n",
      "Early stopping triggered after 73 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6839, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6100, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6452, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5898, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6241, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6275, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6254, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5796, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5932, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5692, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5893, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5572, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5644, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5414, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5356, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5166, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 009 | Train Loss: 0.5057, Acc: 0.7411, F1: 0.8242 | Val Loss: 0.4898, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.4983, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5612, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.5255, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5027, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 012 | Train Loss: 0.4731, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.4715, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.4412, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4713, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4767, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5844, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 015 | Train Loss: 0.5081, Acc: 0.7589, F1: 0.8029 | Val Loss: 0.4875, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 016 | Train Loss: 0.4723, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4992, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 017 | Train Loss: 0.4553, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4814, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 018 | Train Loss: 0.4759, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4813, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4553, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4974, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 020 | Train Loss: 0.4532, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4775, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4256, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4842, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 022 | Train Loss: 0.4371, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4697, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4470, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4720, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 024 | Train Loss: 0.4905, Acc: 0.7589, F1: 0.8058 | Val Loss: 0.5094, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4919, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4888, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 026 | Train Loss: 0.4706, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.5310, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 027 | Train Loss: 0.4307, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.5231, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 028 | Train Loss: 0.4778, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4936, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4534, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5173, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 030 | Train Loss: 0.4376, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4952, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 031 | Train Loss: 0.4793, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4739, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.4608, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.5353, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 033 | Train Loss: 0.4438, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4984, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 034 | Train Loss: 0.4583, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4895, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 035 | Train Loss: 0.4274, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4916, Acc: 0.7368, F1: 0.7826\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 5.7892, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.6510, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.8319, Acc: 0.4464, F1: 0.4918 | Val Loss: 0.6768, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6705, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6154, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6327, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6065, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5589, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5758, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5436, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5650, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5244, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 008 | Train Loss: 0.4981, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.4534, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 009 | Train Loss: 0.5190, Acc: 0.6964, F1: 0.7763 | Val Loss: 0.4547, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 010 | Train Loss: 0.4989, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4491, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 011 | Train Loss: 0.4689, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4742, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 012 | Train Loss: 0.4466, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5698, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 013 | Train Loss: 0.4674, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4721, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 014 | Train Loss: 0.4403, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5280, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 015 | Train Loss: 0.5953, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.6646, Acc: 0.6579, F1: 0.6667\n",
      "Epoch 016 | Train Loss: 0.7153, Acc: 0.4643, F1: 0.3182 | Val Loss: 0.4832, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 017 | Train Loss: 0.5761, Acc: 0.7143, F1: 0.8118 | Val Loss: 0.4778, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 018 | Train Loss: 0.5738, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.6691, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 019 | Train Loss: 0.6485, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5634, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5675, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5042, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5108, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.4994, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 022 | Train Loss: 0.4727, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.6148, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 023 | Train Loss: 0.5130, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5475, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 024 | Train Loss: 0.4613, Acc: 0.8036, F1: 0.8333 | Val Loss: 0.5969, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 025 | Train Loss: 0.6266, Acc: 0.7232, F1: 0.8000 | Val Loss: 0.5183, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 026 | Train Loss: 0.4733, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.4678, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 027 | Train Loss: 0.5220, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.4626, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 028 | Train Loss: 0.4707, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.4668, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7098, Acc: 0.3661, F1: 0.1647 | Val Loss: 0.6740, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6632, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.6305, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6306, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6025, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6177, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5966, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6209, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5932, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6083, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5901, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6009, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5926, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6074, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5932, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5990, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5819, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5925, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5732, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5870, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5677, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5864, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5645, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5710, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5583, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5574, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5522, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5480, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5480, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 016 | Train Loss: 0.5486, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5392, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 017 | Train Loss: 0.5330, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5318, Acc: 0.6579, F1: 0.7869\n",
      "Epoch 018 | Train Loss: 0.5219, Acc: 0.7143, F1: 0.8161 | Val Loss: 0.5261, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 019 | Train Loss: 0.5109, Acc: 0.7054, F1: 0.8000 | Val Loss: 0.5218, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 020 | Train Loss: 0.4915, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5256, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 021 | Train Loss: 0.4908, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.5240, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 022 | Train Loss: 0.4825, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5277, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 023 | Train Loss: 0.4815, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5292, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4830, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5352, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 025 | Train Loss: 0.5045, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5466, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 026 | Train Loss: 0.4930, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5305, Acc: 0.6579, F1: 0.7347\n",
      "Epoch 027 | Train Loss: 0.4698, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5311, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 028 | Train Loss: 0.4577, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5172, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 029 | Train Loss: 0.4661, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5152, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 030 | Train Loss: 0.4706, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5114, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 031 | Train Loss: 0.4747, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5084, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 032 | Train Loss: 0.4521, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5100, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4666, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5140, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 034 | Train Loss: 0.4688, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5124, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 035 | Train Loss: 0.4689, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5035, Acc: 0.6842, F1: 0.7600\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6681, Acc: 0.6339, F1: 0.7760 | Val Loss: 0.5991, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6180, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5878, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.5994, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5684, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5839, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6064, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5753, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5628, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6121, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5891, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 007 | Train Loss: 0.5987, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5415, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 008 | Train Loss: 0.5421, Acc: 0.6607, F1: 0.7889 | Val Loss: 0.5361, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5386, Acc: 0.7232, F1: 0.8229 | Val Loss: 0.5314, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 010 | Train Loss: 0.5101, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5036, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.4751, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.4933, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 012 | Train Loss: 0.4647, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5233, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 013 | Train Loss: 0.4640, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5099, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 014 | Train Loss: 0.4609, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4964, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 015 | Train Loss: 0.4500, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5267, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 016 | Train Loss: 0.4888, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.4792, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 017 | Train Loss: 0.4603, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4753, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 018 | Train Loss: 0.4428, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4786, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 019 | Train Loss: 0.4464, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4952, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 020 | Train Loss: 0.4384, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4915, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 021 | Train Loss: 0.4502, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4802, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 022 | Train Loss: 0.4317, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4806, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 023 | Train Loss: 0.4313, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4833, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4344, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5331, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 025 | Train Loss: 0.4501, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4956, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 026 | Train Loss: 0.4671, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4758, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 027 | Train Loss: 0.4154, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4652, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4099, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4552, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4125, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4816, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 030 | Train Loss: 0.4554, Acc: 0.7946, F1: 0.8345 | Val Loss: 0.5624, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 031 | Train Loss: 0.5320, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5295, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 032 | Train Loss: 0.4220, Acc: 0.8125, F1: 0.8467 | Val Loss: 0.5472, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 033 | Train Loss: 0.5391, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.4819, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 034 | Train Loss: 0.4336, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.5387, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 035 | Train Loss: 0.4756, Acc: 0.7857, F1: 0.8261 | Val Loss: 0.4816, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 036 | Train Loss: 0.4224, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4923, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 037 | Train Loss: 0.4530, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4824, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 038 | Train Loss: 0.4226, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4995, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 039 | Train Loss: 0.4335, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4726, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 040 | Train Loss: 0.4209, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4680, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 041 | Train Loss: 0.4270, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4652, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 042 | Train Loss: 0.4147, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4618, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 043 | Train Loss: 0.4259, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4590, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 044 | Train Loss: 0.4015, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4731, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 045 | Train Loss: 0.4230, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4636, Acc: 0.6842, F1: 0.7273\n",
      "\n",
      "Early stopping triggered after 45 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 3.2331, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.7664, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6603, Acc: 0.6607, F1: 0.7889 | Val Loss: 0.6831, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 003 | Train Loss: 0.6651, Acc: 0.6250, F1: 0.7342 | Val Loss: 0.5914, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6406, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5999, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6320, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6054, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6028, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5401, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5283, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6899, Acc: 0.5263, F1: 0.4706\n",
      "Epoch 008 | Train Loss: 0.5909, Acc: 0.6875, F1: 0.7651 | Val Loss: 0.5244, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 009 | Train Loss: 0.5267, Acc: 0.7232, F1: 0.8187 | Val Loss: 0.6129, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 010 | Train Loss: 0.5456, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5271, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.4827, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5061, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 012 | Train Loss: 0.4661, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4859, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 013 | Train Loss: 0.4930, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5058, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 014 | Train Loss: 0.4620, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5147, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 015 | Train Loss: 0.4786, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5097, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 016 | Train Loss: 0.4610, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4876, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 017 | Train Loss: 0.4812, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4838, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 018 | Train Loss: 0.4678, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.4754, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 019 | Train Loss: 0.4311, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4803, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 020 | Train Loss: 0.4274, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4880, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 021 | Train Loss: 0.4313, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5712, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.5013, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5564, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 023 | Train Loss: 0.5011, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.4873, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 024 | Train Loss: 0.4914, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.5122, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 025 | Train Loss: 0.4766, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5398, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 026 | Train Loss: 0.5193, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4968, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.4384, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5139, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.5224, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5005, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 029 | Train Loss: 0.4412, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5727, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 030 | Train Loss: 0.4992, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.6670, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 031 | Train Loss: 0.4824, Acc: 0.7589, F1: 0.8029 | Val Loss: 0.5050, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 032 | Train Loss: 0.4842, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4828, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 033 | Train Loss: 0.4463, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4834, Acc: 0.7368, F1: 0.7917\n",
      "\n",
      "Early stopping triggered after 33 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6891, Acc: 0.5357, F1: 0.6232 | Val Loss: 0.6340, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6193, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5909, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6258, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5921, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6103, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5830, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6024, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5883, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6058, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5881, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6045, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5793, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5804, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5703, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5824, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5650, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5839, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5591, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5821, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5600, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5754, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5525, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5567, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5426, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5526, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5358, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.5555, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5390, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 016 | Train Loss: 0.5369, Acc: 0.7321, F1: 0.8295 | Val Loss: 0.5297, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 017 | Train Loss: 0.5302, Acc: 0.7054, F1: 0.8092 | Val Loss: 0.5168, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 018 | Train Loss: 0.5195, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.5105, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 019 | Train Loss: 0.5116, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.5052, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.5091, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5125, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4794, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5162, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4824, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5008, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 023 | Train Loss: 0.4751, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5052, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4842, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5080, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4656, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5059, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 026 | Train Loss: 0.4799, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5082, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4647, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5082, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4641, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5188, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 029 | Train Loss: 0.4563, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5010, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4591, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5379, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 031 | Train Loss: 0.4761, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.4990, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.4552, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4998, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4319, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4995, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 034 | Train Loss: 0.4584, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4984, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 035 | Train Loss: 0.4652, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4938, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7650, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6279, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6770, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.6779, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 003 | Train Loss: 0.6740, Acc: 0.6518, F1: 0.7607 | Val Loss: 0.6419, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6411, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5899, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6142, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5833, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5894, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5684, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5761, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5598, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5733, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5465, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5566, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.5220, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5371, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5035, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 011 | Train Loss: 0.4932, Acc: 0.7411, F1: 0.8362 | Val Loss: 0.4923, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 012 | Train Loss: 0.4860, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5746, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 013 | Train Loss: 0.5452, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.8268, Acc: 0.4737, F1: 0.3750\n",
      "Epoch 014 | Train Loss: 0.5867, Acc: 0.6339, F1: 0.6496 | Val Loss: 0.5627, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 015 | Train Loss: 0.6211, Acc: 0.7411, F1: 0.8324 | Val Loss: 0.5040, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 016 | Train Loss: 0.5148, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5922, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 017 | Train Loss: 0.5526, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.5160, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 018 | Train Loss: 0.5061, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5228, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 019 | Train Loss: 0.5075, Acc: 0.7321, F1: 0.8315 | Val Loss: 0.5049, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 020 | Train Loss: 0.4895, Acc: 0.7411, F1: 0.8284 | Val Loss: 0.4974, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 021 | Train Loss: 0.4808, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4957, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4733, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.4884, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 023 | Train Loss: 0.4741, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.4908, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4730, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5013, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4781, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.4967, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4849, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4863, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4432, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4785, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 028 | Train Loss: 0.4869, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4767, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4790, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4687, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4820, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4824, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 031 | Train Loss: 0.4918, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4806, Acc: 0.7368, F1: 0.8148\n",
      "\n",
      "Early stopping triggered after 31 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.9536, Acc: 0.5893, F1: 0.7013 | Val Loss: 0.6037, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6771, Acc: 0.5804, F1: 0.7117 | Val Loss: 0.6096, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6402, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6082, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6344, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6131, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6365, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6295, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6429, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6220, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6253, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5669, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5882, Acc: 0.6607, F1: 0.7841 | Val Loss: 0.5914, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 009 | Train Loss: 0.5459, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5630, Acc: 0.6579, F1: 0.7869\n",
      "Epoch 010 | Train Loss: 0.5210, Acc: 0.7232, F1: 0.8050 | Val Loss: 0.5287, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 011 | Train Loss: 0.5759, Acc: 0.6964, F1: 0.7901 | Val Loss: 0.5516, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 012 | Train Loss: 0.5133, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5014, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 013 | Train Loss: 0.5176, Acc: 0.6875, F1: 0.7826 | Val Loss: 0.4941, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 014 | Train Loss: 0.5568, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5291, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 015 | Train Loss: 0.5191, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4885, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.5062, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.4970, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 017 | Train Loss: 0.4752, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4891, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 018 | Train Loss: 0.4871, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4950, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4587, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5024, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 020 | Train Loss: 0.4822, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5206, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 021 | Train Loss: 0.4852, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.5827, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 022 | Train Loss: 0.5874, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5352, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 023 | Train Loss: 0.5022, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4905, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.4614, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5187, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 025 | Train Loss: 0.4767, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4883, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 026 | Train Loss: 0.5192, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.4829, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.4488, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4799, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 028 | Train Loss: 0.4859, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4865, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4260, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4891, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.5242, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5202, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 031 | Train Loss: 0.4647, Acc: 0.7321, F1: 0.7887 | Val Loss: 0.5043, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.4681, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5050, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 033 | Train Loss: 0.4692, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4795, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 034 | Train Loss: 0.4466, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.4755, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4454, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.4661, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.4798, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4510, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 037 | Train Loss: 0.4374, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4592, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 038 | Train Loss: 0.4684, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.7135, Acc: 0.5789, F1: 0.5556\n",
      "Epoch 039 | Train Loss: 0.5433, Acc: 0.7321, F1: 0.7619 | Val Loss: 0.5167, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.5950, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.4693, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 041 | Train Loss: 0.4935, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4589, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4874, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4870, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 043 | Train Loss: 0.4888, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4568, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 044 | Train Loss: 0.4686, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4598, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 045 | Train Loss: 0.5604, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.4997, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 046 | Train Loss: 0.5218, Acc: 0.7768, F1: 0.8062 | Val Loss: 0.4652, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 047 | Train Loss: 0.5515, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4717, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 048 | Train Loss: 0.4278, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5113, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 049 | Train Loss: 0.4644, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4771, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 050 | Train Loss: 0.4350, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4683, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 051 | Train Loss: 0.4919, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.5144, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 052 | Train Loss: 0.4840, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4692, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 053 | Train Loss: 0.4534, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.4612, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 054 | Train Loss: 0.4327, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4479, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 055 | Train Loss: 0.4606, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4475, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 056 | Train Loss: 0.3987, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4472, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 057 | Train Loss: 0.4348, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5016, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 058 | Train Loss: 0.4397, Acc: 0.8304, F1: 0.8633 | Val Loss: 0.4626, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 059 | Train Loss: 0.4673, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4465, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 060 | Train Loss: 0.4289, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4366, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 061 | Train Loss: 0.4145, Acc: 0.7857, F1: 0.8286 | Val Loss: 0.4483, Acc: 0.7105, F1: 0.7556\n",
      "\n",
      "Early stopping triggered after 61 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.7163, Acc: 0.3839, F1: 0.2069 | Val Loss: 0.6852, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 002 | Train Loss: 0.6747, Acc: 0.6607, F1: 0.7738 | Val Loss: 0.6527, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6496, Acc: 0.6429, F1: 0.7826 | Val Loss: 0.6204, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6166, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5986, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6107, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5917, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6499, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5903, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6236, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5900, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6353, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5928, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6197, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5991, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6249, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6011, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6299, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5937, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6195, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5882, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6207, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5831, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5975, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5806, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6062, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5730, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5924, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5698, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5940, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5645, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.5772, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5610, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.5798, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5577, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.5739, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5526, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.5837, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5444, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6662, Acc: 0.6339, F1: 0.7355 | Val Loss: 0.5870, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6625, Acc: 0.6071, F1: 0.7500 | Val Loss: 0.6517, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6422, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.5870, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6370, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5858, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6144, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6014, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5915, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5639, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5898, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5623, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5980, Acc: 0.6964, F1: 0.8111 | Val Loss: 0.5352, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6093, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5235, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5438, Acc: 0.7589, F1: 0.8421 | Val Loss: 0.5462, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 011 | Train Loss: 0.5280, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.5075, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 012 | Train Loss: 0.4878, Acc: 0.7232, F1: 0.8187 | Val Loss: 0.4986, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 013 | Train Loss: 0.4888, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5070, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 014 | Train Loss: 0.5080, Acc: 0.7321, F1: 0.7826 | Val Loss: 0.5056, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 015 | Train Loss: 0.4992, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5004, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 016 | Train Loss: 0.4931, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4879, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 017 | Train Loss: 0.4675, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4878, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 018 | Train Loss: 0.4909, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.4781, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 019 | Train Loss: 0.4767, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4746, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 020 | Train Loss: 0.4505, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4796, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 021 | Train Loss: 0.4903, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.4812, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 022 | Train Loss: 0.4975, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.5083, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4744, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.5283, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 024 | Train Loss: 0.4488, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4991, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.5354, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.4873, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4613, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5146, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 027 | Train Loss: 0.4622, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4932, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 028 | Train Loss: 0.4677, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.4946, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4763, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4987, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 030 | Train Loss: 0.4290, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4901, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 031 | Train Loss: 0.4777, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.4792, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4200, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4679, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 033 | Train Loss: 0.4406, Acc: 0.8125, F1: 0.8531 | Val Loss: 0.4614, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 034 | Train Loss: 0.4412, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4800, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4982, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4521, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.4563, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.4495, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 037 | Train Loss: 0.4453, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.4427, Acc: 0.7105, F1: 0.7660\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running gin hidden=64, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 1.8919, Acc: 0.5268, F1: 0.6581 | Val Loss: 0.6170, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6933, Acc: 0.5536, F1: 0.6914 | Val Loss: 0.6123, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6196, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5556, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5954, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5464, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5901, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6437, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6291, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5263, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5523, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.4864, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5138, Acc: 0.7411, F1: 0.8284 | Val Loss: 0.4597, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 009 | Train Loss: 0.5534, Acc: 0.7054, F1: 0.7785 | Val Loss: 0.4650, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 010 | Train Loss: 0.5279, Acc: 0.7411, F1: 0.8242 | Val Loss: 0.6002, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 011 | Train Loss: 0.5382, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4699, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 012 | Train Loss: 0.5036, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.4667, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 013 | Train Loss: 0.4801, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.4653, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 014 | Train Loss: 0.4641, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4478, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 015 | Train Loss: 0.4723, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.6900, Acc: 0.6053, F1: 0.5946\n",
      "Epoch 016 | Train Loss: 0.7091, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5470, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 017 | Train Loss: 0.5629, Acc: 0.6786, F1: 0.7143 | Val Loss: 0.4519, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 018 | Train Loss: 0.5201, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.4461, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 019 | Train Loss: 0.5170, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.4539, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 020 | Train Loss: 0.5178, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5693, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 021 | Train Loss: 0.5114, Acc: 0.7589, F1: 0.7939 | Val Loss: 0.4778, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 022 | Train Loss: 0.4878, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4445, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 023 | Train Loss: 0.4518, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4426, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 024 | Train Loss: 0.4263, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4646, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 025 | Train Loss: 0.4667, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4725, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 026 | Train Loss: 0.5030, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.6087, Acc: 0.7105, F1: 0.7317\n",
      "Epoch 027 | Train Loss: 0.5243, Acc: 0.7500, F1: 0.7846 | Val Loss: 0.5345, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 028 | Train Loss: 0.8282, Acc: 0.7143, F1: 0.7808 | Val Loss: 0.8746, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 029 | Train Loss: 0.8139, Acc: 0.3393, F1: 0.0000 | Val Loss: 0.7357, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 030 | Train Loss: 0.7066, Acc: 0.4464, F1: 0.4259 | Val Loss: 0.6506, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 031 | Train Loss: 0.6631, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6286, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 032 | Train Loss: 0.6473, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6236, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 32 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6605, Acc: 0.5893, F1: 0.7013 | Val Loss: 0.6043, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6202, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5945, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6182, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5898, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6108, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5911, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6027, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5774, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5926, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5708, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5839, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5622, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5754, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5598, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5589, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5445, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5618, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5427, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5419, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5371, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 012 | Train Loss: 0.5314, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.5218, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 013 | Train Loss: 0.5080, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5100, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 014 | Train Loss: 0.5034, Acc: 0.7143, F1: 0.7895 | Val Loss: 0.5032, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 015 | Train Loss: 0.4914, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.5111, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 016 | Train Loss: 0.4733, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5331, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 017 | Train Loss: 0.4837, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.5079, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 018 | Train Loss: 0.5060, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5091, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 019 | Train Loss: 0.4759, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5495, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 020 | Train Loss: 0.4733, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5083, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 021 | Train Loss: 0.4749, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5012, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 022 | Train Loss: 0.4612, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.5213, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 023 | Train Loss: 0.4678, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.4934, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4468, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4928, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4444, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.4963, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 026 | Train Loss: 0.4445, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4902, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4383, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4958, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 028 | Train Loss: 0.4344, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4856, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4372, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4839, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4301, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4867, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 031 | Train Loss: 0.4303, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4771, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 032 | Train Loss: 0.4210, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4752, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4283, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4699, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 034 | Train Loss: 0.4164, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4715, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 035 | Train Loss: 0.4185, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4715, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 036 | Train Loss: 0.4165, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4758, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 037 | Train Loss: 0.4123, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4623, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 038 | Train Loss: 0.4076, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4595, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 039 | Train Loss: 0.4131, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4596, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 040 | Train Loss: 0.4128, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4577, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 041 | Train Loss: 0.4022, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4462, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 042 | Train Loss: 0.4080, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4469, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 043 | Train Loss: 0.3915, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4507, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 044 | Train Loss: 0.3905, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4378, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 045 | Train Loss: 0.3871, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4504, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 046 | Train Loss: 0.3909, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4317, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 047 | Train Loss: 0.3801, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4399, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 048 | Train Loss: 0.3730, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4337, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 049 | Train Loss: 0.3713, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4606, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 050 | Train Loss: 0.3792, Acc: 0.8304, F1: 0.8633 | Val Loss: 0.4405, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 051 | Train Loss: 0.4206, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4893, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 052 | Train Loss: 0.4229, Acc: 0.8125, F1: 0.8467 | Val Loss: 0.4401, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 053 | Train Loss: 0.3963, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4376, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 054 | Train Loss: 0.3770, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4285, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 055 | Train Loss: 0.3658, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4297, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 056 | Train Loss: 0.3657, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4264, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 057 | Train Loss: 0.3787, Acc: 0.8214, F1: 0.8571 | Val Loss: 0.4275, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 058 | Train Loss: 0.3976, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4350, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 059 | Train Loss: 0.3581, Acc: 0.8304, F1: 0.8671 | Val Loss: 0.4480, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 060 | Train Loss: 0.3521, Acc: 0.8393, F1: 0.8750 | Val Loss: 0.4177, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 061 | Train Loss: 0.3627, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4327, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 062 | Train Loss: 0.3628, Acc: 0.8482, F1: 0.8777 | Val Loss: 0.4101, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 063 | Train Loss: 0.3649, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4144, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 064 | Train Loss: 0.3576, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.4343, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 065 | Train Loss: 0.3599, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4368, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 066 | Train Loss: 0.3582, Acc: 0.8304, F1: 0.8633 | Val Loss: 0.4186, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 067 | Train Loss: 0.3690, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4098, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 068 | Train Loss: 0.3533, Acc: 0.8304, F1: 0.8633 | Val Loss: 0.4221, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 069 | Train Loss: 0.4181, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4249, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 070 | Train Loss: 0.3723, Acc: 0.8214, F1: 0.8529 | Val Loss: 0.4394, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 071 | Train Loss: 0.3294, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4058, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 072 | Train Loss: 0.3605, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4070, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 073 | Train Loss: 0.3424, Acc: 0.8571, F1: 0.8873 | Val Loss: 0.4050, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 074 | Train Loss: 0.3394, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4085, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 075 | Train Loss: 0.3331, Acc: 0.8571, F1: 0.8889 | Val Loss: 0.4659, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 076 | Train Loss: 0.3370, Acc: 0.8571, F1: 0.8857 | Val Loss: 0.4336, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 077 | Train Loss: 0.3465, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4267, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 078 | Train Loss: 0.3269, Acc: 0.8482, F1: 0.8828 | Val Loss: 0.4252, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 079 | Train Loss: 0.3286, Acc: 0.8571, F1: 0.8857 | Val Loss: 0.3982, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 080 | Train Loss: 0.3463, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4551, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 081 | Train Loss: 0.3464, Acc: 0.8661, F1: 0.8905 | Val Loss: 0.4169, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 082 | Train Loss: 0.3439, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4359, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 083 | Train Loss: 0.3262, Acc: 0.8482, F1: 0.8777 | Val Loss: 0.4174, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 084 | Train Loss: 0.3386, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.3977, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 085 | Train Loss: 0.3308, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4057, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 086 | Train Loss: 0.3184, Acc: 0.8661, F1: 0.8936 | Val Loss: 0.4514, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 087 | Train Loss: 0.3229, Acc: 0.8482, F1: 0.8811 | Val Loss: 0.4277, Acc: 0.8158, F1: 0.8511\n",
      "\n",
      "Early stopping triggered after 87 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7152, Acc: 0.5357, F1: 0.5738 | Val Loss: 0.6224, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6279, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5867, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6554, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6140, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6430, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6554, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6518, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6073, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6188, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5742, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5856, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5691, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5808, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5436, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5444, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5281, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 010 | Train Loss: 0.5138, Acc: 0.7143, F1: 0.8095 | Val Loss: 0.5270, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 011 | Train Loss: 0.5585, Acc: 0.7232, F1: 0.7832 | Val Loss: 0.5762, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 012 | Train Loss: 0.5139, Acc: 0.7500, F1: 0.8272 | Val Loss: 0.5421, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 013 | Train Loss: 0.5165, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5078, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 014 | Train Loss: 0.4744, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4976, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 015 | Train Loss: 0.4730, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5049, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4823, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5021, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4712, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.5010, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4438, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5021, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4449, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4902, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 020 | Train Loss: 0.4319, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4939, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 021 | Train Loss: 0.5165, Acc: 0.7768, F1: 0.8201 | Val Loss: 0.5843, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 022 | Train Loss: 0.4918, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5492, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 023 | Train Loss: 0.5034, Acc: 0.8036, F1: 0.8406 | Val Loss: 0.4898, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 024 | Train Loss: 0.4833, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.4973, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4391, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5246, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 026 | Train Loss: 0.4743, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4970, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4440, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5080, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 028 | Train Loss: 0.4560, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5005, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4404, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5199, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 030 | Train Loss: 0.4610, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4911, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4361, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4887, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.4366, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4816, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 033 | Train Loss: 0.4367, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4841, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 034 | Train Loss: 0.4535, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4733, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4188, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4865, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.4331, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4669, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 037 | Train Loss: 0.4166, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4626, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 038 | Train Loss: 0.4318, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4665, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 039 | Train Loss: 0.4444, Acc: 0.8304, F1: 0.8613 | Val Loss: 0.4764, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 040 | Train Loss: 0.4493, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4593, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 041 | Train Loss: 0.4024, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4557, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 042 | Train Loss: 0.4026, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4533, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 043 | Train Loss: 0.3989, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4749, Acc: 0.6842, F1: 0.7273\n",
      "\n",
      "Early stopping triggered after 43 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 11.4954, Acc: 0.4464, F1: 0.4918 | Val Loss: 0.6295, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 3.1313, Acc: 0.4554, F1: 0.5481 | Val Loss: 0.6191, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6517, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6260, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6460, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6238, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6391, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6314, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6457, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6278, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6442, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6153, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6779, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6417, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6520, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6441, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6477, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6217, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6406, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6171, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6403, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6138, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6355, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6006, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6400, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6349, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6489, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6373, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6513, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6333, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6403, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6190, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6596, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6413, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6534, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6437, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6537, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6299, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6407, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6181, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 21 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6824, Acc: 0.5179, F1: 0.6494 | Val Loss: 0.6064, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6340, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5977, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6171, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5931, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6198, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6010, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6061, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5838, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6071, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5791, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6022, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5730, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5881, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5715, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5794, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5679, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5639, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5513, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5555, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5424, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5438, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5328, Acc: 0.6579, F1: 0.7869\n",
      "Epoch 013 | Train Loss: 0.5215, Acc: 0.7411, F1: 0.8343 | Val Loss: 0.5199, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 014 | Train Loss: 0.4984, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5259, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4895, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5135, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 016 | Train Loss: 0.4874, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5193, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 017 | Train Loss: 0.4705, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5220, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4517, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5503, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 019 | Train Loss: 0.4740, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5174, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4499, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5179, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4510, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5113, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4731, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5054, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4393, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5133, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 024 | Train Loss: 0.4514, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5049, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 025 | Train Loss: 0.4504, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4926, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4458, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4959, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 027 | Train Loss: 0.4450, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5048, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 028 | Train Loss: 0.4316, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.5023, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 029 | Train Loss: 0.4764, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4943, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4407, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5137, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 031 | Train Loss: 0.4368, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4988, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 032 | Train Loss: 0.4412, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4916, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 033 | Train Loss: 0.4353, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4838, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 034 | Train Loss: 0.4453, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4930, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 035 | Train Loss: 0.4492, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4981, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.4409, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4801, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 037 | Train Loss: 0.4144, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4892, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.4266, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4773, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 039 | Train Loss: 0.4155, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4829, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 040 | Train Loss: 0.4122, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4898, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 041 | Train Loss: 0.4165, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4686, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 042 | Train Loss: 0.4039, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4663, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 043 | Train Loss: 0.4050, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4605, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 044 | Train Loss: 0.3996, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4583, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 045 | Train Loss: 0.3959, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4603, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 046 | Train Loss: 0.3968, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4604, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 047 | Train Loss: 0.4151, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4781, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 048 | Train Loss: 0.3935, Acc: 0.8304, F1: 0.8671 | Val Loss: 0.4591, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 049 | Train Loss: 0.3961, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4562, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 050 | Train Loss: 0.3827, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4490, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 051 | Train Loss: 0.3949, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4398, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 052 | Train Loss: 0.4208, Acc: 0.8036, F1: 0.8429 | Val Loss: 0.4450, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 053 | Train Loss: 0.3740, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4559, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 054 | Train Loss: 0.3716, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4694, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 055 | Train Loss: 0.4051, Acc: 0.8304, F1: 0.8633 | Val Loss: 0.4328, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 056 | Train Loss: 0.3852, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4310, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 057 | Train Loss: 0.3693, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4383, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 058 | Train Loss: 0.3531, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4444, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 059 | Train Loss: 0.3577, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4630, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 060 | Train Loss: 0.3485, Acc: 0.8482, F1: 0.8828 | Val Loss: 0.4431, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 061 | Train Loss: 0.3641, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4438, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 062 | Train Loss: 0.3463, Acc: 0.8482, F1: 0.8828 | Val Loss: 0.4294, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 063 | Train Loss: 0.3497, Acc: 0.8214, F1: 0.8684 | Val Loss: 0.4418, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 064 | Train Loss: 0.3557, Acc: 0.8393, F1: 0.8816 | Val Loss: 0.4313, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 065 | Train Loss: 0.3348, Acc: 0.8393, F1: 0.8800 | Val Loss: 0.4423, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 066 | Train Loss: 0.3440, Acc: 0.8661, F1: 0.8966 | Val Loss: 0.4542, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 067 | Train Loss: 0.3765, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4291, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 068 | Train Loss: 0.3611, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.4564, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 069 | Train Loss: 0.3383, Acc: 0.8571, F1: 0.8904 | Val Loss: 0.4232, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 070 | Train Loss: 0.3695, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4480, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 071 | Train Loss: 0.3546, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.4568, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 072 | Train Loss: 0.3450, Acc: 0.8571, F1: 0.8857 | Val Loss: 0.4276, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 073 | Train Loss: 0.3323, Acc: 0.8393, F1: 0.8784 | Val Loss: 0.4380, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 074 | Train Loss: 0.3228, Acc: 0.8571, F1: 0.8889 | Val Loss: 0.4465, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 075 | Train Loss: 0.3277, Acc: 0.8661, F1: 0.8980 | Val Loss: 0.4389, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 076 | Train Loss: 0.3211, Acc: 0.8482, F1: 0.8844 | Val Loss: 0.4194, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 077 | Train Loss: 0.3223, Acc: 0.8482, F1: 0.8874 | Val Loss: 0.4633, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 078 | Train Loss: 0.3376, Acc: 0.8839, F1: 0.9065 | Val Loss: 0.4533, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 079 | Train Loss: 0.3794, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4905, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 080 | Train Loss: 0.4019, Acc: 0.8036, F1: 0.8333 | Val Loss: 0.5154, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 081 | Train Loss: 0.5716, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.4262, Acc: 0.7368, F1: 0.8077\n",
      "\n",
      "Early stopping triggered after 81 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7203, Acc: 0.5714, F1: 0.6883 | Val Loss: 0.6300, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6294, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5942, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6526, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6434, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6372, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5824, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6103, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5695, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5842, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5558, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5611, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5378, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5481, Acc: 0.6429, F1: 0.7778 | Val Loss: 0.5436, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5607, Acc: 0.7054, F1: 0.8114 | Val Loss: 0.5102, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 010 | Train Loss: 0.4920, Acc: 0.7500, F1: 0.8333 | Val Loss: 0.4980, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.4670, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5107, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 012 | Train Loss: 0.4578, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5042, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 013 | Train Loss: 0.4607, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5092, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 014 | Train Loss: 0.4469, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4826, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4363, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4681, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4461, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4693, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4311, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4643, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 018 | Train Loss: 0.4754, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5668, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 019 | Train Loss: 0.4956, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.4691, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4317, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4916, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 021 | Train Loss: 0.4490, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4722, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4441, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.4750, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 023 | Train Loss: 0.4238, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4585, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 024 | Train Loss: 0.4163, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4516, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 025 | Train Loss: 0.4361, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4623, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 026 | Train Loss: 0.4027, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4483, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 027 | Train Loss: 0.4177, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4558, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 028 | Train Loss: 0.3830, Acc: 0.8304, F1: 0.8671 | Val Loss: 0.4763, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 029 | Train Loss: 0.4801, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4393, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 030 | Train Loss: 0.3956, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4247, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 031 | Train Loss: 0.4176, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.4227, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4110, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4596, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 033 | Train Loss: 0.4212, Acc: 0.8214, F1: 0.8571 | Val Loss: 0.4270, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 034 | Train Loss: 0.3962, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4342, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 035 | Train Loss: 0.3932, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4407, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 036 | Train Loss: 0.3785, Acc: 0.8125, F1: 0.8531 | Val Loss: 0.4325, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 037 | Train Loss: 0.3732, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4346, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 038 | Train Loss: 0.3869, Acc: 0.7946, F1: 0.8369 | Val Loss: 0.4294, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 039 | Train Loss: 0.3898, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4500, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 040 | Train Loss: 0.3739, Acc: 0.8036, F1: 0.8429 | Val Loss: 0.4219, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 041 | Train Loss: 0.3671, Acc: 0.8036, F1: 0.8429 | Val Loss: 0.4207, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 042 | Train Loss: 0.3732, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4716, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 043 | Train Loss: 0.3780, Acc: 0.7857, F1: 0.8209 | Val Loss: 0.4154, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 044 | Train Loss: 0.3615, Acc: 0.8214, F1: 0.8551 | Val Loss: 0.4251, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 045 | Train Loss: 0.3791, Acc: 0.8036, F1: 0.8358 | Val Loss: 0.4004, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 046 | Train Loss: 0.4257, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.5840, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 047 | Train Loss: 0.4027, Acc: 0.8482, F1: 0.8741 | Val Loss: 0.5138, Acc: 0.8158, F1: 0.8679\n",
      "Epoch 048 | Train Loss: 0.4604, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.5062, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 049 | Train Loss: 0.4096, Acc: 0.8125, F1: 0.8397 | Val Loss: 0.4219, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 050 | Train Loss: 0.4278, Acc: 0.8125, F1: 0.8531 | Val Loss: 0.4236, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 051 | Train Loss: 0.3597, Acc: 0.8036, F1: 0.8429 | Val Loss: 0.4062, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 052 | Train Loss: 0.3532, Acc: 0.8304, F1: 0.8633 | Val Loss: 0.4150, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 053 | Train Loss: 0.3501, Acc: 0.8304, F1: 0.8633 | Val Loss: 0.3960, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 054 | Train Loss: 0.3690, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4298, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 055 | Train Loss: 0.3656, Acc: 0.8036, F1: 0.8333 | Val Loss: 0.3984, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 056 | Train Loss: 0.3571, Acc: 0.8304, F1: 0.8671 | Val Loss: 0.4254, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 057 | Train Loss: 0.3487, Acc: 0.8036, F1: 0.8308 | Val Loss: 0.4224, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 058 | Train Loss: 0.3378, Acc: 0.8214, F1: 0.8551 | Val Loss: 0.4071, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 059 | Train Loss: 0.3401, Acc: 0.8036, F1: 0.8333 | Val Loss: 0.4388, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 060 | Train Loss: 0.3387, Acc: 0.8214, F1: 0.8507 | Val Loss: 0.4045, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 061 | Train Loss: 0.3303, Acc: 0.8304, F1: 0.8633 | Val Loss: 0.4116, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 062 | Train Loss: 0.3289, Acc: 0.8214, F1: 0.8551 | Val Loss: 0.4140, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 063 | Train Loss: 0.3266, Acc: 0.8214, F1: 0.8485 | Val Loss: 0.3972, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 064 | Train Loss: 0.3306, Acc: 0.8304, F1: 0.8633 | Val Loss: 0.3911, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 065 | Train Loss: 0.3330, Acc: 0.8125, F1: 0.8421 | Val Loss: 0.3855, Acc: 0.7368, F1: 0.7727\n",
      "\n",
      "Early stopping triggered after 65 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 14.6285, Acc: 0.5179, F1: 0.5574 | Val Loss: 2.3900, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 1.1115, Acc: 0.5089, F1: 0.6207 | Val Loss: 0.6747, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6616, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6362, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6367, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5955, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6209, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6292, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6462, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6201, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6381, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6053, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6131, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5701, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5709, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.7895, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 010 | Train Loss: 0.6339, Acc: 0.5982, F1: 0.6715 | Val Loss: 0.5622, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.5760, Acc: 0.6786, F1: 0.7907 | Val Loss: 0.5470, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 012 | Train Loss: 0.5391, Acc: 0.6964, F1: 0.8023 | Val Loss: 0.5087, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 013 | Train Loss: 0.5173, Acc: 0.7232, F1: 0.8166 | Val Loss: 0.5043, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 014 | Train Loss: 0.4953, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.4753, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 015 | Train Loss: 0.6785, Acc: 0.5625, F1: 0.6080 | Val Loss: 0.6042, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 016 | Train Loss: 0.5687, Acc: 0.7321, F1: 0.8235 | Val Loss: 0.5545, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 017 | Train Loss: 0.5183, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.4872, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 018 | Train Loss: 0.4849, Acc: 0.7232, F1: 0.8050 | Val Loss: 0.4896, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 019 | Train Loss: 0.5298, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.5988, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 020 | Train Loss: 0.5205, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.4822, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 021 | Train Loss: 0.4633, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4854, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.4821, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5613, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 023 | Train Loss: 0.6065, Acc: 0.6786, F1: 0.6949 | Val Loss: 0.4832, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 024 | Train Loss: 0.6032, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.5442, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 025 | Train Loss: 0.5014, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4895, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 026 | Train Loss: 0.4816, Acc: 0.7679, F1: 0.8434 | Val Loss: 0.5052, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 027 | Train Loss: 0.4865, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5208, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 028 | Train Loss: 0.4695, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.4967, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 029 | Train Loss: 0.5169, Acc: 0.7589, F1: 0.8344 | Val Loss: 0.5668, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 030 | Train Loss: 0.5121, Acc: 0.7589, F1: 0.8029 | Val Loss: 0.5284, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 031 | Train Loss: 0.4988, Acc: 0.7143, F1: 0.7922 | Val Loss: 0.4975, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4720, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.4954, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 033 | Train Loss: 0.4891, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4810, Acc: 0.7105, F1: 0.7755\n",
      "\n",
      "Early stopping triggered after 33 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6550, Acc: 0.6250, F1: 0.7273 | Val Loss: 0.5998, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6438, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5989, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6139, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5944, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6095, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6071, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6203, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5973, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6058, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5837, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6027, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5779, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6004, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5730, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5878, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5704, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5818, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5615, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5678, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5496, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5563, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5356, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5348, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5311, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 014 | Train Loss: 0.5167, Acc: 0.7232, F1: 0.8121 | Val Loss: 0.5148, Acc: 0.6579, F1: 0.7869\n",
      "Epoch 015 | Train Loss: 0.5060, Acc: 0.7054, F1: 0.8070 | Val Loss: 0.5128, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.5113, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5101, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4841, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5227, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 018 | Train Loss: 0.5081, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5343, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 019 | Train Loss: 0.4840, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5174, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4721, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5206, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4651, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5187, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4665, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5142, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4652, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5120, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4520, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5138, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 025 | Train Loss: 0.4610, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5038, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4580, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.5158, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 027 | Train Loss: 0.4648, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5029, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4441, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5207, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 029 | Train Loss: 0.4654, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.4905, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4431, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.4955, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 031 | Train Loss: 0.4467, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4901, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 032 | Train Loss: 0.4552, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.4940, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 033 | Train Loss: 0.4634, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4883, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 034 | Train Loss: 0.4322, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5055, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 035 | Train Loss: 0.4630, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.4866, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 036 | Train Loss: 0.4332, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4971, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 037 | Train Loss: 0.4473, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4857, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 038 | Train Loss: 0.4348, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4833, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 039 | Train Loss: 0.4337, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4811, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 040 | Train Loss: 0.4211, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4805, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 041 | Train Loss: 0.4346, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.4821, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 042 | Train Loss: 0.4528, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.4931, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 043 | Train Loss: 0.4257, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.5033, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 044 | Train Loss: 0.4670, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4829, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 045 | Train Loss: 0.4268, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4681, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 046 | Train Loss: 0.4357, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4709, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 047 | Train Loss: 0.4448, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4783, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 048 | Train Loss: 0.4764, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.4776, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 049 | Train Loss: 0.4128, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5110, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 050 | Train Loss: 0.4429, Acc: 0.7946, F1: 0.8345 | Val Loss: 0.4719, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 051 | Train Loss: 0.4496, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4954, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 052 | Train Loss: 0.4338, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4767, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 053 | Train Loss: 0.4294, Acc: 0.8036, F1: 0.8429 | Val Loss: 0.4710, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 054 | Train Loss: 0.4102, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4793, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 055 | Train Loss: 0.4383, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4635, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 056 | Train Loss: 0.4211, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4727, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 057 | Train Loss: 0.4303, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4690, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 058 | Train Loss: 0.4222, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4897, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 059 | Train Loss: 0.4055, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4696, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 060 | Train Loss: 0.4110, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4655, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 061 | Train Loss: 0.4029, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4644, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 062 | Train Loss: 0.4214, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4593, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 063 | Train Loss: 0.4099, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4613, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 064 | Train Loss: 0.4172, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4513, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 065 | Train Loss: 0.4125, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4636, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 066 | Train Loss: 0.4039, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4608, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 067 | Train Loss: 0.4069, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4483, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 068 | Train Loss: 0.3853, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4452, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 069 | Train Loss: 0.3873, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4484, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 070 | Train Loss: 0.3842, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4634, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 071 | Train Loss: 0.3857, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4465, Acc: 0.7895, F1: 0.8333\n",
      "\n",
      "Early stopping triggered after 71 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.8859, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6909, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 002 | Train Loss: 0.6815, Acc: 0.6696, F1: 0.7933 | Val Loss: 0.6219, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6425, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5931, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6129, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5894, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6043, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5818, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6057, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5744, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5835, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5631, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5723, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5713, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5871, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5798, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.6302, Acc: 0.7054, F1: 0.7785 | Val Loss: 0.5532, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 011 | Train Loss: 0.5305, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5714, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5381, Acc: 0.6875, F1: 0.8087 | Val Loss: 0.5782, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 013 | Train Loss: 0.5499, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5191, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5589, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5100, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 015 | Train Loss: 0.5221, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.4937, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 016 | Train Loss: 0.4929, Acc: 0.7857, F1: 0.8571 | Val Loss: 0.4951, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 017 | Train Loss: 0.4677, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5005, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4767, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.4813, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 019 | Train Loss: 0.4607, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.4778, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.4471, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4889, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 021 | Train Loss: 0.4518, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5012, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 022 | Train Loss: 0.4696, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5155, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 023 | Train Loss: 0.4816, Acc: 0.7768, F1: 0.8175 | Val Loss: 0.4808, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 024 | Train Loss: 0.4510, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4771, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4490, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4830, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 026 | Train Loss: 0.4275, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5056, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4394, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5225, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 028 | Train Loss: 0.4535, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4915, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 029 | Train Loss: 0.4780, Acc: 0.7768, F1: 0.8175 | Val Loss: 0.4867, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.5025, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.4765, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 031 | Train Loss: 0.4305, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4824, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4541, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4795, Acc: 0.7632, F1: 0.8163\n",
      "\n",
      "Early stopping triggered after 32 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 12.7044, Acc: 0.4732, F1: 0.5203 | Val Loss: 10.1526, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 3.6316, Acc: 0.4911, F1: 0.6122 | Val Loss: 0.6041, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6446, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6036, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6380, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6227, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6270, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5644, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5876, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5669, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5880, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5327, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5470, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5274, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5563, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5103, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5771, Acc: 0.7054, F1: 0.8114 | Val Loss: 0.5160, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.5231, Acc: 0.7054, F1: 0.8136 | Val Loss: 0.5602, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 012 | Train Loss: 0.5959, Acc: 0.6964, F1: 0.7536 | Val Loss: 0.4879, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 013 | Train Loss: 0.5543, Acc: 0.7143, F1: 0.8202 | Val Loss: 0.5621, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 014 | Train Loss: 0.5768, Acc: 0.6875, F1: 0.7682 | Val Loss: 0.7350, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6241, Acc: 0.6964, F1: 0.7976 | Val Loss: 0.5008, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 016 | Train Loss: 0.4973, Acc: 0.7679, F1: 0.8452 | Val Loss: 0.4821, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 017 | Train Loss: 0.4973, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5970, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 018 | Train Loss: 0.5588, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.4743, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 019 | Train Loss: 0.4796, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.4755, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 020 | Train Loss: 0.4581, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4708, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 021 | Train Loss: 0.4539, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4697, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4536, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4685, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4424, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4791, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 024 | Train Loss: 0.4440, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4707, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 025 | Train Loss: 0.4373, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.4696, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 026 | Train Loss: 0.4309, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4688, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 027 | Train Loss: 0.4353, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4675, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 028 | Train Loss: 0.4367, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5288, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 029 | Train Loss: 0.4550, Acc: 0.7857, F1: 0.8261 | Val Loss: 0.5601, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 030 | Train Loss: 0.4773, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.5483, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 031 | Train Loss: 0.4715, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4631, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 032 | Train Loss: 0.4286, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4589, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 033 | Train Loss: 0.4294, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4586, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 034 | Train Loss: 0.4214, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4528, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 035 | Train Loss: 0.4381, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5152, Acc: 0.7105, F1: 0.7442\n",
      "\n",
      "Early stopping triggered after 35 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6433, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5956, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6201, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5898, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6139, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5851, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6073, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5820, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5991, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5708, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5959, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5694, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5788, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5560, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5669, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5467, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5586, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5447, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.5362, Acc: 0.7143, F1: 0.8222 | Val Loss: 0.5216, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 011 | Train Loss: 0.5273, Acc: 0.6875, F1: 0.8045 | Val Loss: 0.5220, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 012 | Train Loss: 0.5093, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5296, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 013 | Train Loss: 0.4970, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5086, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 014 | Train Loss: 0.4830, Acc: 0.7321, F1: 0.8101 | Val Loss: 0.5243, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.4741, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5173, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4700, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5162, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4545, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5312, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 018 | Train Loss: 0.4787, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5167, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4569, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5141, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4808, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5051, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4791, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.5042, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4676, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5064, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4780, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5086, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 024 | Train Loss: 0.4499, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5005, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 025 | Train Loss: 0.4545, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4946, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4511, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4941, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4320, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4986, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 028 | Train Loss: 0.4703, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5007, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4452, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4970, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 030 | Train Loss: 0.4472, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4996, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 031 | Train Loss: 0.4326, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4970, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 032 | Train Loss: 0.4493, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4878, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 033 | Train Loss: 0.4401, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.4866, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 034 | Train Loss: 0.4144, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.4945, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4449, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4812, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 036 | Train Loss: 0.4192, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4732, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 037 | Train Loss: 0.4200, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4740, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 038 | Train Loss: 0.4380, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4726, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 039 | Train Loss: 0.4390, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4707, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 040 | Train Loss: 0.4612, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4654, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 041 | Train Loss: 0.4216, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4856, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 042 | Train Loss: 0.4023, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4779, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4448, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4560, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 044 | Train Loss: 0.4269, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4602, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 045 | Train Loss: 0.4057, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4568, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 046 | Train Loss: 0.4299, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4571, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 047 | Train Loss: 0.4271, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4619, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 048 | Train Loss: 0.3967, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4872, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 049 | Train Loss: 0.4142, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4553, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 050 | Train Loss: 0.3970, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4695, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 051 | Train Loss: 0.3801, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4562, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 052 | Train Loss: 0.3963, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4568, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 053 | Train Loss: 0.4335, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.4538, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 054 | Train Loss: 0.3967, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4554, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 055 | Train Loss: 0.4158, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5145, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 056 | Train Loss: 0.4152, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4515, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 057 | Train Loss: 0.4253, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4431, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 058 | Train Loss: 0.3820, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4856, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 059 | Train Loss: 0.3917, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4572, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 060 | Train Loss: 0.3946, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4670, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 061 | Train Loss: 0.3886, Acc: 0.8393, F1: 0.8750 | Val Loss: 0.4456, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 062 | Train Loss: 0.3975, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4466, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 063 | Train Loss: 0.4187, Acc: 0.8304, F1: 0.8633 | Val Loss: 0.4417, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 064 | Train Loss: 0.4478, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4573, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 065 | Train Loss: 0.4048, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.5056, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 066 | Train Loss: 0.3993, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4414, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 067 | Train Loss: 0.3956, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4576, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 068 | Train Loss: 0.3674, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4355, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 069 | Train Loss: 0.3906, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4289, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 070 | Train Loss: 0.3624, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4428, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 071 | Train Loss: 0.3706, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4363, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 072 | Train Loss: 0.3541, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4609, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 073 | Train Loss: 0.3725, Acc: 0.8571, F1: 0.8873 | Val Loss: 0.4381, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 074 | Train Loss: 0.3496, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4481, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 075 | Train Loss: 0.3472, Acc: 0.8482, F1: 0.8828 | Val Loss: 0.4302, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 076 | Train Loss: 0.3740, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.4406, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 077 | Train Loss: 0.4119, Acc: 0.8214, F1: 0.8529 | Val Loss: 0.4258, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 078 | Train Loss: 0.4123, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.4400, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 079 | Train Loss: 0.3823, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4741, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 080 | Train Loss: 0.3481, Acc: 0.8393, F1: 0.8750 | Val Loss: 0.4249, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 081 | Train Loss: 0.3789, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.4417, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 082 | Train Loss: 0.3690, Acc: 0.8661, F1: 0.8936 | Val Loss: 0.4243, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 083 | Train Loss: 0.3685, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.4270, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 084 | Train Loss: 0.3462, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.4609, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 085 | Train Loss: 0.3557, Acc: 0.8482, F1: 0.8844 | Val Loss: 0.4170, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 086 | Train Loss: 0.3605, Acc: 0.8482, F1: 0.8874 | Val Loss: 0.4303, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 087 | Train Loss: 0.3347, Acc: 0.8661, F1: 0.8980 | Val Loss: 0.4215, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 088 | Train Loss: 0.3505, Acc: 0.8304, F1: 0.8742 | Val Loss: 0.4259, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 089 | Train Loss: 0.3363, Acc: 0.8661, F1: 0.8951 | Val Loss: 0.4673, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 090 | Train Loss: 0.3405, Acc: 0.8482, F1: 0.8794 | Val Loss: 0.4294, Acc: 0.7632, F1: 0.8235\n",
      "\n",
      "Early stopping triggered after 90 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6214, Acc: 0.6429, F1: 0.6970 | Val Loss: 0.5948, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6142, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5839, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.5997, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5606, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5789, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5484, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5445, Acc: 0.6875, F1: 0.7977 | Val Loss: 0.5167, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 006 | Train Loss: 0.5104, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.6748, Acc: 0.7105, F1: 0.7317\n",
      "Epoch 007 | Train Loss: 0.5324, Acc: 0.7411, F1: 0.7852 | Val Loss: 0.5726, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 008 | Train Loss: 0.5073, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.5553, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 009 | Train Loss: 0.4578, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.5611, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 010 | Train Loss: 0.5116, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.4952, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 011 | Train Loss: 0.4648, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.4884, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.4474, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4858, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 013 | Train Loss: 0.4470, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4984, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 014 | Train Loss: 0.4477, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4697, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 015 | Train Loss: 0.4668, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5092, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 016 | Train Loss: 0.4597, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4668, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 017 | Train Loss: 0.4579, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4629, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 018 | Train Loss: 0.4197, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4686, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 019 | Train Loss: 0.4224, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4714, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 020 | Train Loss: 0.4287, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4884, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 021 | Train Loss: 0.4210, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4513, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 022 | Train Loss: 0.4309, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.4333, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 023 | Train Loss: 0.4107, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4404, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 024 | Train Loss: 0.4285, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4727, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 025 | Train Loss: 0.5262, Acc: 0.7589, F1: 0.8000 | Val Loss: 0.4299, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 026 | Train Loss: 0.4799, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4696, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 027 | Train Loss: 0.5308, Acc: 0.7232, F1: 0.7559 | Val Loss: 0.4421, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 028 | Train Loss: 0.4862, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.4485, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 029 | Train Loss: 0.4553, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5369, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 030 | Train Loss: 0.4581, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4782, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 031 | Train Loss: 0.4471, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4564, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 032 | Train Loss: 0.4408, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4796, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 033 | Train Loss: 0.4124, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4693, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 034 | Train Loss: 0.4091, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4715, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 035 | Train Loss: 0.4251, Acc: 0.7946, F1: 0.8345 | Val Loss: 0.4494, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.3933, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4315, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 037 | Train Loss: 0.3686, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4906, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 038 | Train Loss: 0.4209, Acc: 0.8125, F1: 0.8489 | Val Loss: 0.4312, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 039 | Train Loss: 0.3775, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4409, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 040 | Train Loss: 0.3865, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4408, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 041 | Train Loss: 0.3845, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.4382, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 042 | Train Loss: 0.3872, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4384, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 043 | Train Loss: 0.3739, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.4307, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 044 | Train Loss: 0.3646, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4248, Acc: 0.7105, F1: 0.7660\n",
      "\n",
      "Early stopping triggered after 44 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 9.8085, Acc: 0.4821, F1: 0.6184 | Val Loss: 3.3038, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 1.2917, Acc: 0.6250, F1: 0.7273 | Val Loss: 0.6713, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6845, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6395, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6469, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6195, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6339, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5996, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6399, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6254, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6727, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5877, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6179, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6384, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6471, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5881, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5834, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5744, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5603, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5134, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 012 | Train Loss: 0.5313, Acc: 0.7143, F1: 0.7975 | Val Loss: 0.5235, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 013 | Train Loss: 0.5505, Acc: 0.7321, F1: 0.8052 | Val Loss: 0.6126, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 014 | Train Loss: 0.5037, Acc: 0.7500, F1: 0.7941 | Val Loss: 0.7836, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 015 | Train Loss: 0.7423, Acc: 0.6339, F1: 0.7421 | Val Loss: 0.6045, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 016 | Train Loss: 0.5445, Acc: 0.6518, F1: 0.7578 | Val Loss: 0.5190, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5274, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5805, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.5137, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.4975, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 019 | Train Loss: 0.5334, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.8340, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 020 | Train Loss: 0.6412, Acc: 0.7054, F1: 0.8047 | Val Loss: 0.6677, Acc: 0.6842, F1: 0.7000\n",
      "Epoch 021 | Train Loss: 0.5712, Acc: 0.7500, F1: 0.7813 | Val Loss: 0.4935, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 022 | Train Loss: 0.5154, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.4914, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 023 | Train Loss: 0.4963, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5422, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 024 | Train Loss: 0.4970, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5090, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4446, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.5023, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4581, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.6657, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 027 | Train Loss: 0.4864, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.5073, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 028 | Train Loss: 0.4737, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4912, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4494, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4851, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 030 | Train Loss: 0.4318, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5488, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 031 | Train Loss: 0.5023, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.4875, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.4589, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4932, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 033 | Train Loss: 0.4694, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5084, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 034 | Train Loss: 0.4591, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.4969, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4432, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.4798, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 036 | Train Loss: 0.4438, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4909, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 037 | Train Loss: 0.4682, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4975, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 038 | Train Loss: 0.4247, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.6017, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 039 | Train Loss: 0.5342, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.6545, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 040 | Train Loss: 0.5296, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5216, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 041 | Train Loss: 0.4216, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5196, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 042 | Train Loss: 0.4432, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4881, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 043 | Train Loss: 0.4242, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4880, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 044 | Train Loss: 0.4335, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.5340, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 045 | Train Loss: 0.4773, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5112, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 046 | Train Loss: 0.4548, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4750, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 047 | Train Loss: 0.4445, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4672, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 048 | Train Loss: 0.4307, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4770, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 049 | Train Loss: 0.4579, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4770, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 050 | Train Loss: 0.4437, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4643, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 051 | Train Loss: 0.4205, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4678, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 052 | Train Loss: 0.4400, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4743, Acc: 0.7368, F1: 0.7917\n",
      "\n",
      "Early stopping triggered after 52 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6747, Acc: 0.5893, F1: 0.7089 | Val Loss: 0.6083, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6228, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5912, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6212, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5872, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6031, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5828, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5951, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5743, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5930, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5653, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5799, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5588, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5622, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5478, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5481, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5413, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5362, Acc: 0.7232, F1: 0.8229 | Val Loss: 0.5252, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 011 | Train Loss: 0.5116, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5132, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 012 | Train Loss: 0.4939, Acc: 0.7232, F1: 0.8121 | Val Loss: 0.5105, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 013 | Train Loss: 0.5055, Acc: 0.7143, F1: 0.8049 | Val Loss: 0.5110, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4977, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5120, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.5275, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.5255, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 016 | Train Loss: 0.4673, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5182, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 017 | Train Loss: 0.4562, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.5126, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4570, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5153, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4616, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5099, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4548, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5051, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4457, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5066, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4605, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.4991, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4441, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4990, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4316, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4973, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4347, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4972, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 026 | Train Loss: 0.4330, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4899, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4562, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4850, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 028 | Train Loss: 0.4598, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4791, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 029 | Train Loss: 0.4282, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5241, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 030 | Train Loss: 0.4424, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4819, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 031 | Train Loss: 0.4504, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4811, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4175, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5253, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 033 | Train Loss: 0.4575, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4734, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 034 | Train Loss: 0.4161, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4905, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.4490, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4757, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 036 | Train Loss: 0.4583, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.4825, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 037 | Train Loss: 0.4347, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4998, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 038 | Train Loss: 0.4551, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.4644, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 039 | Train Loss: 0.4247, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4991, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 040 | Train Loss: 0.4359, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4638, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 041 | Train Loss: 0.4265, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4613, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 042 | Train Loss: 0.4130, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4701, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 043 | Train Loss: 0.4257, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4601, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 044 | Train Loss: 0.4019, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4588, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 045 | Train Loss: 0.3957, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4592, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 046 | Train Loss: 0.3935, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4652, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 047 | Train Loss: 0.3951, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4645, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 048 | Train Loss: 0.4078, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4492, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 049 | Train Loss: 0.3786, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4539, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 050 | Train Loss: 0.3946, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4499, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 051 | Train Loss: 0.3950, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4600, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 052 | Train Loss: 0.3978, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4464, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 053 | Train Loss: 0.4003, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4423, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 054 | Train Loss: 0.3921, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4347, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 055 | Train Loss: 0.3792, Acc: 0.8393, F1: 0.8732 | Val Loss: 0.4392, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 056 | Train Loss: 0.3673, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4425, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 057 | Train Loss: 0.3715, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4479, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 058 | Train Loss: 0.3708, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4507, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 059 | Train Loss: 0.3660, Acc: 0.8571, F1: 0.8889 | Val Loss: 0.4422, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 060 | Train Loss: 0.3807, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4521, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 061 | Train Loss: 0.3662, Acc: 0.8482, F1: 0.8811 | Val Loss: 0.4460, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 062 | Train Loss: 0.3550, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4443, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 063 | Train Loss: 0.3694, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4449, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 064 | Train Loss: 0.3486, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4197, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 065 | Train Loss: 0.3744, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4326, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 066 | Train Loss: 0.3656, Acc: 0.8482, F1: 0.8828 | Val Loss: 0.4334, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 067 | Train Loss: 0.3429, Acc: 0.8393, F1: 0.8784 | Val Loss: 0.4172, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 068 | Train Loss: 0.3527, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4541, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 069 | Train Loss: 0.3545, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4357, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 070 | Train Loss: 0.3501, Acc: 0.8393, F1: 0.8732 | Val Loss: 0.4360, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 071 | Train Loss: 0.3402, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4341, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 072 | Train Loss: 0.3502, Acc: 0.8482, F1: 0.8844 | Val Loss: 0.4321, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 073 | Train Loss: 0.3391, Acc: 0.8482, F1: 0.8859 | Val Loss: 0.4103, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 074 | Train Loss: 0.3425, Acc: 0.8482, F1: 0.8828 | Val Loss: 0.4276, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 075 | Train Loss: 0.3341, Acc: 0.8571, F1: 0.8904 | Val Loss: 0.4153, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 076 | Train Loss: 0.3330, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.4404, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 077 | Train Loss: 0.3473, Acc: 0.8839, F1: 0.9078 | Val Loss: 0.4198, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 078 | Train Loss: 0.3337, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.4130, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 079 | Train Loss: 0.3581, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.4536, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 080 | Train Loss: 0.3315, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4207, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 081 | Train Loss: 0.3191, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4518, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 082 | Train Loss: 0.3399, Acc: 0.8482, F1: 0.8811 | Val Loss: 0.4311, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 083 | Train Loss: 0.3232, Acc: 0.8393, F1: 0.8750 | Val Loss: 0.4359, Acc: 0.8158, F1: 0.8571\n",
      "\n",
      "Early stopping triggered after 83 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.7714, Acc: 0.6339, F1: 0.7735 | Val Loss: 0.6824, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6733, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6365, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6661, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6081, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6221, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6279, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6331, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6046, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6185, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5886, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6284, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6010, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5940, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5695, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5893, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5590, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5493, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5221, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5066, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.6615, Acc: 0.7105, F1: 0.7317\n",
      "Epoch 012 | Train Loss: 0.5437, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5017, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 013 | Train Loss: 0.5731, Acc: 0.7054, F1: 0.7755 | Val Loss: 0.4910, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 014 | Train Loss: 0.5225, Acc: 0.7589, F1: 0.8421 | Val Loss: 0.4853, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 015 | Train Loss: 0.5261, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4993, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4783, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5087, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 017 | Train Loss: 0.4982, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4938, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 018 | Train Loss: 0.4750, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4945, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 019 | Train Loss: 0.4703, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5045, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 020 | Train Loss: 0.4412, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.5116, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 021 | Train Loss: 0.4616, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.4935, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4451, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.4756, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.4446, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4691, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.4655, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4712, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 025 | Train Loss: 0.4480, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4910, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 026 | Train Loss: 0.4435, Acc: 0.7589, F1: 0.8058 | Val Loss: 0.5078, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 027 | Train Loss: 0.4591, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5515, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 028 | Train Loss: 0.5766, Acc: 0.7143, F1: 0.7377 | Val Loss: 0.4553, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 029 | Train Loss: 0.4520, Acc: 0.7857, F1: 0.8500 | Val Loss: 0.5305, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 030 | Train Loss: 0.4715, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5047, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 031 | Train Loss: 0.4879, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4984, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4565, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5023, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 033 | Train Loss: 0.4452, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.4864, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 034 | Train Loss: 0.4247, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5023, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 035 | Train Loss: 0.4269, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4879, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 036 | Train Loss: 0.4388, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4910, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 037 | Train Loss: 0.4277, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4851, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 038 | Train Loss: 0.4314, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4774, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 039 | Train Loss: 0.4032, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4721, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 040 | Train Loss: 0.4261, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4600, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 041 | Train Loss: 0.4202, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4541, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4169, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4715, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 043 | Train Loss: 0.4097, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4381, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 044 | Train Loss: 0.3780, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4534, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 045 | Train Loss: 0.4379, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5456, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 046 | Train Loss: 0.5348, Acc: 0.7143, F1: 0.7333 | Val Loss: 0.4327, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 047 | Train Loss: 0.4442, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.4451, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 048 | Train Loss: 0.4286, Acc: 0.8214, F1: 0.8592 | Val Loss: 0.5517, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 049 | Train Loss: 0.4416, Acc: 0.8036, F1: 0.8429 | Val Loss: 0.4589, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 050 | Train Loss: 0.4241, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4500, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 051 | Train Loss: 0.3861, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4722, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 052 | Train Loss: 0.3977, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4727, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 053 | Train Loss: 0.3841, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4598, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 054 | Train Loss: 0.3576, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4439, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 055 | Train Loss: 0.3920, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4677, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 056 | Train Loss: 0.3913, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4618, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 057 | Train Loss: 0.3768, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4660, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 058 | Train Loss: 0.3968, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4575, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 059 | Train Loss: 0.3867, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4415, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 060 | Train Loss: 0.3435, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4664, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 061 | Train Loss: 0.3963, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4724, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 062 | Train Loss: 0.3813, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4328, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 063 | Train Loss: 0.3762, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4116, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 064 | Train Loss: 0.3468, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4169, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 065 | Train Loss: 0.3681, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4477, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 066 | Train Loss: 0.3618, Acc: 0.8214, F1: 0.8571 | Val Loss: 0.4464, Acc: 0.7895, F1: 0.8333\n",
      "\n",
      "Early stopping triggered after 66 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 19.9972, Acc: 0.5536, F1: 0.6429 | Val Loss: 2.1564, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 3.0067, Acc: 0.4464, F1: 0.5507 | Val Loss: 0.8278, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.7380, Acc: 0.4375, F1: 0.5468 | Val Loss: 0.6377, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6518, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6275, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.7163, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6342, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6461, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6285, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6445, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6153, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6417, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6219, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6383, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6267, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6400, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6049, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6209, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5865, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6121, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6098, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5895, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5201, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5900, Acc: 0.6875, F1: 0.7879 | Val Loss: 0.5052, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 015 | Train Loss: 0.5375, Acc: 0.6964, F1: 0.8023 | Val Loss: 0.5011, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.5722, Acc: 0.7054, F1: 0.7925 | Val Loss: 0.5800, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.5867, Acc: 0.6875, F1: 0.7904 | Val Loss: 0.4950, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 018 | Train Loss: 0.5129, Acc: 0.7857, F1: 0.8588 | Val Loss: 0.5210, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.5129, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4829, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 020 | Train Loss: 0.5056, Acc: 0.6964, F1: 0.7703 | Val Loss: 0.4779, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 021 | Train Loss: 0.5284, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5821, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 022 | Train Loss: 0.6005, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.4796, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 023 | Train Loss: 0.6326, Acc: 0.5804, F1: 0.6116 | Val Loss: 0.4984, Acc: 0.6842, F1: 0.8065\n",
      "Epoch 024 | Train Loss: 0.5598, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.6381, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.5698, Acc: 0.7143, F1: 0.8140 | Val Loss: 0.5177, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 026 | Train Loss: 0.5136, Acc: 0.7500, F1: 0.8353 | Val Loss: 0.5201, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 027 | Train Loss: 0.5527, Acc: 0.7054, F1: 0.7815 | Val Loss: 0.4930, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 028 | Train Loss: 0.5524, Acc: 0.7143, F1: 0.7922 | Val Loss: 0.4808, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 029 | Train Loss: 0.5514, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5115, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.5309, Acc: 0.7857, F1: 0.8261 | Val Loss: 0.4976, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4775, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4662, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 032 | Train Loss: 0.4660, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5019, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4661, Acc: 0.7321, F1: 0.7945 | Val Loss: 0.4638, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 034 | Train Loss: 0.5327, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.6149, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 035 | Train Loss: 0.5411, Acc: 0.7411, F1: 0.7786 | Val Loss: 0.4567, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 036 | Train Loss: 0.4706, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4603, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 037 | Train Loss: 0.4896, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4995, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.4829, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4648, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 039 | Train Loss: 0.5486, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5249, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 040 | Train Loss: 0.4992, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4752, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 041 | Train Loss: 0.4407, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4783, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 042 | Train Loss: 0.5022, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5095, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 043 | Train Loss: 0.4595, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4946, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 044 | Train Loss: 0.5093, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5259, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 045 | Train Loss: 0.4874, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4557, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 046 | Train Loss: 0.4921, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.4538, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 047 | Train Loss: 0.4651, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4811, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 048 | Train Loss: 0.4601, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5900, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 049 | Train Loss: 0.4910, Acc: 0.7232, F1: 0.7832 | Val Loss: 0.4835, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 050 | Train Loss: 0.4382, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4412, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 051 | Train Loss: 0.4466, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4442, Acc: 0.7632, F1: 0.8235\n",
      "\n",
      "Early stopping triggered after 51 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6679, Acc: 0.5625, F1: 0.6839 | Val Loss: 0.5986, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6348, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5936, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6266, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5865, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6136, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5956, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6055, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5985, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6106, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5906, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6011, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5693, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5976, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5618, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5820, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5560, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5714, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5499, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5606, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5401, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.5462, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5307, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5258, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5201, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5068, Acc: 0.7054, F1: 0.8156 | Val Loss: 0.5107, Acc: 0.6579, F1: 0.7869\n",
      "Epoch 015 | Train Loss: 0.4998, Acc: 0.6875, F1: 0.8000 | Val Loss: 0.5022, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 016 | Train Loss: 0.4855, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5044, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4873, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5046, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 018 | Train Loss: 0.4981, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5149, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 019 | Train Loss: 0.4532, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5121, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 020 | Train Loss: 0.5041, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5148, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 021 | Train Loss: 0.4558, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4965, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4983, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.4951, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 023 | Train Loss: 0.4424, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.5150, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 024 | Train Loss: 0.4902, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4904, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4685, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4925, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4526, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4968, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 027 | Train Loss: 0.4454, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4871, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4688, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4867, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4368, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4879, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4317, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.4875, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 031 | Train Loss: 0.4515, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4871, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 032 | Train Loss: 0.4409, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4801, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 033 | Train Loss: 0.4477, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4737, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 034 | Train Loss: 0.4253, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4754, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 035 | Train Loss: 0.4369, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4657, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 036 | Train Loss: 0.4300, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4618, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 037 | Train Loss: 0.4363, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4670, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.4236, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4572, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 039 | Train Loss: 0.4049, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4572, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 040 | Train Loss: 0.4225, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4639, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 041 | Train Loss: 0.4320, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4537, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 042 | Train Loss: 0.4169, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4542, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 043 | Train Loss: 0.4496, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4484, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 044 | Train Loss: 0.4268, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4458, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 045 | Train Loss: 0.4427, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4684, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 046 | Train Loss: 0.4047, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4649, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 047 | Train Loss: 0.4314, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4437, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 048 | Train Loss: 0.3922, Acc: 0.8125, F1: 0.8531 | Val Loss: 0.4492, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 049 | Train Loss: 0.4040, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4471, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 050 | Train Loss: 0.4166, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4745, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 051 | Train Loss: 0.4185, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4404, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 052 | Train Loss: 0.3853, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4344, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 053 | Train Loss: 0.3832, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4760, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 054 | Train Loss: 0.3878, Acc: 0.8125, F1: 0.8531 | Val Loss: 0.4422, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 055 | Train Loss: 0.4184, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4415, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 056 | Train Loss: 0.3922, Acc: 0.8304, F1: 0.8671 | Val Loss: 0.4506, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 057 | Train Loss: 0.3869, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4355, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 058 | Train Loss: 0.3841, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4477, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 059 | Train Loss: 0.3844, Acc: 0.7946, F1: 0.8345 | Val Loss: 0.4267, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 060 | Train Loss: 0.3810, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4251, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 061 | Train Loss: 0.3726, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4250, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 062 | Train Loss: 0.3589, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4226, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 063 | Train Loss: 0.3703, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4245, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 064 | Train Loss: 0.3604, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4318, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 065 | Train Loss: 0.3661, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.4202, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 066 | Train Loss: 0.3751, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4461, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 067 | Train Loss: 0.4124, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.4153, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 068 | Train Loss: 0.3596, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4116, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 069 | Train Loss: 0.3691, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.4278, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 070 | Train Loss: 0.3605, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.4175, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 071 | Train Loss: 0.3626, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4269, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 072 | Train Loss: 0.3439, Acc: 0.8482, F1: 0.8828 | Val Loss: 0.4236, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 073 | Train Loss: 0.3729, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4251, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 074 | Train Loss: 0.3555, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4201, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 075 | Train Loss: 0.3659, Acc: 0.8304, F1: 0.8671 | Val Loss: 0.4309, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 076 | Train Loss: 0.3667, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.4115, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 077 | Train Loss: 0.3675, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4486, Acc: 0.8421, F1: 0.8696\n",
      "Epoch 078 | Train Loss: 0.3922, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4115, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 079 | Train Loss: 0.3242, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.4143, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 080 | Train Loss: 0.3697, Acc: 0.8214, F1: 0.8551 | Val Loss: 0.4242, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 081 | Train Loss: 0.3848, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4107, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 082 | Train Loss: 0.3755, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4322, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 083 | Train Loss: 0.3118, Acc: 0.8839, F1: 0.9103 | Val Loss: 0.4035, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 084 | Train Loss: 0.3440, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.4102, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 085 | Train Loss: 0.3535, Acc: 0.8304, F1: 0.8652 | Val Loss: 0.4196, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 086 | Train Loss: 0.3939, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.4159, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 087 | Train Loss: 0.3544, Acc: 0.8125, F1: 0.8444 | Val Loss: 0.4302, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 088 | Train Loss: 0.3426, Acc: 0.8571, F1: 0.8889 | Val Loss: 0.4046, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 089 | Train Loss: 0.3626, Acc: 0.8393, F1: 0.8816 | Val Loss: 0.4369, Acc: 0.8421, F1: 0.8750\n",
      "\n",
      "Early stopping triggered after 89 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 0.6418, Acc: 0.6429, F1: 0.7368 | Val Loss: 0.5855, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6428, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5950, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.5886, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5604, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5733, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5636, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 005 | Train Loss: 0.5438, Acc: 0.6607, F1: 0.7816 | Val Loss: 0.4988, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 006 | Train Loss: 0.4973, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5579, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 007 | Train Loss: 0.5338, Acc: 0.7143, F1: 0.7975 | Val Loss: 0.5073, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 008 | Train Loss: 0.4756, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4818, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 009 | Train Loss: 0.4619, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5230, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 010 | Train Loss: 0.4845, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4730, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 011 | Train Loss: 0.4371, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4720, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 012 | Train Loss: 0.4690, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4753, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 013 | Train Loss: 0.4403, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.4611, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 014 | Train Loss: 0.4212, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4504, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 015 | Train Loss: 0.3742, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.4493, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 016 | Train Loss: 0.4415, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4627, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 017 | Train Loss: 0.4064, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4398, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 018 | Train Loss: 0.4201, Acc: 0.8036, F1: 0.8429 | Val Loss: 0.5125, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 019 | Train Loss: 0.5330, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.4538, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 020 | Train Loss: 0.4326, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4383, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 021 | Train Loss: 0.4405, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4494, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 022 | Train Loss: 0.4678, Acc: 0.7946, F1: 0.8345 | Val Loss: 0.4577, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 023 | Train Loss: 0.5811, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.6626, Acc: 0.6579, F1: 0.6667\n",
      "Epoch 024 | Train Loss: 0.5462, Acc: 0.7411, F1: 0.7680 | Val Loss: 0.4257, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 025 | Train Loss: 0.4422, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4258, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 026 | Train Loss: 0.4157, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4514, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 027 | Train Loss: 0.4066, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4347, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 028 | Train Loss: 0.4132, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5062, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 029 | Train Loss: 0.3973, Acc: 0.8125, F1: 0.8467 | Val Loss: 0.4361, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 030 | Train Loss: 0.4731, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4320, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 031 | Train Loss: 0.4215, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4181, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 032 | Train Loss: 0.3952, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4248, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 033 | Train Loss: 0.3962, Acc: 0.8036, F1: 0.8406 | Val Loss: 0.4318, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 034 | Train Loss: 0.4299, Acc: 0.8036, F1: 0.8429 | Val Loss: 0.4248, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 035 | Train Loss: 0.3837, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4404, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 036 | Train Loss: 0.4647, Acc: 0.7500, F1: 0.7778 | Val Loss: 0.4672, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 037 | Train Loss: 0.4316, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.4095, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 038 | Train Loss: 0.4038, Acc: 0.8036, F1: 0.8358 | Val Loss: 0.5755, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 039 | Train Loss: 0.4422, Acc: 0.8125, F1: 0.8397 | Val Loss: 0.4131, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 040 | Train Loss: 0.4254, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4450, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 041 | Train Loss: 0.4032, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4542, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 042 | Train Loss: 0.3675, Acc: 0.7946, F1: 0.8345 | Val Loss: 0.4231, Acc: 0.7895, F1: 0.8261\n",
      "\n",
      "Early stopping triggered after 42 epochs.\n",
      "\n",
      ">>> Running gin hidden=128, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 19.6922, Acc: 0.4554, F1: 0.5793 | Val Loss: 1.4933, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 0.9869, Acc: 0.5000, F1: 0.5410 | Val Loss: 0.6255, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6781, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6409, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6894, Acc: 0.5714, F1: 0.7108 | Val Loss: 0.6590, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6869, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6288, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6635, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6131, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6369, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6370, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6242, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5571, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6141, Acc: 0.6607, F1: 0.7935 | Val Loss: 0.5550, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5429, Acc: 0.6429, F1: 0.7778 | Val Loss: 0.5019, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5866, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.6736, Acc: 0.7105, F1: 0.7317\n",
      "Epoch 012 | Train Loss: 0.5621, Acc: 0.7054, F1: 0.7843 | Val Loss: 0.6135, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 013 | Train Loss: 0.6259, Acc: 0.7321, F1: 0.8214 | Val Loss: 0.5360, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 014 | Train Loss: 0.5582, Acc: 0.6786, F1: 0.7882 | Val Loss: 0.4849, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 015 | Train Loss: 0.5102, Acc: 0.7768, F1: 0.8466 | Val Loss: 0.5468, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 016 | Train Loss: 0.4899, Acc: 0.8304, F1: 0.8758 | Val Loss: 0.5181, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 017 | Train Loss: 0.5115, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5052, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 018 | Train Loss: 0.4973, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.5021, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 019 | Train Loss: 0.5908, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.6499, Acc: 0.7105, F1: 0.7317\n",
      "Epoch 020 | Train Loss: 0.5210, Acc: 0.7411, F1: 0.7820 | Val Loss: 0.6153, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 021 | Train Loss: 0.5988, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5006, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.5629, Acc: 0.8036, F1: 0.8642 | Val Loss: 0.5710, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 023 | Train Loss: 0.5988, Acc: 0.6339, F1: 0.6555 | Val Loss: 0.4963, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 024 | Train Loss: 0.5458, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5053, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.6042, Acc: 0.6696, F1: 0.7299 | Val Loss: 0.4953, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 026 | Train Loss: 0.5308, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5315, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 027 | Train Loss: 0.5594, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5100, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 028 | Train Loss: 0.5184, Acc: 0.7232, F1: 0.7919 | Val Loss: 0.4920, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 029 | Train Loss: 0.4981, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.4941, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 030 | Train Loss: 0.5215, Acc: 0.7411, F1: 0.8000 | Val Loss: 0.5065, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 031 | Train Loss: 0.4942, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5005, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 032 | Train Loss: 0.5826, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.4893, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 033 | Train Loss: 0.5008, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4873, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 034 | Train Loss: 0.5321, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.4871, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 035 | Train Loss: 0.4414, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4980, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 036 | Train Loss: 0.5687, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.4931, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 037 | Train Loss: 0.5342, Acc: 0.7321, F1: 0.7826 | Val Loss: 0.4908, Acc: 0.7632, F1: 0.8085\n",
      "\n",
      "Early stopping triggered after 37 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.0, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6315, Acc: 0.6696, F1: 0.7956 | Val Loss: 0.6032, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6209, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6003, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.5927, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5685, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5894, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5572, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5633, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5458, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5530, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5417, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 007 | Train Loss: 0.5686, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5211, Acc: 0.6579, F1: 0.7869\n",
      "Epoch 008 | Train Loss: 0.5433, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5285, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 009 | Train Loss: 0.4976, Acc: 0.7321, F1: 0.8214 | Val Loss: 0.5659, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 010 | Train Loss: 0.5125, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.5120, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 011 | Train Loss: 0.5026, Acc: 0.7232, F1: 0.8144 | Val Loss: 0.5168, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 012 | Train Loss: 0.4893, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5269, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 013 | Train Loss: 0.4656, Acc: 0.7500, F1: 0.8108 | Val Loss: 0.5119, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.4734, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5106, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4550, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4993, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4669, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4921, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4538, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4922, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4458, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4896, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4353, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5002, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 020 | Train Loss: 0.4519, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5272, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 021 | Train Loss: 0.5355, Acc: 0.7589, F1: 0.8364 | Val Loss: 0.4868, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 022 | Train Loss: 0.4644, Acc: 0.8036, F1: 0.8406 | Val Loss: 0.5013, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 023 | Train Loss: 0.4419, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5104, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 024 | Train Loss: 0.4585, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4896, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4639, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.5018, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 026 | Train Loss: 0.4421, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4995, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 027 | Train Loss: 0.4563, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4833, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4521, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.5187, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 029 | Train Loss: 0.4456, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4866, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 030 | Train Loss: 0.4359, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4946, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 031 | Train Loss: 0.4337, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4878, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.4336, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4879, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 033 | Train Loss: 0.4160, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4821, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 034 | Train Loss: 0.4385, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4715, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.4546, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4716, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.4157, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4729, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 037 | Train Loss: 0.4152, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4719, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.4193, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4691, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 039 | Train Loss: 0.4016, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4725, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 040 | Train Loss: 0.3958, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4734, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 041 | Train Loss: 0.3914, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4616, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 042 | Train Loss: 0.4010, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4625, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 043 | Train Loss: 0.3997, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4443, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 044 | Train Loss: 0.3787, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4492, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 045 | Train Loss: 0.3730, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4567, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 046 | Train Loss: 0.3880, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4740, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 047 | Train Loss: 0.4035, Acc: 0.8304, F1: 0.8671 | Val Loss: 0.4362, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 048 | Train Loss: 0.3586, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4611, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 049 | Train Loss: 0.3681, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4310, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 050 | Train Loss: 0.3810, Acc: 0.8304, F1: 0.8652 | Val Loss: 0.4587, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 051 | Train Loss: 0.4271, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4934, Acc: 0.7895, F1: 0.8182\n",
      "Epoch 052 | Train Loss: 0.3712, Acc: 0.8482, F1: 0.8777 | Val Loss: 0.4334, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 053 | Train Loss: 0.3840, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4311, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 054 | Train Loss: 0.3556, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4326, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 055 | Train Loss: 0.3923, Acc: 0.8214, F1: 0.8571 | Val Loss: 0.4312, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 056 | Train Loss: 0.4069, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4363, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 057 | Train Loss: 0.3760, Acc: 0.8482, F1: 0.8794 | Val Loss: 0.4227, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 058 | Train Loss: 0.3536, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.4284, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 059 | Train Loss: 0.3984, Acc: 0.8125, F1: 0.8444 | Val Loss: 0.4301, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 060 | Train Loss: 0.4137, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.4230, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 061 | Train Loss: 0.3340, Acc: 0.8571, F1: 0.8904 | Val Loss: 0.5134, Acc: 0.8158, F1: 0.8444\n",
      "Epoch 062 | Train Loss: 0.3417, Acc: 0.8571, F1: 0.8873 | Val Loss: 0.4621, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 063 | Train Loss: 0.4013, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.4372, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 064 | Train Loss: 0.3646, Acc: 0.8750, F1: 0.9014 | Val Loss: 0.4202, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 065 | Train Loss: 0.3393, Acc: 0.8304, F1: 0.8742 | Val Loss: 0.4234, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 066 | Train Loss: 0.3430, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4601, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 067 | Train Loss: 0.3237, Acc: 0.8571, F1: 0.8904 | Val Loss: 0.4182, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 068 | Train Loss: 0.3390, Acc: 0.8304, F1: 0.8742 | Val Loss: 0.4505, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 069 | Train Loss: 0.3254, Acc: 0.8571, F1: 0.8904 | Val Loss: 0.4127, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 070 | Train Loss: 0.3179, Acc: 0.8393, F1: 0.8800 | Val Loss: 0.4310, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 071 | Train Loss: 0.3170, Acc: 0.8839, F1: 0.9091 | Val Loss: 0.4317, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 072 | Train Loss: 0.3841, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.4362, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 073 | Train Loss: 0.3286, Acc: 0.8750, F1: 0.9000 | Val Loss: 0.4365, Acc: 0.7895, F1: 0.8400\n",
      "\n",
      "Early stopping triggered after 73 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.0, lr=0.01\n",
      "Epoch 001 | Train Loss: 2.0794, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6569, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6472, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6266, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6320, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5981, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6236, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5820, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5985, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5901, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5917, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5683, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5804, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.6386, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 008 | Train Loss: 0.6013, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5460, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5397, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.5234, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 010 | Train Loss: 0.4942, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.7025, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 011 | Train Loss: 0.5954, Acc: 0.7143, F1: 0.7808 | Val Loss: 0.5563, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 012 | Train Loss: 0.5905, Acc: 0.7232, F1: 0.7559 | Val Loss: 0.5036, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 013 | Train Loss: 0.5108, Acc: 0.7411, F1: 0.8343 | Val Loss: 0.5058, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 014 | Train Loss: 0.4847, Acc: 0.7411, F1: 0.8263 | Val Loss: 0.5119, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 015 | Train Loss: 0.4964, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4912, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.4518, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5334, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 017 | Train Loss: 0.4355, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5769, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 018 | Train Loss: 0.4751, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.4755, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 019 | Train Loss: 0.4219, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4673, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 020 | Train Loss: 0.4424, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4604, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 021 | Train Loss: 0.4170, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4825, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 022 | Train Loss: 0.4475, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4568, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 023 | Train Loss: 0.3976, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4753, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 024 | Train Loss: 0.3970, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.5210, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 025 | Train Loss: 0.4599, Acc: 0.7589, F1: 0.8112 | Val Loss: 0.4424, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 026 | Train Loss: 0.4114, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4355, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 027 | Train Loss: 0.3847, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4290, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 028 | Train Loss: 0.3926, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4601, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 029 | Train Loss: 0.5685, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.7838, Acc: 0.6316, F1: 0.6316\n",
      "Epoch 030 | Train Loss: 0.6619, Acc: 0.5714, F1: 0.5200 | Val Loss: 0.4706, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 031 | Train Loss: 0.4855, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.5007, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 032 | Train Loss: 0.4649, Acc: 0.7946, F1: 0.8553 | Val Loss: 0.5105, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 033 | Train Loss: 0.4690, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4694, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 034 | Train Loss: 0.4127, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5185, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 035 | Train Loss: 0.4197, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5131, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 036 | Train Loss: 0.4042, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4703, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 037 | Train Loss: 0.4132, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4648, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 038 | Train Loss: 0.4017, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5105, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 039 | Train Loss: 0.4063, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4928, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 040 | Train Loss: 0.3856, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4498, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 041 | Train Loss: 0.3769, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4474, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 042 | Train Loss: 0.3735, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4566, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 043 | Train Loss: 0.3641, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4666, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 044 | Train Loss: 0.3822, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4771, Acc: 0.7368, F1: 0.7727\n",
      "\n",
      "Early stopping triggered after 44 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.0, lr=0.05\n",
      "Epoch 001 | Train Loss: 83.3185, Acc: 0.5714, F1: 0.6364 | Val Loss: 64.2722, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 30.1279, Acc: 0.4732, F1: 0.5816 | Val Loss: 0.7068, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 1.0845, Acc: 0.6607, F1: 0.7957 | Val Loss: 2.0471, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 004 | Train Loss: 2.0088, Acc: 0.5357, F1: 0.6232 | Val Loss: 1.0274, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.7695, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6096, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6554, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6313, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.7427, Acc: 0.5804, F1: 0.6928 | Val Loss: 0.9681, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.7821, Acc: 0.6071, F1: 0.7412 | Val Loss: 0.5534, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6128, Acc: 0.6786, F1: 0.8022 | Val Loss: 7.8477, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 010 | Train Loss: 3.2632, Acc: 0.4554, F1: 0.5612 | Val Loss: 3.5139, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 011 | Train Loss: 1.8380, Acc: 0.5357, F1: 0.6623 | Val Loss: 0.5615, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.7338, Acc: 0.5000, F1: 0.5410 | Val Loss: 0.5689, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6118, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5609, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.5705, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5520, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 015 | Train Loss: 0.5420, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.5282, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 016 | Train Loss: 0.5461, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5104, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 017 | Train Loss: 0.5512, Acc: 0.7500, F1: 0.8391 | Val Loss: 0.6038, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 018 | Train Loss: 0.5058, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5216, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.4770, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5133, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 020 | Train Loss: 0.4794, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.7493, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 021 | Train Loss: 0.5157, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.4974, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 022 | Train Loss: 0.5028, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.5095, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 023 | Train Loss: 0.4579, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5229, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 024 | Train Loss: 0.4830, Acc: 0.7857, F1: 0.8261 | Val Loss: 0.6081, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.5821, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.7935, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 026 | Train Loss: 0.6967, Acc: 0.6696, F1: 0.7643 | Val Loss: 0.5106, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.4622, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5028, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 028 | Train Loss: 0.4493, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5300, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 029 | Train Loss: 0.4553, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5241, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4469, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5122, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 031 | Train Loss: 0.4398, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5134, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 032 | Train Loss: 0.4400, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5019, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 033 | Train Loss: 0.4674, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.5229, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 034 | Train Loss: 0.4328, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.5240, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 035 | Train Loss: 0.4519, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5157, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 036 | Train Loss: 0.4469, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.5380, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 037 | Train Loss: 0.4349, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4943, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 038 | Train Loss: 0.4234, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5369, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 039 | Train Loss: 0.4623, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.5477, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 040 | Train Loss: 0.4843, Acc: 0.7500, F1: 0.8228 | Val Loss: 0.5387, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 041 | Train Loss: 0.4336, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5153, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 042 | Train Loss: 0.4338, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4996, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 043 | Train Loss: 0.4396, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5340, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 044 | Train Loss: 0.4265, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5406, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 045 | Train Loss: 0.4207, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5104, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 046 | Train Loss: 0.4287, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.5166, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 047 | Train Loss: 0.4182, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.5325, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 048 | Train Loss: 0.4211, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5208, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 049 | Train Loss: 0.4170, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5154, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 050 | Train Loss: 0.4155, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5081, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 051 | Train Loss: 0.5162, Acc: 0.6786, F1: 0.7273 | Val Loss: 0.6961, Acc: 0.6316, F1: 0.7667\n",
      "Epoch 052 | Train Loss: 0.6367, Acc: 0.7232, F1: 0.8121 | Val Loss: 0.5959, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 053 | Train Loss: 0.4262, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.5166, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 054 | Train Loss: 0.4429, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5119, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 055 | Train Loss: 0.4333, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4933, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 056 | Train Loss: 0.4212, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4939, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 057 | Train Loss: 0.4173, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5018, Acc: 0.7368, F1: 0.8077\n",
      "\n",
      "Early stopping triggered after 57 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.01, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6601, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.5974, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6280, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5942, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6139, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6242, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6152, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5818, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5992, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5767, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5859, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5679, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5875, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5718, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5645, Acc: 0.6786, F1: 0.8022 | Val Loss: 0.5453, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5398, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5310, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 010 | Train Loss: 0.5222, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5189, Acc: 0.6579, F1: 0.7636\n",
      "Epoch 011 | Train Loss: 0.5015, Acc: 0.7143, F1: 0.8072 | Val Loss: 0.5427, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 012 | Train Loss: 0.5906, Acc: 0.6964, F1: 0.7463 | Val Loss: 0.5404, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 013 | Train Loss: 0.5819, Acc: 0.6964, F1: 0.8046 | Val Loss: 0.5352, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 014 | Train Loss: 0.4958, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5648, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 015 | Train Loss: 0.5097, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.5121, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 016 | Train Loss: 0.4866, Acc: 0.7321, F1: 0.8148 | Val Loss: 0.5185, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 017 | Train Loss: 0.4866, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5051, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 018 | Train Loss: 0.4637, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5087, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4535, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5052, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 020 | Train Loss: 0.4730, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.5131, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4870, Acc: 0.7679, F1: 0.8088 | Val Loss: 0.5268, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 022 | Train Loss: 0.4637, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5241, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 023 | Train Loss: 0.4686, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5113, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 024 | Train Loss: 0.4483, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4948, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4626, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4949, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 026 | Train Loss: 0.4428, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4902, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.4449, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4837, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4491, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4899, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 029 | Train Loss: 0.4507, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4879, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4193, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4937, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 031 | Train Loss: 0.4494, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4806, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.4204, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.5072, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 033 | Train Loss: 0.4389, Acc: 0.7768, F1: 0.8201 | Val Loss: 0.4812, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 034 | Train Loss: 0.4363, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4803, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 035 | Train Loss: 0.4150, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4911, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 036 | Train Loss: 0.4213, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4691, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 037 | Train Loss: 0.4161, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4728, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.4108, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4587, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 039 | Train Loss: 0.4144, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4585, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 040 | Train Loss: 0.4210, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4494, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 041 | Train Loss: 0.4039, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4573, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 042 | Train Loss: 0.3892, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4436, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 043 | Train Loss: 0.3965, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4424, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 044 | Train Loss: 0.3943, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4479, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 045 | Train Loss: 0.3849, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4389, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 046 | Train Loss: 0.4099, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4904, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 047 | Train Loss: 0.4625, Acc: 0.8036, F1: 0.8333 | Val Loss: 0.4930, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 048 | Train Loss: 0.4977, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4552, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 049 | Train Loss: 0.5600, Acc: 0.7143, F1: 0.7419 | Val Loss: 0.4285, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 050 | Train Loss: 0.5059, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5580, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 051 | Train Loss: 0.4871, Acc: 0.7768, F1: 0.8521 | Val Loss: 0.4550, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 052 | Train Loss: 0.4254, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4767, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 053 | Train Loss: 0.4015, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4545, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 054 | Train Loss: 0.4168, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4670, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 055 | Train Loss: 0.4018, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.5002, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 056 | Train Loss: 0.4035, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4596, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 057 | Train Loss: 0.3918, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4535, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 058 | Train Loss: 0.3910, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4676, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 059 | Train Loss: 0.3781, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4456, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 060 | Train Loss: 0.3953, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4472, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 061 | Train Loss: 0.3736, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4595, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 062 | Train Loss: 0.3685, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4339, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 063 | Train Loss: 0.3711, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4304, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 064 | Train Loss: 0.3617, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4403, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 065 | Train Loss: 0.3613, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4413, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 066 | Train Loss: 0.3559, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4385, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 067 | Train Loss: 0.3655, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4379, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 068 | Train Loss: 0.3635, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4243, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 069 | Train Loss: 0.3745, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4373, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 070 | Train Loss: 0.3557, Acc: 0.8482, F1: 0.8811 | Val Loss: 0.4199, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 071 | Train Loss: 0.3451, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.4400, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 072 | Train Loss: 0.3565, Acc: 0.8661, F1: 0.8936 | Val Loss: 0.4281, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 073 | Train Loss: 0.3359, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.4402, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 074 | Train Loss: 0.3426, Acc: 0.8482, F1: 0.8811 | Val Loss: 0.4265, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 075 | Train Loss: 0.3325, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4623, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 076 | Train Loss: 0.3249, Acc: 0.8661, F1: 0.8936 | Val Loss: 0.4401, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 077 | Train Loss: 0.3401, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.5014, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 078 | Train Loss: 0.3563, Acc: 0.8482, F1: 0.8759 | Val Loss: 0.4446, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 079 | Train Loss: 0.3503, Acc: 0.8304, F1: 0.8742 | Val Loss: 0.5413, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 080 | Train Loss: 0.3636, Acc: 0.8393, F1: 0.8732 | Val Loss: 0.4326, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 081 | Train Loss: 0.3478, Acc: 0.8482, F1: 0.8874 | Val Loss: 0.5695, Acc: 0.8158, F1: 0.8444\n",
      "Epoch 082 | Train Loss: 0.3672, Acc: 0.8393, F1: 0.8714 | Val Loss: 0.4351, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 083 | Train Loss: 0.3274, Acc: 0.8482, F1: 0.8859 | Val Loss: 0.4362, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 084 | Train Loss: 0.3200, Acc: 0.8482, F1: 0.8844 | Val Loss: 0.4133, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 085 | Train Loss: 0.3280, Acc: 0.8393, F1: 0.8800 | Val Loss: 0.4594, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 086 | Train Loss: 0.3328, Acc: 0.8750, F1: 0.9014 | Val Loss: 0.4413, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 087 | Train Loss: 0.3179, Acc: 0.8482, F1: 0.8859 | Val Loss: 0.4368, Acc: 0.7895, F1: 0.8400\n",
      "\n",
      "Early stopping triggered after 87 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.01, lr=0.01\n",
      "Epoch 001 | Train Loss: 1.9507, Acc: 0.5000, F1: 0.6364 | Val Loss: 0.6522, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6434, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.8052, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.7194, Acc: 0.5357, F1: 0.5738 | Val Loss: 0.6479, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6189, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6697, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6459, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6550, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6614, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6284, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6371, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5906, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6082, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5763, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5886, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5580, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5589, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5632, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 011 | Train Loss: 0.5649, Acc: 0.7054, F1: 0.8047 | Val Loss: 0.5188, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 012 | Train Loss: 0.5623, Acc: 0.7054, F1: 0.8024 | Val Loss: 0.5495, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 013 | Train Loss: 0.5240, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5176, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 014 | Train Loss: 0.4825, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.5005, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 015 | Train Loss: 0.4920, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5628, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 016 | Train Loss: 0.5184, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.4803, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 017 | Train Loss: 0.4732, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4806, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4794, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.4814, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4580, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.5351, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 020 | Train Loss: 0.4927, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5632, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 021 | Train Loss: 0.4896, Acc: 0.7411, F1: 0.7972 | Val Loss: 0.4837, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 022 | Train Loss: 0.5021, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4665, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 023 | Train Loss: 0.4728, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.4716, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4596, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4724, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 025 | Train Loss: 0.4549, Acc: 0.7768, F1: 0.8408 | Val Loss: 0.4981, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 026 | Train Loss: 0.4536, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4759, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4277, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5865, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 028 | Train Loss: 0.5143, Acc: 0.7589, F1: 0.8000 | Val Loss: 0.4891, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 029 | Train Loss: 0.4614, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4664, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 030 | Train Loss: 0.4297, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4654, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 031 | Train Loss: 0.4212, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4888, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 032 | Train Loss: 0.4194, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4965, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 033 | Train Loss: 0.4297, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4948, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 034 | Train Loss: 0.4030, Acc: 0.8482, F1: 0.8811 | Val Loss: 0.5398, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 035 | Train Loss: 0.6072, Acc: 0.7411, F1: 0.7914 | Val Loss: 0.4384, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 036 | Train Loss: 0.4938, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.4510, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 037 | Train Loss: 0.4627, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4741, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 038 | Train Loss: 0.4298, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4819, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 039 | Train Loss: 0.4443, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4762, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 040 | Train Loss: 0.3996, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4903, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 041 | Train Loss: 0.4349, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4739, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 042 | Train Loss: 0.4045, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4712, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 043 | Train Loss: 0.4014, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4517, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 044 | Train Loss: 0.4163, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4803, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 045 | Train Loss: 0.4233, Acc: 0.8214, F1: 0.8592 | Val Loss: 0.4349, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 046 | Train Loss: 0.3880, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4655, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 047 | Train Loss: 0.3980, Acc: 0.8036, F1: 0.8429 | Val Loss: 0.4429, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 048 | Train Loss: 0.4012, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.5139, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 049 | Train Loss: 0.3982, Acc: 0.8125, F1: 0.8444 | Val Loss: 0.4391, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 050 | Train Loss: 0.3975, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4955, Acc: 0.7105, F1: 0.7442\n",
      "\n",
      "Early stopping triggered after 50 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.01, lr=0.05\n",
      "Epoch 001 | Train Loss: 184.6970, Acc: 0.6607, F1: 0.7532 | Val Loss: 57.7516, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 20.7368, Acc: 0.5357, F1: 0.5738 | Val Loss: 0.6157, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6679, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6129, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6586, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.9776, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6594, Acc: 0.6161, F1: 0.7226 | Val Loss: 0.5673, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5475, Acc: 0.7054, F1: 0.8136 | Val Loss: 1.0672, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.9170, Acc: 0.6161, F1: 0.6993 | Val Loss: 0.5774, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.5900, Acc: 0.6071, F1: 0.6901 | Val Loss: 0.5346, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.6403, Acc: 0.6786, F1: 0.8043 | Val Loss: 0.6259, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 010 | Train Loss: 0.6386, Acc: 0.7232, F1: 0.7947 | Val Loss: 0.6224, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5797, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5051, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 012 | Train Loss: 0.4920, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.6152, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 013 | Train Loss: 0.5474, Acc: 0.7321, F1: 0.8000 | Val Loss: 0.5079, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4725, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5124, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 015 | Train Loss: 0.4865, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4926, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 016 | Train Loss: 0.6205, Acc: 0.6071, F1: 0.6765 | Val Loss: 0.6144, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 017 | Train Loss: 0.6273, Acc: 0.7143, F1: 0.8161 | Val Loss: 0.5162, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4857, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5002, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 019 | Train Loss: 0.4709, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.4885, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4488, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.5205, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 021 | Train Loss: 0.4521, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5156, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 022 | Train Loss: 0.4673, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4866, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 023 | Train Loss: 0.4285, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4828, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 024 | Train Loss: 0.4148, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.6668, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 025 | Train Loss: 0.5047, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.4648, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 026 | Train Loss: 0.4340, Acc: 0.7946, F1: 0.8369 | Val Loss: 0.5203, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 027 | Train Loss: 0.4530, Acc: 0.7857, F1: 0.8481 | Val Loss: 0.5964, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 028 | Train Loss: 0.4908, Acc: 0.8036, F1: 0.8358 | Val Loss: 0.4793, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 029 | Train Loss: 0.4405, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.6033, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 030 | Train Loss: 0.4728, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4854, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4150, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.5111, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4542, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.5064, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 033 | Train Loss: 0.4328, Acc: 0.7768, F1: 0.8201 | Val Loss: 0.4898, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 034 | Train Loss: 0.4220, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4948, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 035 | Train Loss: 0.4207, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.5059, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.4318, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5280, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 037 | Train Loss: 0.4371, Acc: 0.7857, F1: 0.8286 | Val Loss: 0.4902, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 038 | Train Loss: 0.4399, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5232, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 039 | Train Loss: 0.4646, Acc: 0.7768, F1: 0.8227 | Val Loss: 0.5274, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 040 | Train Loss: 0.4569, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4938, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 041 | Train Loss: 0.4153, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5288, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 042 | Train Loss: 0.5253, Acc: 0.8214, F1: 0.8592 | Val Loss: 0.4953, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 043 | Train Loss: 0.4335, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4949, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 044 | Train Loss: 0.4318, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4765, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 045 | Train Loss: 0.4606, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5368, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 046 | Train Loss: 0.4161, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4533, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 047 | Train Loss: 0.4392, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.4778, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 048 | Train Loss: 0.4229, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4769, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 049 | Train Loss: 0.3851, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4730, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 050 | Train Loss: 0.3831, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.4796, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 051 | Train Loss: 0.3741, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5082, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 052 | Train Loss: 0.4057, Acc: 0.8125, F1: 0.8489 | Val Loss: 0.4488, Acc: 0.8158, F1: 0.8679\n",
      "Epoch 053 | Train Loss: 0.4032, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.4914, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 054 | Train Loss: 0.4410, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4541, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 055 | Train Loss: 0.4681, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.7403, Acc: 0.5526, F1: 0.5143\n",
      "Epoch 056 | Train Loss: 0.5681, Acc: 0.6607, F1: 0.6607 | Val Loss: 0.7337, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 057 | Train Loss: 0.5944, Acc: 0.7946, F1: 0.8589 | Val Loss: 0.5741, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 058 | Train Loss: 0.5374, Acc: 0.7857, F1: 0.8182 | Val Loss: 0.4618, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 059 | Train Loss: 0.4849, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5070, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 060 | Train Loss: 0.4384, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.5109, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 061 | Train Loss: 0.4385, Acc: 0.8036, F1: 0.8608 | Val Loss: 0.4849, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 062 | Train Loss: 0.4146, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.5400, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 063 | Train Loss: 0.4442, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5072, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 064 | Train Loss: 0.4942, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4604, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 065 | Train Loss: 0.4442, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4943, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 066 | Train Loss: 0.4313, Acc: 0.7857, F1: 0.8310 | Val Loss: 0.4911, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 067 | Train Loss: 0.4254, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4664, Acc: 0.8158, F1: 0.8679\n",
      "Epoch 068 | Train Loss: 0.4112, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5216, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 069 | Train Loss: 0.3867, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.5105, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 070 | Train Loss: 0.4372, Acc: 0.8125, F1: 0.8662 | Val Loss: 0.5378, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 071 | Train Loss: 0.4298, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4936, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 072 | Train Loss: 0.3853, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4583, Acc: 0.8158, F1: 0.8511\n",
      "\n",
      "Early stopping triggered after 72 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.05, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6640, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.5932, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6305, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5862, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6102, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5923, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5958, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5688, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5874, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5599, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5690, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5500, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5475, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5378, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5316, Acc: 0.6964, F1: 0.8111 | Val Loss: 0.5314, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 009 | Train Loss: 0.5158, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5678, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5433, Acc: 0.6964, F1: 0.8111 | Val Loss: 0.5179, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 011 | Train Loss: 0.5362, Acc: 0.7411, F1: 0.7943 | Val Loss: 0.5088, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 012 | Train Loss: 0.5696, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.5276, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 013 | Train Loss: 0.4690, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.5777, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 014 | Train Loss: 0.5547, Acc: 0.7500, F1: 0.7879 | Val Loss: 0.5304, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 015 | Train Loss: 0.4669, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5348, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.5165, Acc: 0.7232, F1: 0.8144 | Val Loss: 0.5157, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 017 | Train Loss: 0.4744, Acc: 0.7411, F1: 0.8105 | Val Loss: 0.5476, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 018 | Train Loss: 0.4912, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.5116, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4577, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5195, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 020 | Train Loss: 0.4719, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5102, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4801, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.5276, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 022 | Train Loss: 0.4858, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5105, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 023 | Train Loss: 0.4609, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.5078, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 024 | Train Loss: 0.4500, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.4994, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4523, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5020, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 026 | Train Loss: 0.4584, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5019, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 027 | Train Loss: 0.4440, Acc: 0.7589, F1: 0.8138 | Val Loss: 0.4994, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4376, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4949, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 029 | Train Loss: 0.4383, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4867, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4276, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4833, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 031 | Train Loss: 0.4295, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4834, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4284, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4766, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 033 | Train Loss: 0.4305, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4833, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 034 | Train Loss: 0.4697, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4845, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 035 | Train Loss: 0.4693, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.4848, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 036 | Train Loss: 0.4334, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4975, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 037 | Train Loss: 0.4316, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4979, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.4264, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4829, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 039 | Train Loss: 0.4204, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4951, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 040 | Train Loss: 0.4281, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4854, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 041 | Train Loss: 0.4183, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4853, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 042 | Train Loss: 0.4252, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4744, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 043 | Train Loss: 0.4108, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4784, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 044 | Train Loss: 0.4086, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4699, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 045 | Train Loss: 0.4077, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4713, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 046 | Train Loss: 0.4077, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4879, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 047 | Train Loss: 0.4330, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.4705, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 048 | Train Loss: 0.4058, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4740, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 049 | Train Loss: 0.4145, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4552, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 050 | Train Loss: 0.3929, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4552, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 051 | Train Loss: 0.3983, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4573, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 052 | Train Loss: 0.3938, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4627, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 053 | Train Loss: 0.3931, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4631, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 054 | Train Loss: 0.3916, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4836, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 055 | Train Loss: 0.3831, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.4523, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 056 | Train Loss: 0.3811, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4263, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 057 | Train Loss: 0.3854, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4514, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 058 | Train Loss: 0.3696, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4385, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 059 | Train Loss: 0.3788, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4652, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 060 | Train Loss: 0.3901, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4516, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 061 | Train Loss: 0.3683, Acc: 0.8393, F1: 0.8732 | Val Loss: 0.4399, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 062 | Train Loss: 0.4075, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4464, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 063 | Train Loss: 0.3404, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4140, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 064 | Train Loss: 0.3664, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4098, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 065 | Train Loss: 0.3528, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4646, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 066 | Train Loss: 0.3420, Acc: 0.8393, F1: 0.8732 | Val Loss: 0.4610, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 067 | Train Loss: 0.3613, Acc: 0.8125, F1: 0.8511 | Val Loss: 0.4467, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 068 | Train Loss: 0.3459, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.4101, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 069 | Train Loss: 0.3352, Acc: 0.8482, F1: 0.8828 | Val Loss: 0.4988, Acc: 0.7895, F1: 0.8182\n",
      "Epoch 070 | Train Loss: 0.3563, Acc: 0.8661, F1: 0.8936 | Val Loss: 0.4179, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 071 | Train Loss: 0.3559, Acc: 0.8482, F1: 0.8859 | Val Loss: 0.3981, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 072 | Train Loss: 0.3377, Acc: 0.8393, F1: 0.8816 | Val Loss: 0.7440, Acc: 0.6053, F1: 0.5946\n",
      "Epoch 073 | Train Loss: 0.4665, Acc: 0.7589, F1: 0.7939 | Val Loss: 0.8088, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 074 | Train Loss: 0.6947, Acc: 0.8036, F1: 0.8659 | Val Loss: 0.4910, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 075 | Train Loss: 0.6124, Acc: 0.6071, F1: 0.5849 | Val Loss: 0.4234, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 076 | Train Loss: 0.3904, Acc: 0.7946, F1: 0.8623 | Val Loss: 0.5139, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 077 | Train Loss: 0.4621, Acc: 0.7768, F1: 0.8538 | Val Loss: 0.4451, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 078 | Train Loss: 0.3972, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.5012, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 079 | Train Loss: 0.3891, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4644, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 080 | Train Loss: 0.3947, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4657, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 081 | Train Loss: 0.3852, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.4650, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 082 | Train Loss: 0.3903, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4521, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 083 | Train Loss: 0.3833, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4451, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 084 | Train Loss: 0.3761, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.4473, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 085 | Train Loss: 0.3650, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4400, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 086 | Train Loss: 0.3559, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4395, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 087 | Train Loss: 0.3625, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4413, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 088 | Train Loss: 0.3501, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4410, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 089 | Train Loss: 0.3491, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4129, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 090 | Train Loss: 0.3493, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.4313, Acc: 0.8421, F1: 0.8750\n",
      "\n",
      "Early stopping triggered after 90 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.05, lr=0.01\n",
      "Epoch 001 | Train Loss: 1.3844, Acc: 0.5268, F1: 0.6074 | Val Loss: 0.8636, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6945, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6001, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6333, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5958, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6430, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5916, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6163, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6070, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6040, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5858, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6179, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5753, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5908, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5575, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5625, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5515, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5519, Acc: 0.6429, F1: 0.7778 | Val Loss: 0.5372, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 011 | Train Loss: 0.5485, Acc: 0.6786, F1: 0.7978 | Val Loss: 0.5022, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 012 | Train Loss: 0.4818, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.5002, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.6234, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4920, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.6306, Acc: 0.5893, F1: 0.5818 | Val Loss: 0.6499, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 015 | Train Loss: 0.5707, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5398, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.5037, Acc: 0.6964, F1: 0.8068 | Val Loss: 0.5505, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 017 | Train Loss: 0.4807, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5275, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 018 | Train Loss: 0.4842, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5383, Acc: 0.6842, F1: 0.7391\n",
      "Epoch 019 | Train Loss: 0.4494, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.5320, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 020 | Train Loss: 0.5054, Acc: 0.7411, F1: 0.8153 | Val Loss: 0.5136, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 021 | Train Loss: 0.4669, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4904, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 022 | Train Loss: 0.4635, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.4867, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4584, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5135, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4748, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4896, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 025 | Train Loss: 0.4486, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4961, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 026 | Train Loss: 0.4533, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4855, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 027 | Train Loss: 0.4369, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4886, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 028 | Train Loss: 0.4278, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4909, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 029 | Train Loss: 0.4257, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4808, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 030 | Train Loss: 0.4081, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4866, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 031 | Train Loss: 0.4451, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4866, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 032 | Train Loss: 0.4154, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5022, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 033 | Train Loss: 0.4263, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4615, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 034 | Train Loss: 0.4669, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4591, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 035 | Train Loss: 0.3986, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4809, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 036 | Train Loss: 0.4169, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.5055, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 037 | Train Loss: 0.4193, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4498, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 038 | Train Loss: 0.3912, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.5047, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 039 | Train Loss: 0.4024, Acc: 0.8214, F1: 0.8571 | Val Loss: 0.4410, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 040 | Train Loss: 0.4069, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4723, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 041 | Train Loss: 0.3965, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4428, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 042 | Train Loss: 0.3585, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4683, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 043 | Train Loss: 0.3683, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.4514, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 044 | Train Loss: 0.3476, Acc: 0.8482, F1: 0.8844 | Val Loss: 0.4807, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 045 | Train Loss: 0.3574, Acc: 0.8304, F1: 0.8671 | Val Loss: 0.4388, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 046 | Train Loss: 0.4697, Acc: 0.7679, F1: 0.8415 | Val Loss: 0.4990, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 047 | Train Loss: 0.3758, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.5231, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 048 | Train Loss: 0.4637, Acc: 0.7589, F1: 0.7907 | Val Loss: 0.7957, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 049 | Train Loss: 0.5942, Acc: 0.8036, F1: 0.8571 | Val Loss: 0.7610, Acc: 0.4474, F1: 0.3226\n",
      "Epoch 050 | Train Loss: 0.6038, Acc: 0.6161, F1: 0.5981 | Val Loss: 0.4201, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 051 | Train Loss: 0.4564, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.4305, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 052 | Train Loss: 0.4389, Acc: 0.8214, F1: 0.8592 | Val Loss: 0.5222, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 053 | Train Loss: 0.4339, Acc: 0.8214, F1: 0.8611 | Val Loss: 0.4288, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 054 | Train Loss: 0.3832, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4584, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 055 | Train Loss: 0.3726, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4701, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 056 | Train Loss: 0.3768, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4506, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 057 | Train Loss: 0.3549, Acc: 0.8482, F1: 0.8828 | Val Loss: 0.4364, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 058 | Train Loss: 0.3615, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.4264, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 059 | Train Loss: 0.3439, Acc: 0.8304, F1: 0.8671 | Val Loss: 0.4626, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 060 | Train Loss: 0.3553, Acc: 0.8214, F1: 0.8592 | Val Loss: 0.4264, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 061 | Train Loss: 0.3398, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4386, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 062 | Train Loss: 0.3330, Acc: 0.8571, F1: 0.8919 | Val Loss: 0.4639, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 063 | Train Loss: 0.3677, Acc: 0.8661, F1: 0.8905 | Val Loss: 0.3973, Acc: 0.7632, F1: 0.8235\n",
      "\n",
      "Early stopping triggered after 63 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.05, lr=0.05\n",
      "Epoch 001 | Train Loss: 184.1311, Acc: 0.5000, F1: 0.5410 | Val Loss: 44.9580, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 17.3711, Acc: 0.4821, F1: 0.5246 | Val Loss: 0.9371, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.7896, Acc: 0.6429, F1: 0.7802 | Val Loss: 0.8085, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 1.0810, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.8330, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 005 | Train Loss: 0.7370, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6519, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 1.6010, Acc: 0.6071, F1: 0.7442 | Val Loss: 0.6495, Acc: 0.7632, F1: 0.8421\n",
      "Epoch 007 | Train Loss: 0.6699, Acc: 0.6696, F1: 0.7886 | Val Loss: 0.6092, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 008 | Train Loss: 0.6960, Acc: 0.6071, F1: 0.7143 | Val Loss: 0.7067, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.8207, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6170, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.6627, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.5593, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.6599, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.6107, Acc: 0.7368, F1: 0.8333\n",
      "Epoch 012 | Train Loss: 0.6401, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.6645, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 013 | Train Loss: 0.6281, Acc: 0.7321, F1: 0.8235 | Val Loss: 0.6000, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 014 | Train Loss: 0.5966, Acc: 0.7232, F1: 0.8229 | Val Loss: 0.5723, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 015 | Train Loss: 0.5772, Acc: 0.7232, F1: 0.8187 | Val Loss: 0.6044, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 016 | Train Loss: 0.6828, Acc: 0.7232, F1: 0.8208 | Val Loss: 0.6635, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 017 | Train Loss: 0.5729, Acc: 0.7143, F1: 0.8072 | Val Loss: 0.6447, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 018 | Train Loss: 0.5675, Acc: 0.7679, F1: 0.8375 | Val Loss: 0.5875, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 019 | Train Loss: 0.5582, Acc: 0.7232, F1: 0.8208 | Val Loss: 0.5788, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 020 | Train Loss: 0.5348, Acc: 0.7411, F1: 0.8242 | Val Loss: 0.6122, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 021 | Train Loss: 0.5238, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5765, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 022 | Train Loss: 0.5390, Acc: 0.7679, F1: 0.8395 | Val Loss: 0.5778, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 023 | Train Loss: 0.5208, Acc: 0.7679, F1: 0.8471 | Val Loss: 0.6349, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 024 | Train Loss: 0.5384, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5838, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 025 | Train Loss: 0.5365, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.5942, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 026 | Train Loss: 0.5020, Acc: 0.7768, F1: 0.8447 | Val Loss: 0.5697, Acc: 0.7105, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 26 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.1, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6446, Acc: 0.5536, F1: 0.6753 | Val Loss: 0.6094, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6442, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6011, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6381, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6348, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6309, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5839, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5966, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5726, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5932, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5642, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5827, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5645, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5605, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5441, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5446, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5372, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 010 | Train Loss: 0.5372, Acc: 0.7321, F1: 0.8077 | Val Loss: 0.5186, Acc: 0.6579, F1: 0.7797\n",
      "Epoch 011 | Train Loss: 0.4940, Acc: 0.7321, F1: 0.8171 | Val Loss: 0.5172, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 012 | Train Loss: 0.4957, Acc: 0.7232, F1: 0.8000 | Val Loss: 0.5248, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 013 | Train Loss: 0.4774, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.5233, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 014 | Train Loss: 0.4811, Acc: 0.7411, F1: 0.8079 | Val Loss: 0.5223, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 015 | Train Loss: 0.4575, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5180, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4450, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.5155, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 017 | Train Loss: 0.4598, Acc: 0.7500, F1: 0.8133 | Val Loss: 0.5038, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 018 | Train Loss: 0.4372, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5171, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 019 | Train Loss: 0.4675, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5040, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 020 | Train Loss: 0.4446, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4915, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 021 | Train Loss: 0.4531, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4887, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 022 | Train Loss: 0.4358, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5046, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 023 | Train Loss: 0.4854, Acc: 0.7679, F1: 0.8333 | Val Loss: 0.4790, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 024 | Train Loss: 0.4420, Acc: 0.7946, F1: 0.8369 | Val Loss: 0.4799, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 025 | Train Loss: 0.4591, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.5074, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 026 | Train Loss: 0.4489, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5116, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 027 | Train Loss: 0.4274, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4966, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 028 | Train Loss: 0.4581, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4794, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 029 | Train Loss: 0.4313, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4804, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 030 | Train Loss: 0.4193, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4750, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 031 | Train Loss: 0.4228, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4745, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 032 | Train Loss: 0.4235, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4765, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 033 | Train Loss: 0.4389, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4734, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 034 | Train Loss: 0.4265, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4714, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 035 | Train Loss: 0.4247, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4732, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 036 | Train Loss: 0.4111, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4761, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 037 | Train Loss: 0.4252, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4743, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.4368, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4674, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 039 | Train Loss: 0.4018, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4580, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 040 | Train Loss: 0.3944, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4620, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 041 | Train Loss: 0.3999, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.5093, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 042 | Train Loss: 0.4209, Acc: 0.8214, F1: 0.8551 | Val Loss: 0.4800, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 043 | Train Loss: 0.4379, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4670, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 044 | Train Loss: 0.4279, Acc: 0.8125, F1: 0.8489 | Val Loss: 0.4563, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 045 | Train Loss: 0.4371, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4478, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 046 | Train Loss: 0.4077, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4529, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 047 | Train Loss: 0.4209, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4611, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 048 | Train Loss: 0.4029, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4877, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 049 | Train Loss: 0.3807, Acc: 0.8571, F1: 0.8873 | Val Loss: 0.4535, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 050 | Train Loss: 0.4102, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4472, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 051 | Train Loss: 0.3874, Acc: 0.8393, F1: 0.8750 | Val Loss: 0.4424, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 052 | Train Loss: 0.3738, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4424, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 053 | Train Loss: 0.3834, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4443, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 054 | Train Loss: 0.3864, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4373, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 055 | Train Loss: 0.3614, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.4709, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 056 | Train Loss: 0.3672, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4254, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 057 | Train Loss: 0.3614, Acc: 0.8393, F1: 0.8800 | Val Loss: 0.4468, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 058 | Train Loss: 0.3674, Acc: 0.8304, F1: 0.8742 | Val Loss: 0.4238, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 059 | Train Loss: 0.3562, Acc: 0.8214, F1: 0.8667 | Val Loss: 0.4667, Acc: 0.8158, F1: 0.8511\n",
      "Epoch 060 | Train Loss: 0.3693, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4588, Acc: 0.8421, F1: 0.8750\n",
      "Epoch 061 | Train Loss: 0.3614, Acc: 0.8482, F1: 0.8828 | Val Loss: 0.4324, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 062 | Train Loss: 0.3465, Acc: 0.8393, F1: 0.8784 | Val Loss: 0.4211, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 063 | Train Loss: 0.3516, Acc: 0.8661, F1: 0.8951 | Val Loss: 0.4264, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 064 | Train Loss: 0.3478, Acc: 0.8393, F1: 0.8800 | Val Loss: 0.4211, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 065 | Train Loss: 0.3325, Acc: 0.8393, F1: 0.8784 | Val Loss: 0.4250, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 066 | Train Loss: 0.3404, Acc: 0.8661, F1: 0.8951 | Val Loss: 0.4233, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 067 | Train Loss: 0.3462, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4217, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 068 | Train Loss: 0.3415, Acc: 0.8304, F1: 0.8774 | Val Loss: 0.4318, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 069 | Train Loss: 0.3464, Acc: 0.8482, F1: 0.8794 | Val Loss: 0.4870, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 070 | Train Loss: 0.4293, Acc: 0.8125, F1: 0.8627 | Val Loss: 0.6145, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 071 | Train Loss: 0.3462, Acc: 0.8750, F1: 0.8955 | Val Loss: 0.4926, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 072 | Train Loss: 0.4742, Acc: 0.7946, F1: 0.8571 | Val Loss: 0.4893, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 073 | Train Loss: 0.3744, Acc: 0.8304, F1: 0.8690 | Val Loss: 0.4011, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 074 | Train Loss: 0.3320, Acc: 0.8393, F1: 0.8816 | Val Loss: 0.4333, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 075 | Train Loss: 0.3256, Acc: 0.8839, F1: 0.9103 | Val Loss: 0.4236, Acc: 0.8158, F1: 0.8571\n",
      "Epoch 076 | Train Loss: 0.3257, Acc: 0.8482, F1: 0.8844 | Val Loss: 0.4261, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 077 | Train Loss: 0.3130, Acc: 0.8661, F1: 0.8980 | Val Loss: 0.4249, Acc: 0.8158, F1: 0.8571\n",
      "\n",
      "Early stopping triggered after 77 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.1, lr=0.01\n",
      "Epoch 001 | Train Loss: 2.3493, Acc: 0.5000, F1: 0.6000 | Val Loss: 0.6200, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6466, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6216, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6339, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5920, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6225, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5861, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6206, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5780, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5911, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5710, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5858, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5449, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5717, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6152, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 009 | Train Loss: 0.5740, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5355, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.5593, Acc: 0.6786, F1: 0.7882 | Val Loss: 0.5052, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 011 | Train Loss: 0.5004, Acc: 0.7321, F1: 0.8214 | Val Loss: 0.5040, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 012 | Train Loss: 0.4786, Acc: 0.7321, F1: 0.8125 | Val Loss: 0.5190, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 013 | Train Loss: 0.4831, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.4936, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.4680, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.5654, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 015 | Train Loss: 0.5138, Acc: 0.7589, F1: 0.8258 | Val Loss: 0.4881, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.4659, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.4875, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 017 | Train Loss: 0.4459, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4948, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 018 | Train Loss: 0.4715, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.4957, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 019 | Train Loss: 0.4456, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4809, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 020 | Train Loss: 0.4345, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4864, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 021 | Train Loss: 0.4372, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4680, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 022 | Train Loss: 0.4417, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4713, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 023 | Train Loss: 0.4263, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5351, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 024 | Train Loss: 0.4683, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5642, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 025 | Train Loss: 0.4998, Acc: 0.7321, F1: 0.7973 | Val Loss: 0.4617, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 026 | Train Loss: 0.4619, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4725, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 027 | Train Loss: 0.4639, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4657, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 028 | Train Loss: 0.4301, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4700, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 029 | Train Loss: 0.4044, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4741, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4247, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5907, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 031 | Train Loss: 0.4631, Acc: 0.7500, F1: 0.8056 | Val Loss: 0.4673, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 032 | Train Loss: 0.4654, Acc: 0.7768, F1: 0.8252 | Val Loss: 0.4620, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 033 | Train Loss: 0.4038, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5287, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 034 | Train Loss: 0.4314, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4447, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 035 | Train Loss: 0.4149, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4318, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 036 | Train Loss: 0.4020, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4360, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 037 | Train Loss: 0.4383, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.7688, Acc: 0.5000, F1: 0.4242\n",
      "Epoch 038 | Train Loss: 0.6500, Acc: 0.6071, F1: 0.5849 | Val Loss: 0.5218, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 039 | Train Loss: 0.4952, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.4468, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 040 | Train Loss: 0.4327, Acc: 0.7946, F1: 0.8414 | Val Loss: 0.4467, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 041 | Train Loss: 0.4237, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4452, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4282, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4353, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 043 | Train Loss: 0.4496, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4716, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 044 | Train Loss: 0.5548, Acc: 0.7589, F1: 0.7907 | Val Loss: 0.4782, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 045 | Train Loss: 0.4482, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4981, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 046 | Train Loss: 0.4395, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4741, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 047 | Train Loss: 0.4154, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4811, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 048 | Train Loss: 0.4304, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5011, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 049 | Train Loss: 0.4167, Acc: 0.8125, F1: 0.8467 | Val Loss: 0.4389, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 050 | Train Loss: 0.4374, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4337, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 051 | Train Loss: 0.4521, Acc: 0.8125, F1: 0.8421 | Val Loss: 0.4625, Acc: 0.7105, F1: 0.7442\n",
      "\n",
      "Early stopping triggered after 51 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.1, lr=0.05\n",
      "Epoch 001 | Train Loss: 114.6426, Acc: 0.5268, F1: 0.6131 | Val Loss: 15.5277, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 5.4515, Acc: 0.4821, F1: 0.4528 | Val Loss: 0.6143, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 2.2020, Acc: 0.5000, F1: 0.6456 | Val Loss: 0.6413, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6380, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6395, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6524, Acc: 0.6429, F1: 0.7802 | Val Loss: 0.6231, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6283, Acc: 0.6161, F1: 0.7571 | Val Loss: 0.6473, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 007 | Train Loss: 0.6875, Acc: 0.6607, F1: 0.7841 | Val Loss: 0.6065, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 008 | Train Loss: 0.6175, Acc: 0.6964, F1: 0.8090 | Val Loss: 0.5628, Acc: 0.7895, F1: 0.8621\n",
      "Epoch 009 | Train Loss: 0.5918, Acc: 0.6964, F1: 0.7733 | Val Loss: 1.6402, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.9601, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6055, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 011 | Train Loss: 0.5983, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5303, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 012 | Train Loss: 0.6426, Acc: 0.6250, F1: 0.7407 | Val Loss: 0.5428, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.5853, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5313, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.5378, Acc: 0.6786, F1: 0.7955 | Val Loss: 0.5102, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 015 | Train Loss: 0.5849, Acc: 0.6964, F1: 0.7821 | Val Loss: 0.5852, Acc: 0.7368, F1: 0.8387\n",
      "Epoch 016 | Train Loss: 0.5472, Acc: 0.7411, F1: 0.8304 | Val Loss: 0.5045, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 017 | Train Loss: 0.5161, Acc: 0.7321, F1: 0.8193 | Val Loss: 0.5009, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 018 | Train Loss: 0.4758, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5017, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 019 | Train Loss: 0.4702, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5068, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 020 | Train Loss: 0.4708, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5375, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 021 | Train Loss: 0.4631, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4879, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 022 | Train Loss: 0.4445, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.5011, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 023 | Train Loss: 0.4797, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.5809, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 024 | Train Loss: 0.4719, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.5751, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 025 | Train Loss: 0.5487, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5617, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 026 | Train Loss: 0.5173, Acc: 0.7679, F1: 0.8060 | Val Loss: 0.5065, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.5376, Acc: 0.7411, F1: 0.8129 | Val Loss: 0.5347, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 028 | Train Loss: 0.4671, Acc: 0.7589, F1: 0.8085 | Val Loss: 0.4989, Acc: 0.7368, F1: 0.8000\n",
      "\n",
      "Early stopping triggered after 28 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.25, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6270, Acc: 0.6071, F1: 0.7284 | Val Loss: 0.6156, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6383, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5939, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6138, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5819, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.5876, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5627, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.5713, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5472, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5548, Acc: 0.6696, F1: 0.7978 | Val Loss: 0.5322, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 007 | Train Loss: 0.5375, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5186, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 008 | Train Loss: 0.5095, Acc: 0.6964, F1: 0.8046 | Val Loss: 0.5496, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 009 | Train Loss: 0.5162, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5134, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 010 | Train Loss: 0.5216, Acc: 0.7143, F1: 0.8118 | Val Loss: 0.5125, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 011 | Train Loss: 0.5134, Acc: 0.7589, F1: 0.8029 | Val Loss: 0.5353, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 012 | Train Loss: 0.4763, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5349, Acc: 0.7105, F1: 0.8136\n",
      "Epoch 013 | Train Loss: 0.5181, Acc: 0.7143, F1: 0.8095 | Val Loss: 0.5053, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 014 | Train Loss: 0.4809, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5219, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 015 | Train Loss: 0.4786, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5052, Acc: 0.6842, F1: 0.7778\n",
      "Epoch 016 | Train Loss: 0.4752, Acc: 0.7411, F1: 0.8199 | Val Loss: 0.5017, Acc: 0.6842, F1: 0.7692\n",
      "Epoch 017 | Train Loss: 0.4528, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5210, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 018 | Train Loss: 0.4888, Acc: 0.7768, F1: 0.8201 | Val Loss: 0.5053, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 019 | Train Loss: 0.4577, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5272, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 020 | Train Loss: 0.4501, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5225, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 021 | Train Loss: 0.4736, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.5055, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 022 | Train Loss: 0.4486, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4940, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 023 | Train Loss: 0.4437, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4904, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4610, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4927, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 025 | Train Loss: 0.4737, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5029, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 026 | Train Loss: 0.4662, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5018, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 027 | Train Loss: 0.4511, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4836, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 028 | Train Loss: 0.4500, Acc: 0.7946, F1: 0.8535 | Val Loss: 0.4775, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 029 | Train Loss: 0.4612, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.4956, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 030 | Train Loss: 0.4760, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4879, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 031 | Train Loss: 0.4501, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.5009, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 032 | Train Loss: 0.4384, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4812, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 033 | Train Loss: 0.4451, Acc: 0.7589, F1: 0.8235 | Val Loss: 0.4862, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 034 | Train Loss: 0.4318, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4939, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 035 | Train Loss: 0.4217, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4857, Acc: 0.7368, F1: 0.8000\n",
      "Epoch 036 | Train Loss: 0.4139, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4851, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 037 | Train Loss: 0.4223, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4833, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 038 | Train Loss: 0.4250, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4822, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 039 | Train Loss: 0.4160, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4896, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 040 | Train Loss: 0.4473, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4783, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 041 | Train Loss: 0.4151, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.4782, Acc: 0.7368, F1: 0.7826\n",
      "\n",
      "Early stopping triggered after 41 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.25, lr=0.01\n",
      "Epoch 001 | Train Loss: 2.5059, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.6294, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.7212, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6250, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6329, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5943, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6431, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6191, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6361, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5817, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6241, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5847, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6026, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5644, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5975, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5502, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.5479, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5167, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 010 | Train Loss: 0.5285, Acc: 0.7500, F1: 0.8333 | Val Loss: 0.5050, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 011 | Train Loss: 0.4813, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.8065, Acc: 0.6842, F1: 0.7000\n",
      "Epoch 012 | Train Loss: 0.5709, Acc: 0.7143, F1: 0.7714 | Val Loss: 0.5105, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 013 | Train Loss: 0.4939, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4875, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 014 | Train Loss: 0.5159, Acc: 0.7857, F1: 0.8554 | Val Loss: 0.5573, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 015 | Train Loss: 0.5532, Acc: 0.8036, F1: 0.8406 | Val Loss: 0.4949, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 016 | Train Loss: 0.5036, Acc: 0.7411, F1: 0.8221 | Val Loss: 0.4887, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 017 | Train Loss: 0.5096, Acc: 0.7411, F1: 0.8027 | Val Loss: 0.4869, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4765, Acc: 0.7321, F1: 0.8026 | Val Loss: 0.4887, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 019 | Train Loss: 0.4669, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.4857, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 020 | Train Loss: 0.4746, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5381, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 021 | Train Loss: 0.5362, Acc: 0.7768, F1: 0.8120 | Val Loss: 0.5756, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 022 | Train Loss: 0.5920, Acc: 0.7321, F1: 0.8276 | Val Loss: 0.4804, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 023 | Train Loss: 0.4616, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4875, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 024 | Train Loss: 0.4880, Acc: 0.7589, F1: 0.8302 | Val Loss: 0.4819, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 025 | Train Loss: 0.4657, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.4795, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4692, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.4845, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 027 | Train Loss: 0.4515, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.4787, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4527, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4879, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 029 | Train Loss: 0.4555, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4609, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4145, Acc: 0.7679, F1: 0.8219 | Val Loss: 0.4574, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 031 | Train Loss: 0.4243, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4479, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 032 | Train Loss: 0.4413, Acc: 0.7500, F1: 0.8028 | Val Loss: 0.4765, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 033 | Train Loss: 0.3944, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4299, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 034 | Train Loss: 0.4012, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4258, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 035 | Train Loss: 0.3889, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.5439, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 036 | Train Loss: 0.4812, Acc: 0.7679, F1: 0.8194 | Val Loss: 0.8094, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 037 | Train Loss: 0.5521, Acc: 0.6875, F1: 0.7200 | Val Loss: 0.5443, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 038 | Train Loss: 0.4635, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5961, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 039 | Train Loss: 0.4885, Acc: 0.7857, F1: 0.8356 | Val Loss: 0.4801, Acc: 0.7895, F1: 0.8519\n",
      "Epoch 040 | Train Loss: 0.4416, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4755, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 041 | Train Loss: 0.4269, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4576, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 042 | Train Loss: 0.4600, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4738, Acc: 0.6842, F1: 0.7273\n",
      "Epoch 043 | Train Loss: 0.4145, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4311, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 044 | Train Loss: 0.3881, Acc: 0.8304, F1: 0.8707 | Val Loss: 0.4330, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 045 | Train Loss: 0.3948, Acc: 0.8036, F1: 0.8514 | Val Loss: 0.4299, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 046 | Train Loss: 0.3815, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4412, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 047 | Train Loss: 0.3934, Acc: 0.7946, F1: 0.8456 | Val Loss: 0.4270, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 048 | Train Loss: 0.3823, Acc: 0.8304, F1: 0.8671 | Val Loss: 0.4421, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 049 | Train Loss: 0.3963, Acc: 0.8393, F1: 0.8767 | Val Loss: 0.4224, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 050 | Train Loss: 0.3805, Acc: 0.8214, F1: 0.8649 | Val Loss: 0.4292, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 051 | Train Loss: 0.3896, Acc: 0.8036, F1: 0.8472 | Val Loss: 0.4109, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 052 | Train Loss: 0.3773, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4593, Acc: 0.7368, F1: 0.7727\n",
      "\n",
      "Early stopping triggered after 52 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.25, lr=0.05\n",
      "Epoch 001 | Train Loss: 136.3234, Acc: 0.6071, F1: 0.7143 | Val Loss: 9.7556, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 3.7137, Acc: 0.4196, F1: 0.4037 | Val Loss: 0.6341, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6462, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6140, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6917, Acc: 0.6071, F1: 0.7528 | Val Loss: 0.5532, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 1.1072, Acc: 0.6875, F1: 0.8066 | Val Loss: 1.1955, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 006 | Train Loss: 0.7986, Acc: 0.5357, F1: 0.6338 | Val Loss: 0.6594, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.6100, Acc: 0.7411, F1: 0.8343 | Val Loss: 0.6131, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 008 | Train Loss: 0.5513, Acc: 0.7411, F1: 0.8242 | Val Loss: 1.1781, Acc: 0.6579, F1: 0.6977\n",
      "Epoch 009 | Train Loss: 0.6852, Acc: 0.6607, F1: 0.7532 | Val Loss: 0.5737, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6104, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5248, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6566, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6601, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 012 | Train Loss: 0.6532, Acc: 0.6518, F1: 0.7719 | Val Loss: 0.6087, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 013 | Train Loss: 0.7107, Acc: 0.6786, F1: 0.7600 | Val Loss: 0.6721, Acc: 0.7105, F1: 0.8254\n",
      "Epoch 014 | Train Loss: 0.6603, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.6439, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 015 | Train Loss: 0.5981, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5923, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 016 | Train Loss: 0.6214, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5577, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 017 | Train Loss: 0.5947, Acc: 0.6964, F1: 0.8132 | Val Loss: 0.6138, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 018 | Train Loss: 0.5783, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5849, Acc: 0.7105, F1: 0.8197\n",
      "Epoch 019 | Train Loss: 0.5396, Acc: 0.6964, F1: 0.8090 | Val Loss: 0.5572, Acc: 0.6579, F1: 0.7547\n",
      "Epoch 020 | Train Loss: 0.4977, Acc: 0.7232, F1: 0.7974 | Val Loss: 0.7076, Acc: 0.6842, F1: 0.7857\n",
      "Epoch 021 | Train Loss: 0.8564, Acc: 0.6696, F1: 0.7483 | Val Loss: 0.5026, Acc: 0.7632, F1: 0.8475\n",
      "Epoch 022 | Train Loss: 0.5157, Acc: 0.7232, F1: 0.8229 | Val Loss: 0.5190, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 023 | Train Loss: 0.4876, Acc: 0.7321, F1: 0.8214 | Val Loss: 0.5397, Acc: 0.7368, F1: 0.8276\n",
      "Epoch 024 | Train Loss: 0.5323, Acc: 0.7232, F1: 0.8075 | Val Loss: 0.5263, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 025 | Train Loss: 0.4775, Acc: 0.7589, F1: 0.8383 | Val Loss: 0.5424, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 026 | Train Loss: 0.4747, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5498, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 027 | Train Loss: 0.4683, Acc: 0.8036, F1: 0.8590 | Val Loss: 0.5264, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 028 | Train Loss: 0.4776, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5316, Acc: 0.6842, F1: 0.7500\n",
      "Epoch 029 | Train Loss: 0.4495, Acc: 0.7857, F1: 0.8442 | Val Loss: 0.5032, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 030 | Train Loss: 0.5488, Acc: 0.7500, F1: 0.8313 | Val Loss: 0.5594, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 031 | Train Loss: 0.4777, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.5012, Acc: 0.7105, F1: 0.8000\n",
      "Epoch 032 | Train Loss: 0.5075, Acc: 0.7411, F1: 0.8242 | Val Loss: 0.5392, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 033 | Train Loss: 0.4662, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.5153, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 034 | Train Loss: 0.4839, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.4954, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 035 | Train Loss: 0.4425, Acc: 0.7946, F1: 0.8497 | Val Loss: 0.5300, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 036 | Train Loss: 0.5220, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.5459, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 037 | Train Loss: 0.4678, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4798, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 038 | Train Loss: 0.4757, Acc: 0.7857, F1: 0.8537 | Val Loss: 0.5184, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 039 | Train Loss: 0.4774, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.5068, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 040 | Train Loss: 0.4476, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.5407, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 041 | Train Loss: 0.4424, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.5498, Acc: 0.7368, F1: 0.7826\n",
      "\n",
      "Early stopping triggered after 41 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.5, lr=0.001\n",
      "Epoch 001 | Train Loss: 0.6272, Acc: 0.6607, F1: 0.7889 | Val Loss: 0.6065, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6271, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5971, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6061, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5902, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6069, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5757, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6044, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5656, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.5816, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5581, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5548, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.5390, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5605, Acc: 0.6875, F1: 0.8066 | Val Loss: 0.5260, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 009 | Train Loss: 0.5256, Acc: 0.7054, F1: 0.8047 | Val Loss: 0.5299, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 010 | Train Loss: 0.5118, Acc: 0.7500, F1: 0.8158 | Val Loss: 0.5321, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 011 | Train Loss: 0.5253, Acc: 0.6964, F1: 0.8023 | Val Loss: 0.5124, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 012 | Train Loss: 0.5009, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.5146, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 013 | Train Loss: 0.5597, Acc: 0.7054, F1: 0.7950 | Val Loss: 0.5119, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 014 | Train Loss: 0.5581, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.5321, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 015 | Train Loss: 0.5057, Acc: 0.7500, F1: 0.8293 | Val Loss: 0.5286, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 016 | Train Loss: 0.5102, Acc: 0.7500, F1: 0.8353 | Val Loss: 0.5099, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 017 | Train Loss: 0.5060, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.5206, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 018 | Train Loss: 0.4851, Acc: 0.7589, F1: 0.8212 | Val Loss: 0.5041, Acc: 0.6579, F1: 0.7451\n",
      "Epoch 019 | Train Loss: 0.4745, Acc: 0.7679, F1: 0.8354 | Val Loss: 0.5021, Acc: 0.7105, F1: 0.7925\n",
      "Epoch 020 | Train Loss: 0.4822, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.5029, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 021 | Train Loss: 0.4699, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5031, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 022 | Train Loss: 0.4633, Acc: 0.7679, F1: 0.8267 | Val Loss: 0.5233, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 023 | Train Loss: 0.4484, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.5046, Acc: 0.6842, F1: 0.7600\n",
      "Epoch 024 | Train Loss: 0.4710, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5011, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 025 | Train Loss: 0.4413, Acc: 0.7768, F1: 0.8276 | Val Loss: 0.4993, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 026 | Train Loss: 0.4526, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4996, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.4617, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4904, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 028 | Train Loss: 0.4607, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4852, Acc: 0.7368, F1: 0.8077\n",
      "Epoch 029 | Train Loss: 0.4360, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4795, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 030 | Train Loss: 0.4469, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4792, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 031 | Train Loss: 0.4324, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4804, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 032 | Train Loss: 0.4547, Acc: 0.7946, F1: 0.8435 | Val Loss: 0.4936, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 033 | Train Loss: 0.4608, Acc: 0.7857, F1: 0.8400 | Val Loss: 0.4755, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 034 | Train Loss: 0.4516, Acc: 0.7589, F1: 0.8280 | Val Loss: 0.4747, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 035 | Train Loss: 0.5296, Acc: 0.7589, F1: 0.8163 | Val Loss: 0.4665, Acc: 0.7895, F1: 0.8462\n",
      "Epoch 036 | Train Loss: 0.4807, Acc: 0.7946, F1: 0.8589 | Val Loss: 0.5122, Acc: 0.7368, F1: 0.8214\n",
      "Epoch 037 | Train Loss: 0.4707, Acc: 0.7857, F1: 0.8462 | Val Loss: 0.5052, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.4635, Acc: 0.7500, F1: 0.8082 | Val Loss: 0.4735, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 039 | Train Loss: 0.4368, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4759, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 040 | Train Loss: 0.4173, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4869, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 041 | Train Loss: 0.4346, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4871, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 042 | Train Loss: 0.4363, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4941, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 043 | Train Loss: 0.4272, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4875, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 044 | Train Loss: 0.4594, Acc: 0.7679, F1: 0.8169 | Val Loss: 0.4816, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 045 | Train Loss: 0.4666, Acc: 0.7679, F1: 0.8312 | Val Loss: 0.4799, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 046 | Train Loss: 0.4121, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.5286, Acc: 0.7632, F1: 0.8085\n",
      "Epoch 047 | Train Loss: 0.4407, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4676, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 048 | Train Loss: 0.4273, Acc: 0.7946, F1: 0.8516 | Val Loss: 0.4818, Acc: 0.7368, F1: 0.8148\n",
      "Epoch 049 | Train Loss: 0.4293, Acc: 0.7768, F1: 0.8366 | Val Loss: 0.4798, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 050 | Train Loss: 0.4132, Acc: 0.8036, F1: 0.8533 | Val Loss: 0.4983, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 051 | Train Loss: 0.3928, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4891, Acc: 0.7632, F1: 0.8302\n",
      "\n",
      "Early stopping triggered after 51 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.5, lr=0.01\n",
      "Epoch 001 | Train Loss: 1.9607, Acc: 0.6339, F1: 0.7657 | Val Loss: 0.6378, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 002 | Train Loss: 0.6656, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6480, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 003 | Train Loss: 0.6378, Acc: 0.6696, F1: 0.8000 | Val Loss: 0.6176, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6712, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5987, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.6249, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5714, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.6095, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5892, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 007 | Train Loss: 0.5974, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.5447, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.5808, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6172, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 009 | Train Loss: 0.6713, Acc: 0.6161, F1: 0.6325 | Val Loss: 0.6218, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 010 | Train Loss: 0.6027, Acc: 0.6607, F1: 0.7865 | Val Loss: 0.5396, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.5728, Acc: 0.7054, F1: 0.8177 | Val Loss: 0.5395, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 012 | Train Loss: 0.5244, Acc: 0.7589, F1: 0.8323 | Val Loss: 0.5450, Acc: 0.6842, F1: 0.8000\n",
      "Epoch 013 | Train Loss: 0.6527, Acc: 0.6250, F1: 0.7162 | Val Loss: 0.4892, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 014 | Train Loss: 0.5616, Acc: 0.7143, F1: 0.8182 | Val Loss: 0.4934, Acc: 0.6842, F1: 0.7931\n",
      "Epoch 015 | Train Loss: 0.5217, Acc: 0.7054, F1: 0.7898 | Val Loss: 0.5304, Acc: 0.7105, F1: 0.7755\n",
      "Epoch 016 | Train Loss: 0.5144, Acc: 0.7500, F1: 0.8250 | Val Loss: 0.4991, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 017 | Train Loss: 0.4652, Acc: 0.7768, F1: 0.8503 | Val Loss: 0.5099, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 018 | Train Loss: 0.5721, Acc: 0.7411, F1: 0.7883 | Val Loss: 0.4849, Acc: 0.7105, F1: 0.7843\n",
      "Epoch 019 | Train Loss: 0.4588, Acc: 0.7768, F1: 0.8428 | Val Loss: 0.5328, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 020 | Train Loss: 0.5630, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5324, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 021 | Train Loss: 0.4979, Acc: 0.7589, F1: 0.8188 | Val Loss: 0.4934, Acc: 0.7632, F1: 0.8364\n",
      "Epoch 022 | Train Loss: 0.4919, Acc: 0.7411, F1: 0.8176 | Val Loss: 0.5045, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 023 | Train Loss: 0.5092, Acc: 0.8036, F1: 0.8451 | Val Loss: 0.5221, Acc: 0.7105, F1: 0.7556\n",
      "Epoch 024 | Train Loss: 0.5068, Acc: 0.7143, F1: 0.7949 | Val Loss: 0.4994, Acc: 0.7105, F1: 0.8070\n",
      "Epoch 025 | Train Loss: 0.4788, Acc: 0.7857, F1: 0.8519 | Val Loss: 0.4964, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 026 | Train Loss: 0.4759, Acc: 0.7411, F1: 0.8054 | Val Loss: 0.4800, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 027 | Train Loss: 0.4418, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.4846, Acc: 0.7368, F1: 0.7917\n",
      "Epoch 028 | Train Loss: 0.4276, Acc: 0.8036, F1: 0.8493 | Val Loss: 0.4951, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 029 | Train Loss: 0.5274, Acc: 0.7857, F1: 0.8421 | Val Loss: 0.4808, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 030 | Train Loss: 0.4782, Acc: 0.7679, F1: 0.8243 | Val Loss: 0.4775, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 031 | Train Loss: 0.4463, Acc: 0.7857, F1: 0.8333 | Val Loss: 0.4623, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 032 | Train Loss: 0.4421, Acc: 0.7768, F1: 0.8344 | Val Loss: 0.4614, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 033 | Train Loss: 0.4349, Acc: 0.7679, F1: 0.8143 | Val Loss: 0.4841, Acc: 0.7632, F1: 0.8235\n",
      "Epoch 034 | Train Loss: 0.4725, Acc: 0.7500, F1: 0.8205 | Val Loss: 0.4725, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 035 | Train Loss: 0.4404, Acc: 0.8214, F1: 0.8571 | Val Loss: 0.4621, Acc: 0.7105, F1: 0.7660\n",
      "Epoch 036 | Train Loss: 0.4389, Acc: 0.8036, F1: 0.8553 | Val Loss: 0.4533, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 037 | Train Loss: 0.4090, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.4507, Acc: 0.7368, F1: 0.7826\n",
      "Epoch 038 | Train Loss: 0.4365, Acc: 0.7857, F1: 0.8378 | Val Loss: 0.4340, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 039 | Train Loss: 0.4437, Acc: 0.8125, F1: 0.8552 | Val Loss: 0.4324, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 040 | Train Loss: 0.3803, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4281, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 041 | Train Loss: 0.3716, Acc: 0.8214, F1: 0.8630 | Val Loss: 0.4468, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 042 | Train Loss: 0.4632, Acc: 0.7768, F1: 0.8299 | Val Loss: 0.4315, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 043 | Train Loss: 0.4161, Acc: 0.8304, F1: 0.8725 | Val Loss: 0.4554, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 044 | Train Loss: 0.4232, Acc: 0.7857, F1: 0.8261 | Val Loss: 0.4216, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 045 | Train Loss: 0.4584, Acc: 0.8125, F1: 0.8609 | Val Loss: 0.4755, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 046 | Train Loss: 0.4575, Acc: 0.7857, F1: 0.8235 | Val Loss: 0.4168, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 047 | Train Loss: 0.4853, Acc: 0.8125, F1: 0.8591 | Val Loss: 0.5315, Acc: 0.7368, F1: 0.7619\n",
      "Epoch 048 | Train Loss: 0.4493, Acc: 0.7857, F1: 0.8261 | Val Loss: 0.4274, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 049 | Train Loss: 0.4541, Acc: 0.7500, F1: 0.8182 | Val Loss: 0.5377, Acc: 0.7105, F1: 0.7442\n",
      "Epoch 050 | Train Loss: 0.4859, Acc: 0.8125, F1: 0.8372 | Val Loss: 0.4328, Acc: 0.7632, F1: 0.8163\n",
      "Epoch 051 | Train Loss: 0.4450, Acc: 0.7768, F1: 0.8387 | Val Loss: 0.4328, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 052 | Train Loss: 0.4295, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4755, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 053 | Train Loss: 0.4125, Acc: 0.8393, F1: 0.8732 | Val Loss: 0.4359, Acc: 0.7895, F1: 0.8400\n",
      "Epoch 054 | Train Loss: 0.3982, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4428, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 055 | Train Loss: 0.4541, Acc: 0.7946, F1: 0.8392 | Val Loss: 0.4768, Acc: 0.7632, F1: 0.8000\n",
      "Epoch 056 | Train Loss: 0.3978, Acc: 0.8125, F1: 0.8531 | Val Loss: 0.4328, Acc: 0.7632, F1: 0.8302\n",
      "Epoch 057 | Train Loss: 0.4257, Acc: 0.7946, F1: 0.8477 | Val Loss: 0.4490, Acc: 0.7895, F1: 0.8261\n",
      "Epoch 058 | Train Loss: 0.4428, Acc: 0.7946, F1: 0.8345 | Val Loss: 0.4313, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 059 | Train Loss: 0.3964, Acc: 0.7768, F1: 0.8322 | Val Loss: 0.4297, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 060 | Train Loss: 0.3863, Acc: 0.8125, F1: 0.8571 | Val Loss: 0.4728, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 061 | Train Loss: 0.4499, Acc: 0.7768, F1: 0.8120 | Val Loss: 0.4427, Acc: 0.7895, F1: 0.8333\n",
      "Epoch 062 | Train Loss: 0.4438, Acc: 0.7679, F1: 0.8289 | Val Loss: 0.4141, Acc: 0.8158, F1: 0.8627\n",
      "Epoch 063 | Train Loss: 0.3982, Acc: 0.8482, F1: 0.8844 | Val Loss: 0.4903, Acc: 0.7368, F1: 0.7727\n",
      "Epoch 064 | Train Loss: 0.4051, Acc: 0.7946, F1: 0.8345 | Val Loss: 0.4200, Acc: 0.8158, F1: 0.8627\n",
      "\n",
      "Early stopping triggered after 64 epochs.\n",
      "\n",
      ">>> Running gin hidden=256, dropout=0.5, lr=0.05\n",
      "Epoch 001 | Train Loss: 302.6585, Acc: 0.5179, F1: 0.6250 | Val Loss: 48.3633, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 002 | Train Loss: 20.9229, Acc: 0.5804, F1: 0.6667 | Val Loss: 1.2515, Acc: 0.3158, F1: 0.0000\n",
      "Epoch 003 | Train Loss: 0.9970, Acc: 0.5179, F1: 0.5781 | Val Loss: 0.5990, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 004 | Train Loss: 0.6466, Acc: 0.6339, F1: 0.7735 | Val Loss: 0.5840, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 005 | Train Loss: 0.7322, Acc: 0.5804, F1: 0.7219 | Val Loss: 0.5887, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 006 | Train Loss: 0.7303, Acc: 0.5804, F1: 0.7251 | Val Loss: 0.7067, Acc: 0.5263, F1: 0.6897\n",
      "Epoch 007 | Train Loss: 1.2007, Acc: 0.5089, F1: 0.6497 | Val Loss: 0.6427, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 008 | Train Loss: 0.6576, Acc: 0.6161, F1: 0.7543 | Val Loss: 0.5984, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 009 | Train Loss: 0.7269, Acc: 0.5714, F1: 0.7143 | Val Loss: 0.6183, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 010 | Train Loss: 0.6874, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.6357, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 011 | Train Loss: 0.6323, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6354, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 012 | Train Loss: 0.6549, Acc: 0.6518, F1: 0.7892 | Val Loss: 0.6369, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 013 | Train Loss: 0.6267, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6176, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 014 | Train Loss: 0.6968, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6399, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 015 | Train Loss: 0.6516, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6400, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 016 | Train Loss: 0.6461, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6369, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 017 | Train Loss: 0.6568, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6281, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 018 | Train Loss: 0.6444, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6304, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 019 | Train Loss: 0.6466, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6278, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 020 | Train Loss: 0.6338, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6264, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 021 | Train Loss: 0.6443, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6260, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 022 | Train Loss: 0.6483, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6252, Acc: 0.6842, F1: 0.8125\n",
      "Epoch 023 | Train Loss: 0.6333, Acc: 0.6607, F1: 0.7957 | Val Loss: 0.6200, Acc: 0.6842, F1: 0.8125\n",
      "\n",
      "Early stopping triggered after 23 epochs.\n",
      "\n",
      "=== GNN Ablation Results ===\n",
      "    model_type  hidden_channels  dropout     lr  seed  test_loss  test_acc  \\\n",
      "0          gcn               16     0.00  0.001     2   0.685639  0.710526   \n",
      "1          gcn               16     0.00  0.010     2   0.593560  0.657895   \n",
      "2          gcn               16     0.00  0.050     2   0.588974  0.684211   \n",
      "3          gcn               16     0.01  0.001     2   0.668247  0.657895   \n",
      "4          gcn               16     0.01  0.010     2   0.591419  0.684211   \n",
      "..         ...              ...      ...    ...   ...        ...       ...   \n",
      "265        gin              256     0.25  0.010     2   0.607568  0.789474   \n",
      "266        gin              256     0.25  0.050     2   0.570403  0.710526   \n",
      "267        gin              256     0.50  0.001     2   0.509299  0.789474   \n",
      "268        gin              256     0.50  0.010     2   0.502086  0.789474   \n",
      "269        gin              256     0.50  0.050     2   0.630050  0.657895   \n",
      "\n",
      "      test_f1  test_roc_auc  train_time_sec  \n",
      "0    0.813559      0.720000   -1.765287e+09  \n",
      "1    0.786885      0.707692   -1.765287e+09  \n",
      "2    0.800000      0.698462   -1.765287e+09  \n",
      "3    0.793651      0.636923   -1.765287e+09  \n",
      "4    0.806452      0.698462   -1.765287e+09  \n",
      "..        ...           ...             ...  \n",
      "265  0.846154      0.821538   -1.765287e+09  \n",
      "266  0.813559      0.787692   -1.765287e+09  \n",
      "267  0.846154      0.803077   -1.765287e+09  \n",
      "268  0.846154      0.843077   -1.765287e+09  \n",
      "269  0.793651      0.821538   -1.765287e+09  \n",
      "\n",
      "[270 rows x 10 columns]\n",
      "Saved GNN ablation results -> C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\results\\gnn_ablation_results.csv\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# A.2) Ablation over multiple GNN configurations\n",
    "# ============================================================\n",
    "\n",
    "def ablate_gnn_models(\n",
    "    model_types=(\"gcn\", \"sage\", \"gin\"),\n",
    "    hidden_list=(32, 64, 128, 256),\n",
    "    dropout_list=(0.0, 0.1, 0.25, 0.5),\n",
    "    lr_list=(0.001, 0.01, 0.05)\n",
    "):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "        Systematically explore different GNN configurations and\n",
    "        record their performance and runtime.\n",
    "\n",
    "    WHY:\n",
    "        This is the ablation study: we vary architecture knobs\n",
    "        (hidden size, dropout) and measure the effect on:\n",
    "          - validation accuracy (model selection)\n",
    "          - test accuracy / F1 (performance)\n",
    "          - train time (efficiency)\n",
    "\n",
    "    HOW:\n",
    "        For each combination of:\n",
    "            model_type ∈ model_types\n",
    "            hidden_channels ∈ hidden_list\n",
    "            dropout ∈ dropout_list\n",
    "        run a full train+test experiment and store the results.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for model_type in model_types:\n",
    "        for hidden in hidden_list:\n",
    "            for dropout in dropout_list:\n",
    "                for lr in lr_list:\n",
    "                    print(f\"\\n>>> Running {model_type} hidden={hidden}, dropout={dropout}, lr={lr}\")\n",
    "                    result, model, history = run_gnn_experiment(\n",
    "                        model_type=model_type,\n",
    "                        hidden_channels=hidden,\n",
    "                        dropout=dropout,\n",
    "                        lr=lr,\n",
    "                        # you can also override max_epochs, weight_decay, patience if desired\n",
    "                    )\n",
    "                    results.append(result)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Example usage (feel free to reduce if it trains too long):\n",
    "ablation_results = ablate_gnn_models(\n",
    "    model_types=(\"gcn\", \"sage\", \"gin\"),\n",
    "    hidden_list=(16, 32, 64, 128, 256),\n",
    "    dropout_list=(0.0, 0.01, 0.05, 0.1, 0.25, 0.5),\n",
    "    lr_list=(0.001, 0.01, 0.05)\n",
    ")\n",
    "\n",
    "print(\"\\n=== GNN Ablation Results ===\")\n",
    "print(ablation_results)\n",
    "ablation_results.to_csv(r\"C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\gnn_ablation_results.csv\")\n",
    "gnn_ablation_csv = RESULTS_DIR / \"gnn_ablation_results.csv\"\n",
    "ablation_results.to_csv(gnn_ablation_csv, index=False)\n",
    "print(f\"Saved GNN ablation results -> {gnn_ablation_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c8d8b004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# B.1) Helper: get class probabilities from a model for one graph\n",
    "# ============================================================\n",
    "\n",
    "def get_probs_for_graph(model, data, device):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "        Run a trained graph classifier on a single Data object and\n",
    "        return softmax probabilities.\n",
    "\n",
    "    WHY:\n",
    "        We need probabilities to compute fidelity metrics\n",
    "        for explanations (before/after masking).\n",
    "\n",
    "    HOW:\n",
    "        - Put model in eval mode\n",
    "        - Move 'data' to device\n",
    "        - Run forward pass\n",
    "        - Apply softmax to logits\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data = data.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(data)        # shape [1, num_classes] for one graph\n",
    "        probs = F.softmax(logits, dim=-1)  # convert logits -> probabilities\n",
    "\n",
    "    return probs.squeeze(0).cpu()   # shape [num_classes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6682b58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GCN GNNExplainer Results ===\n",
      "\n",
      "Explaining test graph index 0 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.6681492328643799, 'p_keep': 0.7614424824714661, 'p_comp': 0.674484372138977, 'sparsity': 0.6818181818181819, 'fidelity_plus': 1.1396293597571527, 'fidelity_minus': -0.00948162321078816, 'expl_time_sec': 0.544358491897583, 'num_edges': 44, 'num_important_edges': 14}\n",
      "\n",
      "Explaining test graph index 1 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7891699075698853, 'p_keep': 0.8418275713920593, 'p_comp': 0.8337270617485046, 'sparsity': 0.631578947368421, 'fidelity_plus': 1.066725382350582, 'fidelity_minus': -0.05646078715270031, 'expl_time_sec': 0.4954817295074463, 'num_edges': 38, 'num_important_edges': 14}\n",
      "\n",
      "Explaining test graph index 2 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7318605780601501, 'p_keep': 0.8197770714759827, 'p_comp': 0.7852122187614441, 'sparsity': 0.4545454545454546, 'fidelity_plus': 1.1201273795192817, 'fidelity_minus': -0.0728986398511946, 'expl_time_sec': 0.49319005012512207, 'num_edges': 22, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 3 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.519120454788208, 'p_keep': 0.6956319212913513, 'p_comp': 0.5213789939880371, 'sparsity': 0.7142857142857143, 'fidelity_plus': 1.3400202493950213, 'fidelity_minus': -0.004350703539028444, 'expl_time_sec': 0.5025773048400879, 'num_edges': 28, 'num_important_edges': 8}\n",
      "\n",
      "Explaining test graph index 4 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.5651737451553345, 'p_keep': 0.5390782356262207, 'p_comp': 0.7938321232795715, 'sparsity': 0.5454545454545454, 'fidelity_plus': 0.9538274561534319, 'fidelity_minus': -0.40458067998433256, 'expl_time_sec': 0.46973323822021484, 'num_edges': 22, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 5 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7604776620864868, 'p_keep': 0.8259636163711548, 'p_comp': 0.7814087867736816, 'sparsity': 0.8666666666666667, 'fidelity_plus': 1.0861116079399324, 'fidelity_minus': -0.027523654843151002, 'expl_time_sec': 0.49843645095825195, 'num_edges': 60, 'num_important_edges': 8}\n",
      "\n",
      "Explaining test graph index 6 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7155494093894958, 'p_keep': 0.8043681383132935, 'p_comp': 0.6936900615692139, 'sparsity': 0.6470588235294117, 'fidelity_plus': 1.1241266190123438, 'fidelity_minus': 0.03054904040649309, 'expl_time_sec': 0.4969477653503418, 'num_edges': 34, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 7 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.6988117694854736, 'p_keep': 0.8082571029663086, 'p_comp': 0.7423487901687622, 'sparsity': 0.8181818181818181, 'fidelity_plus': 1.1566163282587787, 'fidelity_minus': -0.06230149889339209, 'expl_time_sec': 0.48874378204345703, 'num_edges': 44, 'num_important_edges': 8}\n",
      "\n",
      "Explaining test graph index 8 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.5249577164649963, 'p_keep': 0.6363304257392883, 'p_comp': 0.6163126230239868, 'sparsity': 0.6875, 'fidelity_plus': 1.2121555808804236, 'fidelity_minus': -0.17402336167980104, 'expl_time_sec': 0.47631216049194336, 'num_edges': 48, 'num_important_edges': 15}\n",
      "\n",
      "Explaining test graph index 9 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7133660912513733, 'p_keep': 0.8275852799415588, 'p_comp': 0.7327222228050232, 'sparsity': 0.8636363636363636, 'fidelity_plus': 1.160113005217033, 'fidelity_minus': -0.02713351782630391, 'expl_time_sec': 0.49045658111572266, 'num_edges': 44, 'num_important_edges': 6}\n",
      "\n",
      "Explaining test graph index 10 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.6365073323249817, 'p_keep': 0.844634473323822, 'p_comp': 0.6311572194099426, 'sparsity': 0.6666666666666667, 'fidelity_plus': 1.3269831004752304, 'fidelity_minus': 0.008405422283976893, 'expl_time_sec': 0.4836912155151367, 'num_edges': 24, 'num_important_edges': 8}\n",
      "\n",
      "Explaining test graph index 11 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7180905938148499, 'p_keep': 0.7516661286354065, 'p_comp': 0.7601479291915894, 'sparsity': 0.6764705882352942, 'fidelity_plus': 1.0467566837802273, 'fidelity_minus': -0.05856828614522058, 'expl_time_sec': 0.5135252475738525, 'num_edges': 34, 'num_important_edges': 11}\n",
      "\n",
      "Explaining test graph index 12 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7269888520240784, 'p_keep': 0.8152084350585938, 'p_comp': 0.7517545223236084, 'sparsity': 0.6842105263157895, 'fidelity_plus': 1.1213492927558586, 'fidelity_minus': -0.034066093627952654, 'expl_time_sec': 0.4885690212249756, 'num_edges': 38, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 13 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.6437618136405945, 'p_keep': 0.7251920104026794, 'p_comp': 0.789409339427948, 'sparsity': 0.55, 'fidelity_plus': 1.126491188257318, 'fidelity_minus': -0.22624443187098864, 'expl_time_sec': 0.4359700679779053, 'num_edges': 20, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 14 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7626630067825317, 'p_keep': 0.8351753354072571, 'p_comp': 0.8116496205329895, 'sparsity': 0.631578947368421, 'fidelity_plus': 1.095077810225299, 'fidelity_minus': -0.0642310080793338, 'expl_time_sec': 0.49677205085754395, 'num_edges': 38, 'num_important_edges': 14}\n",
      "\n",
      "Explaining test graph index 15 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.6672361493110657, 'p_keep': 0.6737427711486816, 'p_comp': 0.7524407505989075, 'sparsity': 0.5588235294117647, 'fidelity_plus': 1.0097516027036817, 'fidelity_minus': -0.12769781939395397, 'expl_time_sec': 0.49761104583740234, 'num_edges': 34, 'num_important_edges': 15}\n",
      "\n",
      "Explaining test graph index 16 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7319067120552063, 'p_keep': 0.8346923589706421, 'p_comp': 0.7693090438842773, 'sparsity': 0.6, 'fidelity_plus': 1.1404354478821652, 'fidelity_minus': -0.05110259437851683, 'expl_time_sec': 0.48911118507385254, 'num_edges': 40, 'num_important_edges': 16}\n",
      "\n",
      "Explaining test graph index 17 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7350937724113464, 'p_keep': 0.775007426738739, 'p_comp': 0.7832162380218506, 'sparsity': 0.6799999999999999, 'fidelity_plus': 1.0542973642620626, 'fidelity_minus': -0.0654643902813199, 'expl_time_sec': 0.4916849136352539, 'num_edges': 50, 'num_important_edges': 16}\n",
      "\n",
      "Explaining test graph index 18 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.8002590537071228, 'p_keep': 0.8535372018814087, 'p_comp': 0.7930564880371094, 'sparsity': 0.5681818181818181, 'fidelity_plus': 1.0665761267273142, 'fidelity_minus': 0.009000292638550289, 'expl_time_sec': 0.4925816059112549, 'num_edges': 44, 'num_important_edges': 19}\n",
      "\n",
      "Explaining test graph index 19 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.8137006759643555, 'p_keep': 0.865790605545044, 'p_comp': 0.8274968862533569, 'sparsity': 0.7962962962962963, 'fidelity_plus': 1.064016082472777, 'fidelity_minus': -0.016954895942111525, 'expl_time_sec': 0.43079543113708496, 'num_edges': 54, 'num_important_edges': 11}\n",
      "\n",
      "Explaining test graph index 20 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.8001840114593506, 'p_keep': 0.852952778339386, 'p_comp': 0.8094629645347595, 'sparsity': 0.6590909090909092, 'fidelity_plus': 1.065945790123696, 'fidelity_minus': -0.011596024092616286, 'expl_time_sec': 0.46315765380859375, 'num_edges': 44, 'num_important_edges': 15}\n",
      "\n",
      "Explaining test graph index 21 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.788478434085846, 'p_keep': 0.8363648653030396, 'p_comp': 0.817897379398346, 'sparsity': 0.7222222222222222, 'fidelity_plus': 1.060732708907521, 'fidelity_minus': -0.037311033556178375, 'expl_time_sec': 0.43715667724609375, 'num_edges': 36, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 22 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7697973251342773, 'p_keep': 0.8379577994346619, 'p_comp': 0.780467689037323, 'sparsity': 0.803030303030303, 'fidelity_plus': 1.0885434023669738, 'fidelity_minus': -0.013861263938770296, 'expl_time_sec': 0.46248674392700195, 'num_edges': 66, 'num_important_edges': 13}\n",
      "\n",
      "Explaining test graph index 23 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7134205102920532, 'p_keep': 0.8286730051040649, 'p_comp': 0.7415194511413574, 'sparsity': 0.5555555555555556, 'fidelity_plus': 1.161549174924661, 'fidelity_minus': -0.03938622515604617, 'expl_time_sec': 0.45099520683288574, 'num_edges': 36, 'num_important_edges': 16}\n",
      "\n",
      "Explaining test graph index 24 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7990625500679016, 'p_keep': 0.8247568607330322, 'p_comp': 0.8474670052528381, 'sparsity': 0.5, 'fidelity_plus': 1.0321555686259445, 'fidelity_minus': -0.06057655333843792, 'expl_time_sec': 0.4789094924926758, 'num_edges': 44, 'num_important_edges': 22}\n",
      "\n",
      "Explaining test graph index 25 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.8137167692184448, 'p_keep': 0.8517686128616333, 'p_comp': 0.8172082304954529, 'sparsity': 0.7407407407407407, 'fidelity_plus': 1.0467630078211811, 'fidelity_minus': -0.004290757434385339, 'expl_time_sec': 0.44365954399108887, 'num_edges': 54, 'num_important_edges': 14}\n",
      "\n",
      "Explaining test graph index 26 (GCN)...\n",
      "{'pred_class': 0, 'p_full': 0.5756959319114685, 'p_keep': 0.4404003918170929, 'p_comp': 0.35142847895622253, 'sparsity': 0.5416666666666667, 'fidelity_plus': 0.7649878475862123, 'fidelity_minus': 0.38955886349694446, 'expl_time_sec': 0.45786523818969727, 'num_edges': 24, 'num_important_edges': 11}\n",
      "\n",
      "Explaining test graph index 27 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.5167961716651917, 'p_keep': 0.7399537563323975, 'p_comp': 0.4781547486782074, 'sparsity': 0.8181818181818181, 'fidelity_plus': 1.431809670625384, 'fidelity_minus': 0.07477110920244634, 'expl_time_sec': 0.4440181255340576, 'num_edges': 22, 'num_important_edges': 4}\n",
      "\n",
      "Explaining test graph index 28 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7777178883552551, 'p_keep': 0.8509591221809387, 'p_comp': 0.7884761691093445, 'sparsity': 0.6470588235294117, 'fidelity_plus': 1.0941745521381496, 'fidelity_minus': -0.013833140416560807, 'expl_time_sec': 0.4440779685974121, 'num_edges': 34, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 29 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.597287654876709, 'p_keep': 0.8052784204483032, 'p_comp': 0.6156222820281982, 'sparsity': 0.736842105263158, 'fidelity_plus': 1.3482254553118587, 'fidelity_minus': -0.030696477654931442, 'expl_time_sec': 0.45670461654663086, 'num_edges': 38, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 30 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.508020281791687, 'p_keep': 0.5230194926261902, 'p_comp': 0.6308292150497437, 'sparsity': 0.6538461538461539, 'fidelity_plus': 1.0295248268073156, 'fidelity_minus': -0.241740217191593, 'expl_time_sec': 0.4301877021789551, 'num_edges': 26, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 31 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7341558933258057, 'p_keep': 0.8197067379951477, 'p_comp': 0.7588711977005005, 'sparsity': 0.78, 'fidelity_plus': 1.1165295347310875, 'fidelity_minus': -0.03366492675381494, 'expl_time_sec': 0.4508492946624756, 'num_edges': 50, 'num_important_edges': 11}\n",
      "\n",
      "Explaining test graph index 32 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.5598541498184204, 'p_keep': 0.7171396613121033, 'p_comp': 0.5444570779800415, 'sparsity': 0.875, 'fidelity_plus': 1.2809401547611925, 'fidelity_minus': 0.02750193392935052, 'expl_time_sec': 0.45789217948913574, 'num_edges': 56, 'num_important_edges': 7}\n",
      "\n",
      "Explaining test graph index 33 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7683888077735901, 'p_keep': 0.8364522457122803, 'p_comp': 0.7609030604362488, 'sparsity': 0.7, 'fidelity_plus': 1.0885794239193882, 'fidelity_minus': 0.009742134791149937, 'expl_time_sec': 0.44417309761047363, 'num_edges': 30, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 34 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.7821987867355347, 'p_keep': 0.8061172962188721, 'p_comp': 0.8297958374023438, 'sparsity': 0.6176470588235294, 'fidelity_plus': 1.0305785561022922, 'fidelity_minus': -0.06085032535712931, 'expl_time_sec': 0.44977736473083496, 'num_edges': 34, 'num_important_edges': 13}\n",
      "\n",
      "Explaining test graph index 35 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.6295819282531738, 'p_keep': 0.8028354048728943, 'p_comp': 0.5999419093132019, 'sparsity': 0.5, 'fidelity_plus': 1.2751881349269765, 'fidelity_minus': 0.0470788909430907, 'expl_time_sec': 0.4491894245147705, 'num_edges': 28, 'num_important_edges': 14}\n",
      "\n",
      "Explaining test graph index 36 (GCN)...\n",
      "{'pred_class': 1, 'p_full': 0.6319640278816223, 'p_keep': 0.7355884313583374, 'p_comp': 0.6484322547912598, 'sparsity': 0.5357142857142857, 'fidelity_plus': 1.163971996672136, 'fidelity_minus': -0.026058804272198532, 'expl_time_sec': 0.45094919204711914, 'num_edges': 28, 'num_important_edges': 13}\n",
      "\n",
      "=== GraphSAGE GNNExplainer Results ===\n",
      "\n",
      "Explaining test graph index 0 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.650341272354126, 'p_keep': 0.5493952035903931, 'p_comp': 0.6034443378448486, 'sparsity': 0.75, 'fidelity_plus': 0.8447798516641499, 'fidelity_minus': 0.07211126911800991, 'expl_time_sec': 0.38846802711486816, 'num_edges': 44, 'num_important_edges': 11}\n",
      "\n",
      "Explaining test graph index 1 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.7901636958122253, 'p_keep': 0.6307173371315002, 'p_comp': 0.6651089191436768, 'sparsity': 0.631578947368421, 'fidelity_plus': 0.7982109789075706, 'fidelity_minus': 0.1582643917093688, 'expl_time_sec': 0.37479186058044434, 'num_edges': 38, 'num_important_edges': 14}\n",
      "\n",
      "Explaining test graph index 2 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.7232300639152527, 'p_keep': 0.5805680751800537, 'p_comp': 0.7269662022590637, 'sparsity': 0.5909090909090908, 'fidelity_plus': 0.8027432820437675, 'fidelity_minus': -0.005165905747315236, 'expl_time_sec': 0.3744378089904785, 'num_edges': 22, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 3 (GraphSAGE)...\n",
      "{'pred_class': 0, 'p_full': 0.5378313064575195, 'p_keep': 0.5354936122894287, 'p_comp': 0.5776751637458801, 'sparsity': 0.6071428571428572, 'fidelity_plus': 0.9956534806731719, 'fidelity_minus': -0.07408244334231151, 'expl_time_sec': 0.3676261901855469, 'num_edges': 28, 'num_important_edges': 11}\n",
      "\n",
      "Explaining test graph index 4 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.5435460209846497, 'p_keep': 0.44000330567359924, 'p_comp': 0.5178424715995789, 'sparsity': 0.4545454545454546, 'fidelity_plus': 0.8095051544605556, 'fidelity_minus': 0.04728863498716829, 'expl_time_sec': 0.38155126571655273, 'num_edges': 22, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 5 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.7564041018486023, 'p_keep': 0.6456741094589233, 'p_comp': 0.7477415800094604, 'sparsity': 0.7833333333333333, 'fidelity_plus': 0.8536100053938601, 'fidelity_minus': 0.011452240697758298, 'expl_time_sec': 0.35791659355163574, 'num_edges': 60, 'num_important_edges': 13}\n",
      "\n",
      "Explaining test graph index 6 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.7143514752388, 'p_keep': 0.645401656627655, 'p_comp': 0.5907594561576843, 'sparsity': 0.5588235294117647, 'fidelity_plus': 0.9034791401696262, 'fidelity_minus': 0.1730128982232454, 'expl_time_sec': 0.3609309196472168, 'num_edges': 34, 'num_important_edges': 15}\n",
      "\n",
      "Explaining test graph index 7 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.6866961717605591, 'p_keep': 0.5571268200874329, 'p_comp': 0.7118560075759888, 'sparsity': 0.7727272727272727, 'fidelity_plus': 0.8113148769405035, 'fidelity_minus': -0.036638963270939184, 'expl_time_sec': 0.3607923984527588, 'num_edges': 44, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 8 (GraphSAGE)...\n",
      "{'pred_class': 0, 'p_full': 0.5283772349357605, 'p_keep': 0.5300357937812805, 'p_comp': 0.5402522087097168, 'sparsity': 0.875, 'fidelity_plus': 1.0031389672678113, 'fidelity_minus': -0.022474423553467515, 'expl_time_sec': 0.35908985137939453, 'num_edges': 48, 'num_important_edges': 6}\n",
      "\n",
      "Explaining test graph index 9 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.6980006694793701, 'p_keep': 0.5382296442985535, 'p_comp': 0.6981145739555359, 'sparsity': 0.7954545454545454, 'fidelity_plus': 0.7711019026672455, 'fidelity_minus': -0.00016318677208548849, 'expl_time_sec': 0.3675525188446045, 'num_edges': 44, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 10 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.589313268661499, 'p_keep': 0.5398953557014465, 'p_comp': 0.5656896233558655, 'sparsity': 0.7083333333333333, 'fidelity_plus': 0.9161432202734975, 'fidelity_minus': 0.04008673580231725, 'expl_time_sec': 0.3708326816558838, 'num_edges': 24, 'num_important_edges': 7}\n",
      "\n",
      "Explaining test graph index 11 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.7165447473526001, 'p_keep': 0.6277362704277039, 'p_comp': 0.576465368270874, 'sparsity': 0.6176470588235294, 'fidelity_plus': 0.8760601103378195, 'fidelity_minus': 0.19549285595808752, 'expl_time_sec': 0.36951518058776855, 'num_edges': 34, 'num_important_edges': 13}\n",
      "\n",
      "Explaining test graph index 12 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.7271265387535095, 'p_keep': 0.5538555979728699, 'p_comp': 0.7268033027648926, 'sparsity': 0.8421052631578947, 'fidelity_plus': 0.7617045568469105, 'fidelity_minus': 0.0004445388407512407, 'expl_time_sec': 0.3537418842315674, 'num_edges': 38, 'num_important_edges': 6}\n",
      "\n",
      "Explaining test graph index 13 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.619084358215332, 'p_keep': 0.6678060293197632, 'p_comp': 0.4447839558124542, 'sparsity': 0.5, 'fidelity_plus': 1.0786995672849558, 'fidelity_minus': 0.2815454793678571, 'expl_time_sec': 0.36293458938598633, 'num_edges': 20, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 14 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.7578468918800354, 'p_keep': 0.6233320236206055, 'p_comp': 0.6972427368164062, 'sparsity': 0.6842105263157895, 'fidelity_plus': 0.8225038992695068, 'fidelity_minus': 0.07996886404493242, 'expl_time_sec': 0.36763739585876465, 'num_edges': 38, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 15 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.6442548632621765, 'p_keep': 0.47668930888175964, 'p_comp': 0.6274719834327698, 'sparsity': 0.8529411764705882, 'fidelity_plus': 0.7399079713082013, 'fidelity_minus': 0.026050063082840857, 'expl_time_sec': 0.35784268379211426, 'num_edges': 34, 'num_important_edges': 5}\n",
      "\n",
      "Explaining test graph index 16 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.719098687171936, 'p_keep': 0.5433144569396973, 'p_comp': 0.6585692763328552, 'sparsity': 0.75, 'fidelity_plus': 0.7555492265970319, 'fidelity_minus': 0.08417399714235363, 'expl_time_sec': 0.3975362777709961, 'num_edges': 40, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 17 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.726355791091919, 'p_keep': 0.5396190881729126, 'p_comp': 0.7291452884674072, 'sparsity': 0.8, 'fidelity_plus': 0.7429129013506066, 'fidelity_minus': -0.0038404008196792905, 'expl_time_sec': 0.3706364631652832, 'num_edges': 50, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 18 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.8030545115470886, 'p_keep': 0.6573201417922974, 'p_comp': 0.7846817970275879, 'sparsity': 0.7272727272727273, 'fidelity_plus': 0.8185249349087981, 'fidelity_minus': 0.022878539694778155, 'expl_time_sec': 0.35883402824401855, 'num_edges': 44, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 19 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.8187869191169739, 'p_keep': 0.5645792484283447, 'p_comp': 0.8143044114112854, 'sparsity': 0.8703703703703703, 'fidelity_plus': 0.6895313484455992, 'fidelity_minus': 0.005474571712164922, 'expl_time_sec': 0.4011716842651367, 'num_edges': 54, 'num_important_edges': 7}\n",
      "\n",
      "Explaining test graph index 20 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.8030545115470886, 'p_keep': 0.5886561274528503, 'p_comp': 0.7433686852455139, 'sparsity': 0.7727272727272727, 'fidelity_plus': 0.7330213814735955, 'fidelity_minus': 0.07432350537025145, 'expl_time_sec': 0.37053775787353516, 'num_edges': 44, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 21 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.7918400168418884, 'p_keep': 0.6276189684867859, 'p_comp': 0.7627377510070801, 'sparsity': 0.6388888888888888, 'fidelity_plus': 0.7926082985675962, 'fidelity_minus': 0.036752709153141216, 'expl_time_sec': 0.3587915897369385, 'num_edges': 36, 'num_important_edges': 13}\n",
      "\n",
      "Explaining test graph index 22 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.767578125, 'p_keep': 0.530448853969574, 'p_comp': 0.7598176002502441, 'sparsity': 0.8636363636363636, 'fidelity_plus': 0.6910682270545085, 'fidelity_minus': 0.01011040374522898, 'expl_time_sec': 0.3772470951080322, 'num_edges': 66, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 23 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.6986224055290222, 'p_keep': 0.5952444672584534, 'p_comp': 0.755121111869812, 'sparsity': 0.6666666666666667, 'fidelity_plus': 0.852026019417045, 'fidelity_minus': -0.0808715922845431, 'expl_time_sec': 0.3610520362854004, 'num_edges': 36, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 24 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.8016156554222107, 'p_keep': 0.6325672268867493, 'p_comp': 0.7260329723358154, 'sparsity': 0.7272727272727273, 'fidelity_plus': 0.7891153604698206, 'fidelity_minus': 0.09428793284555537, 'expl_time_sec': 0.3603799343109131, 'num_edges': 44, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 25 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.8187869191169739, 'p_keep': 0.582030177116394, 'p_comp': 0.8250009417533875, 'sparsity': 0.8148148148148149, 'fidelity_plus': 0.710844498766649, 'fidelity_minus': -0.007589303750864973, 'expl_time_sec': 0.37264013290405273, 'num_edges': 54, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 26 (GraphSAGE)...\n",
      "{'pred_class': 0, 'p_full': 0.6588218212127686, 'p_keep': 0.6601826548576355, 'p_comp': 0.5356576442718506, 'sparsity': 0.5, 'fidelity_plus': 1.0020655564236804, 'fidelity_minus': 0.18694611042815124, 'expl_time_sec': 0.37259507179260254, 'num_edges': 24, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 27 (GraphSAGE)...\n",
      "{'pred_class': 0, 'p_full': 0.5015071034431458, 'p_keep': 0.44600555300712585, 'p_comp': 0.4775271415710449, 'sparsity': 0.5909090909090908, 'fidelity_plus': 0.8893304799573752, 'fidelity_minus': 0.04781579703949168, 'expl_time_sec': 0.37016797065734863, 'num_edges': 22, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 28 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.776969313621521, 'p_keep': 0.5889449715614319, 'p_comp': 0.7178618311882019, 'sparsity': 0.5882352941176471, 'fidelity_plus': 0.7580028724896593, 'fidelity_minus': 0.07607441040086127, 'expl_time_sec': 0.39135146141052246, 'num_edges': 34, 'num_important_edges': 14}\n",
      "\n",
      "Explaining test graph index 29 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.5842775106430054, 'p_keep': 0.5276452302932739, 'p_comp': 0.6403001546859741, 'sparsity': 0.6578947368421053, 'fidelity_plus': 0.9030729759093297, 'fidelity_minus': -0.09588362211873447, 'expl_time_sec': 0.3627662658691406, 'num_edges': 38, 'num_important_edges': 13}\n",
      "\n",
      "Explaining test graph index 30 (GraphSAGE)...\n",
      "{'pred_class': 0, 'p_full': 0.5314269065856934, 'p_keep': 0.5022271871566772, 'p_comp': 0.5205032825469971, 'sparsity': 0.6153846153846154, 'fidelity_plus': 0.9450541192642687, 'fidelity_minus': 0.020555270919341062, 'expl_time_sec': 0.36668920516967773, 'num_edges': 26, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 31 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.7247956395149231, 'p_keep': 0.5298082828521729, 'p_comp': 0.7176693677902222, 'sparsity': 0.86, 'fidelity_plus': 0.7309760903180273, 'fidelity_minus': 0.009832111751486572, 'expl_time_sec': 0.3785412311553955, 'num_edges': 50, 'num_important_edges': 7}\n",
      "\n",
      "Explaining test graph index 32 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.518815279006958, 'p_keep': 0.4305582642555237, 'p_comp': 0.5192723870277405, 'sparsity': 0.8392857142857143, 'fidelity_plus': 0.8298874024674769, 'fidelity_minus': -0.0008810612163492326, 'expl_time_sec': 0.36345338821411133, 'num_edges': 56, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 33 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.767578125, 'p_keep': 0.5876317620277405, 'p_comp': 0.6888846755027771, 'sparsity': 0.7, 'fidelity_plus': 0.7655660614712547, 'fidelity_minus': 0.10252174590986796, 'expl_time_sec': 0.3500552177429199, 'num_edges': 30, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 34 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.783137857913971, 'p_keep': 0.6410850286483765, 'p_comp': 0.7669722437858582, 'sparsity': 0.6764705882352942, 'fidelity_plus': 0.8186106981930642, 'fidelity_minus': 0.02064210530081234, 'expl_time_sec': 0.3650522232055664, 'num_edges': 34, 'num_important_edges': 11}\n",
      "\n",
      "Explaining test graph index 35 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.6067212224006653, 'p_keep': 0.5941228866577148, 'p_comp': 0.7009173035621643, 'sparsity': 0.5714285714285714, 'fidelity_plus': 0.9792353798123271, 'fidelity_minus': -0.15525430409173002, 'expl_time_sec': 0.37693333625793457, 'num_edges': 28, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 36 (GraphSAGE)...\n",
      "{'pred_class': 1, 'p_full': 0.6098825335502625, 'p_keep': 0.4819004535675049, 'p_comp': 0.5598152279853821, 'sparsity': 0.8214285714285714, 'fidelity_plus': 0.7901529016780571, 'fidelity_minus': 0.08209335865617828, 'expl_time_sec': 0.38482022285461426, 'num_edges': 28, 'num_important_edges': 5}\n",
      "\n",
      "=== GIN GNNExplainer Results ===\n",
      "\n",
      "Explaining test graph index 0 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9026919603347778, 'p_keep': 0.38957443833351135, 'p_comp': 0.6192048788070679, 'sparsity': 0.7954545454545454, 'fidelity_plus': 0.43156963333209636, 'fidelity_minus': 0.3140463125677714, 'expl_time_sec': 0.37769150733947754, 'num_edges': 44, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 1 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9770978689193726, 'p_keep': 0.38625556230545044, 'p_comp': 0.8227369785308838, 'sparsity': 0.7631578947368421, 'fidelity_plus': 0.39530898039173107, 'fidelity_minus': 0.1579789448923936, 'expl_time_sec': 0.35575199127197266, 'num_edges': 38, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 2 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.7397313714027405, 'p_keep': 0.38935765624046326, 'p_comp': 0.5009313225746155, 'sparsity': 0.7272727272727273, 'fidelity_plus': 0.5263500661086344, 'fidelity_minus': 0.3228199560812087, 'expl_time_sec': 0.36508846282958984, 'num_edges': 22, 'num_important_edges': 6}\n",
      "\n",
      "Explaining test graph index 3 (GIN)...\n",
      "{'pred_class': 0, 'p_full': 0.5984244346618652, 'p_keep': 0.619936466217041, 'p_comp': 0.5933841466903687, 'sparsity': 0.5, 'fidelity_plus': 1.0359477827260362, 'fidelity_minus': 0.008422597206186211, 'expl_time_sec': 0.37412405014038086, 'num_edges': 28, 'num_important_edges': 14}\n",
      "\n",
      "Explaining test graph index 4 (GIN)...\n",
      "{'pred_class': 0, 'p_full': 0.6977838277816772, 'p_keep': 0.6810049414634705, 'p_comp': 0.6196019649505615, 'sparsity': 0.4545454545454546, 'fidelity_plus': 0.9759540338280002, 'fidelity_minus': 0.11204309947919466, 'expl_time_sec': 0.38848233222961426, 'num_edges': 22, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 5 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9847797751426697, 'p_keep': 0.382777601480484, 'p_comp': 0.8834902048110962, 'sparsity': 0.8666666666666667, 'fidelity_plus': 0.38869360555767835, 'fidelity_minus': 0.10285504727886918, 'expl_time_sec': 0.4235067367553711, 'num_edges': 60, 'num_important_edges': 8}\n",
      "\n",
      "Explaining test graph index 6 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.865451455116272, 'p_keep': 0.3737075626850128, 'p_comp': 0.38045692443847656, 'sparsity': 0.7058823529411764, 'fidelity_plus': 0.431806498765209, 'fidelity_minus': 0.5603948411094152, 'expl_time_sec': 0.4020843505859375, 'num_edges': 34, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 7 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9399629235267639, 'p_keep': 0.3714655935764313, 'p_comp': 0.4414202868938446, 'sparsity': 0.7954545454545454, 'fidelity_plus': 0.39519175094979625, 'fidelity_minus': 0.5303854270787354, 'expl_time_sec': 0.39398646354675293, 'num_edges': 44, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 8 (GIN)...\n",
      "{'pred_class': 0, 'p_full': 0.6129538416862488, 'p_keep': 0.6255345940589905, 'p_comp': 0.6105692386627197, 'sparsity': 0.8541666666666666, 'fidelity_plus': 1.0205247956977181, 'fidelity_minus': 0.0038903468113830053, 'expl_time_sec': 0.3815624713897705, 'num_edges': 48, 'num_important_edges': 7}\n",
      "\n",
      "Explaining test graph index 9 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9491440057754517, 'p_keep': 0.3912765681743622, 'p_comp': 0.7263308167457581, 'sparsity': 0.8181818181818181, 'fidelity_plus': 0.41224152056324564, 'fidelity_minus': 0.23475172120763166, 'expl_time_sec': 0.3816089630126953, 'num_edges': 44, 'num_important_edges': 8}\n",
      "\n",
      "Explaining test graph index 10 (GIN)...\n",
      "{'pred_class': 0, 'p_full': 0.5789844989776611, 'p_keep': 0.6056034564971924, 'p_comp': 0.6090720295906067, 'sparsity': 0.6666666666666667, 'fidelity_plus': 1.0459752507477031, 'fidelity_minus': -0.05196603823776358, 'expl_time_sec': 0.38103270530700684, 'num_edges': 24, 'num_important_edges': 8}\n",
      "\n",
      "Explaining test graph index 11 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.6912153959274292, 'p_keep': 0.3737638294696808, 'p_comp': 0.3804287314414978, 'sparsity': 0.6470588235294117, 'fidelity_plus': 0.5407342366386213, 'fidelity_minus': 0.44962346949598464, 'expl_time_sec': 0.37811851501464844, 'num_edges': 34, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 12 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9647322297096252, 'p_keep': 0.38251277804374695, 'p_comp': 0.49340909719467163, 'sparsity': 0.8157894736842105, 'fidelity_plus': 0.39649631914845374, 'fidelity_minus': 0.4885533187346889, 'expl_time_sec': 0.3945496082305908, 'num_edges': 38, 'num_important_edges': 7}\n",
      "\n",
      "Explaining test graph index 13 (GIN)...\n",
      "{'pred_class': 0, 'p_full': 0.6110138297080994, 'p_keep': 0.6324260830879211, 'p_comp': 0.6089034676551819, 'sparsity': 0.44999999999999996, 'fidelity_plus': 1.0350438113488383, 'fidelity_minus': 0.0034538695366775896, 'expl_time_sec': 0.3636767864227295, 'num_edges': 20, 'num_important_edges': 11}\n",
      "\n",
      "Explaining test graph index 14 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9754054546356201, 'p_keep': 0.3826960027217865, 'p_comp': 0.5440239906311035, 'sparsity': 0.8157894736842105, 'fidelity_plus': 0.39234556348134153, 'fidelity_minus': 0.4422586135379638, 'expl_time_sec': 0.39534950256347656, 'num_edges': 38, 'num_important_edges': 7}\n",
      "\n",
      "Explaining test graph index 15 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.5690597891807556, 'p_keep': 0.39543020725250244, 'p_comp': 0.49342623353004456, 'sparsity': 0.8235294117647058, 'fidelity_plus': 0.694883410795519, 'fidelity_minus': 0.13290968205572318, 'expl_time_sec': 0.3904995918273926, 'num_edges': 34, 'num_important_edges': 6}\n",
      "\n",
      "Explaining test graph index 16 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9639529585838318, 'p_keep': 0.39100298285484314, 'p_comp': 0.7844331860542297, 'sparsity': 0.85, 'fidelity_plus': 0.40562454772614187, 'fidelity_minus': 0.18623291824669452, 'expl_time_sec': 0.3795790672302246, 'num_edges': 40, 'num_important_edges': 6}\n",
      "\n",
      "Explaining test graph index 17 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9667311906814575, 'p_keep': 0.3912599980831146, 'p_comp': 0.8244042992591858, 'sparsity': 0.84, 'fidelity_plus': 0.4047247071932291, 'fidelity_minus': 0.1472248881531838, 'expl_time_sec': 0.39142727851867676, 'num_edges': 50, 'num_important_edges': 8}\n",
      "\n",
      "Explaining test graph index 18 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9908802509307861, 'p_keep': 0.38783788681030273, 'p_comp': 0.780297577381134, 'sparsity': 0.75, 'fidelity_plus': 0.39140742430378056, 'fidelity_minus': 0.21252081000891954, 'expl_time_sec': 0.37467527389526367, 'num_edges': 44, 'num_important_edges': 11}\n",
      "\n",
      "Explaining test graph index 19 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9955984354019165, 'p_keep': 0.3845638036727905, 'p_comp': 0.9275737404823303, 'sparsity': 0.8703703703703703, 'fidelity_plus': 0.3862639694863971, 'fidelity_minus': 0.06832543372984012, 'expl_time_sec': 0.3884270191192627, 'num_edges': 54, 'num_important_edges': 7}\n",
      "\n",
      "Explaining test graph index 20 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9911682605743408, 'p_keep': 0.38593631982803345, 'p_comp': 0.6638278365135193, 'sparsity': 0.8181818181818181, 'fidelity_plus': 0.389375179956226, 'fidelity_minus': 0.3302571693237446, 'expl_time_sec': 0.3816063404083252, 'num_edges': 44, 'num_important_edges': 8}\n",
      "\n",
      "Explaining test graph index 21 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.942498505115509, 'p_keep': 0.38734346628189087, 'p_comp': 0.8579028844833374, 'sparsity': 0.8888888888888888, 'fidelity_plus': 0.4109751518750892, 'fidelity_minus': 0.08975676902723995, 'expl_time_sec': 0.3654365539550781, 'num_edges': 36, 'num_important_edges': 4}\n",
      "\n",
      "Explaining test graph index 22 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9941374063491821, 'p_keep': 0.3883132338523865, 'p_comp': 0.8486012816429138, 'sparsity': 0.8787878787878788, 'fidelity_plus': 0.3906031815847344, 'fidelity_minus': 0.14639437544225153, 'expl_time_sec': 0.381638765335083, 'num_edges': 66, 'num_important_edges': 8}\n",
      "\n",
      "Explaining test graph index 23 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.6893593072891235, 'p_keep': 0.40018969774246216, 'p_comp': 0.5714366436004639, 'sparsity': 0.6944444444444444, 'fidelity_plus': 0.5805241091415612, 'fidelity_minus': 0.17106124838204562, 'expl_time_sec': 0.3884568214416504, 'num_edges': 36, 'num_important_edges': 11}\n",
      "\n",
      "Explaining test graph index 24 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.994403064250946, 'p_keep': 0.37645986676216125, 'p_comp': 0.8076690435409546, 'sparsity': 0.8409090909090909, 'fidelity_plus': 0.3785787476889335, 'fidelity_minus': 0.1877850415220237, 'expl_time_sec': 0.3745267391204834, 'num_edges': 44, 'num_important_edges': 7}\n",
      "\n",
      "Explaining test graph index 25 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9947524070739746, 'p_keep': 0.3837229013442993, 'p_comp': 0.9626668691635132, 'sparsity': 0.8703703703703703, 'fidelity_plus': 0.38574714533539584, 'fidelity_minus': 0.03225479796006703, 'expl_time_sec': 0.38033604621887207, 'num_edges': 54, 'num_important_edges': 7}\n",
      "\n",
      "Explaining test graph index 26 (GIN)...\n",
      "{'pred_class': 0, 'p_full': 0.7157957553863525, 'p_keep': 0.6818416714668274, 'p_comp': 0.622083842754364, 'sparsity': 0.45833333333333337, 'fidelity_plus': 0.9525645637543655, 'fidelity_minus': 0.1309199054713691, 'expl_time_sec': 0.39328742027282715, 'num_edges': 24, 'num_important_edges': 13}\n",
      "\n",
      "Explaining test graph index 27 (GIN)...\n",
      "{'pred_class': 0, 'p_full': 0.7146998047828674, 'p_keep': 0.7154501676559448, 'p_comp': 0.6216034293174744, 'sparsity': 0.36363636363636365, 'fidelity_plus': 1.001049899367617, 'fidelity_minus': 0.13025941079370607, 'expl_time_sec': 0.3954904079437256, 'num_edges': 22, 'num_important_edges': 14}\n",
      "\n",
      "Explaining test graph index 28 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9747247695922852, 'p_keep': 0.38602325320243835, 'p_comp': 0.400358647108078, 'sparsity': 0.6176470588235294, 'fidelity_plus': 0.39603308056273867, 'fidelity_minus': 0.5892597996914115, 'expl_time_sec': 0.3867008686065674, 'num_edges': 34, 'num_important_edges': 13}\n",
      "\n",
      "Explaining test graph index 29 (GIN)...\n",
      "{'pred_class': 0, 'p_full': 0.6550905108451843, 'p_keep': 0.6401898860931396, 'p_comp': 0.6431175470352173, 'sparsity': 0.6842105263157895, 'fidelity_plus': 0.9772540977080859, 'fidelity_minus': 0.018276808489440244, 'expl_time_sec': 0.3873176574707031, 'num_edges': 38, 'num_important_edges': 12}\n",
      "\n",
      "Explaining test graph index 30 (GIN)...\n",
      "{'pred_class': 0, 'p_full': 0.6762861609458923, 'p_keep': 0.6710073947906494, 'p_comp': 0.6048612594604492, 'sparsity': 0.6153846153846154, 'fidelity_plus': 0.9921944785209567, 'fidelity_minus': 0.10561343054180528, 'expl_time_sec': 0.37230753898620605, 'num_edges': 26, 'num_important_edges': 10}\n",
      "\n",
      "Explaining test graph index 31 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9728333950042725, 'p_keep': 0.39220288395881653, 'p_comp': 0.6833922266960144, 'sparsity': 0.8200000000000001, 'fidelity_plus': 0.4031552432028652, 'fidelity_minus': 0.29752388208978675, 'expl_time_sec': 0.38161706924438477, 'num_edges': 50, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 32 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.6227807998657227, 'p_keep': 0.39530566334724426, 'p_comp': 0.40879935026168823, 'sparsity': 0.8392857142857143, 'fidelity_plus': 0.6347428556443548, 'fidelity_minus': 0.3435903124344405, 'expl_time_sec': 0.3653416633605957, 'num_edges': 56, 'num_important_edges': 9}\n",
      "\n",
      "Explaining test graph index 33 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.9146447777748108, 'p_keep': 0.3820517063140869, 'p_comp': 0.5324065685272217, 'sparsity': 0.7666666666666666, 'fidelity_plus': 0.4177050102921482, 'fidelity_minus': 0.41790891779595085, 'expl_time_sec': 0.4053764343261719, 'num_edges': 30, 'num_important_edges': 7}\n",
      "\n",
      "Explaining test graph index 34 (GIN)...\n",
      "{'pred_class': 1, 'p_full': 0.936612606048584, 'p_keep': 0.3725332021713257, 'p_comp': 0.5340113043785095, 'sparsity': 0.7647058823529411, 'fidelity_plus': 0.3977452361472931, 'fidelity_minus': 0.4298482628464545, 'expl_time_sec': 0.3797121047973633, 'num_edges': 34, 'num_important_edges': 8}\n",
      "\n",
      "Explaining test graph index 35 (GIN)...\n",
      "{'pred_class': 0, 'p_full': 0.5528962016105652, 'p_keep': 0.6844126582145691, 'p_comp': 0.6142237782478333, 'sparsity': 0.5, 'fidelity_plus': 1.2378682584921032, 'fidelity_minus': -0.11092059677498822, 'expl_time_sec': 0.37462663650512695, 'num_edges': 28, 'num_important_edges': 14}\n",
      "\n",
      "Explaining test graph index 36 (GIN)...\n",
      "{'pred_class': 0, 'p_full': 0.5449326634407043, 'p_keep': 0.6544389724731445, 'p_comp': 0.5874045491218567, 'sparsity': 0.75, 'fidelity_plus': 1.200953835912528, 'fidelity_minus': -0.07793969517808841, 'expl_time_sec': 0.37943053245544434, 'num_edges': 28, 'num_important_edges': 7}\n",
      "   pred_class    p_full    p_keep    p_comp  sparsity  fidelity_plus  \\\n",
      "0           1  0.668149  0.761442  0.674484  0.681818       1.139629   \n",
      "1           1  0.789170  0.841828  0.833727  0.631579       1.066725   \n",
      "2           1  0.731861  0.819777  0.785212  0.454545       1.120127   \n",
      "3           1  0.519120  0.695632  0.521379  0.714286       1.340020   \n",
      "4           1  0.565174  0.539078  0.793832  0.545455       0.953827   \n",
      "\n",
      "   fidelity_minus  expl_time_sec  num_edges  num_important_edges  \n",
      "0       -0.009482       0.544358         44                   14  \n",
      "1       -0.056461       0.495482         38                   14  \n",
      "2       -0.072899       0.493190         22                   12  \n",
      "3       -0.004351       0.502577         28                    8  \n",
      "4       -0.404581       0.469733         22                   10  \n",
      "   pred_class    p_full    p_keep    p_comp  sparsity  fidelity_plus  \\\n",
      "0           1  0.650341  0.549395  0.603444  0.750000       0.844780   \n",
      "1           1  0.790164  0.630717  0.665109  0.631579       0.798211   \n",
      "2           1  0.723230  0.580568  0.726966  0.590909       0.802743   \n",
      "3           0  0.537831  0.535494  0.577675  0.607143       0.995653   \n",
      "4           1  0.543546  0.440003  0.517842  0.454545       0.809505   \n",
      "\n",
      "   fidelity_minus  expl_time_sec  num_edges  num_important_edges  \n",
      "0        0.072111       0.388468         44                   11  \n",
      "1        0.158264       0.374792         38                   14  \n",
      "2       -0.005166       0.374438         22                    9  \n",
      "3       -0.074082       0.367626         28                   11  \n",
      "4        0.047289       0.381551         22                   12  \n",
      "   pred_class    p_full    p_keep    p_comp  sparsity  fidelity_plus  \\\n",
      "0           1  0.902692  0.389574  0.619205  0.795455       0.431570   \n",
      "1           1  0.977098  0.386256  0.822737  0.763158       0.395309   \n",
      "2           1  0.739731  0.389358  0.500931  0.727273       0.526350   \n",
      "3           0  0.598424  0.619936  0.593384  0.500000       1.035948   \n",
      "4           0  0.697784  0.681005  0.619602  0.454545       0.975954   \n",
      "\n",
      "   fidelity_minus  expl_time_sec  num_edges  num_important_edges  \n",
      "0        0.314046       0.377692         44                    9  \n",
      "1        0.157979       0.355752         38                    9  \n",
      "2        0.322820       0.365088         22                    6  \n",
      "3        0.008423       0.374124         28                   14  \n",
      "4        0.112043       0.388482         22                   12  \n",
      "Saved GCN explanation metrics -> C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\results\\gcn_explanation_metrics.csv\n",
      "Saved GraphSAGE explanation metrics -> C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\results\\sage_explanation_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.explain import Explainer, GNNExplainer, ModelConfig\n",
    "import time\n",
    "\n",
    "def explain_graph_with_metrics(\n",
    "    model,\n",
    "    data,\n",
    "    device,\n",
    "    threshold=0.5,\n",
    "    explainer_epochs=200,\n",
    "):\n",
    "    \"\"\"\n",
    "    WHAT:\n",
    "        Use the *new* PyG Explainer + GNNExplainer API to produce an\n",
    "        edge-level explanation for a single graph, then compute:\n",
    "          - sparsity\n",
    "          - fidelity+  (keep important edges)\n",
    "          - fidelity-  (keep only unimportant edges)\n",
    "          - explanation runtime\n",
    "\n",
    "    HOW (high level):\n",
    "        1. Wrap your trained model in `Explainer` with `GNNExplainer`.\n",
    "        2. Call the explainer on (x, edge_index, batch) to get an\n",
    "           Explanation object.\n",
    "        3. Extract edge_mask and threshold it.\n",
    "        4. Build:\n",
    "            - subgraph_keep: only important edges\n",
    "            - subgraph_comp: only unimportant edges\n",
    "        5. Evaluate model probabilities on:\n",
    "            - full graph\n",
    "            - subgraph_keep\n",
    "            - subgraph_comp\n",
    "        6. Compute sparsity, fidelity+, fidelity-.\n",
    "\n",
    "    NOTE:\n",
    "        We assume your model returns *raw logits* (as in your GCN/GraphSAGE),\n",
    "        so we use ModelConfig(return_type='raw').\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # We'll use the *wrapped* model for Explainer,\n",
    "    # but the original model for probability calculations.\n",
    "    wrapped_model = GraphClassifierWrapper(model).to(device)\n",
    "    wrapped_model.eval()\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # --------- 1) Build the Explainer wrapper ----------\n",
    "    explainer = Explainer(\n",
    "        model=wrapped_model,\n",
    "        algorithm=GNNExplainer(epochs=explainer_epochs),\n",
    "        explanation_type=\"model\",      # explain the model's own prediction\n",
    "        model_config=dict(\n",
    "            mode=\"binary_classification\",   # MUTAG is binary graph classification\n",
    "            task_level=\"graph\",             # graph-level prediction\n",
    "            return_type=\"raw\",              # model outputs raw logits\n",
    "        ),\n",
    "        node_mask_type=\"attributes\",\n",
    "        edge_mask_type=\"object\",\n",
    "    )\n",
    "\n",
    "    # Single-graph tensors\n",
    "    x = data.x.to(device)\n",
    "    edge_index = data.edge_index.to(device)\n",
    "    batch = torch.zeros(data.num_nodes, dtype=torch.long, device=device)\n",
    "\n",
    "    # --------- 2) Run the explainer ----------\n",
    "    start_time = time.time()\n",
    "    explanation = explainer(x, edge_index, batch=batch)\n",
    "    expl_time = time.time() - start_time\n",
    "\n",
    "    node_feat_mask = explanation.node_mask\n",
    "    edge_mask = explanation.edge_mask          # [num_edges]\n",
    "    num_edges = edge_mask.size(0)\n",
    "\n",
    "    # --------- 3) Threshold edge_mask to select important edges ----------\n",
    "    important_mask = edge_mask > threshold\n",
    "    num_important = int(important_mask.sum().item())\n",
    "\n",
    "    # If nothing survives the threshold, keep the single most important edge\n",
    "    if num_important == 0:\n",
    "        top_idx = edge_mask.argmax().unsqueeze(0)\n",
    "        important_mask = torch.zeros_like(edge_mask, dtype=torch.bool)\n",
    "        important_mask[top_idx] = True\n",
    "        num_important = 1\n",
    "\n",
    "    sparsity = 1.0 - num_important / num_edges\n",
    "\n",
    "    # --------- 4) Probabilities on full graph (use original model) ---------\n",
    "    probs_full = get_probs_for_graph(model, data, device)\n",
    "    pred_class = int(probs_full.argmax().item())\n",
    "    p_full = float(probs_full[pred_class].item())\n",
    "\n",
    "    # Helper to create a new Data object with a subset of edges\n",
    "    def make_subgraph(data, edge_mask_bool):\n",
    "        edge_index_sub = data.edge_index[:, edge_mask_bool.cpu()]\n",
    "        return Data(\n",
    "            x=data.x,\n",
    "            edge_index=edge_index_sub,\n",
    "            y=data.y,\n",
    "        )\n",
    "\n",
    "    # Subgraph with only important edges\n",
    "    data_keep = make_subgraph(data, important_mask)\n",
    "    # Complement: only unimportant edges\n",
    "    comp_mask = ~important_mask\n",
    "    data_comp = make_subgraph(data, comp_mask)\n",
    "\n",
    "    # --------- 5) Probabilities on subgraphs (original model) ----------\n",
    "    probs_keep = get_probs_for_graph(model, data_keep, device)\n",
    "    probs_comp = get_probs_for_graph(model, data_comp, device)\n",
    "\n",
    "    p_keep = float(probs_keep[pred_class].item())\n",
    "    p_comp = float(probs_comp[pred_class].item())\n",
    "\n",
    "    # --------- 6) Fidelity+ and Fidelity- ----------\n",
    "    fidelity_plus  = p_keep / p_full if p_full > 0 else 0.0\n",
    "    fidelity_minus = 1.0 - (p_comp / p_full) if p_full > 0 else 0.0\n",
    "\n",
    "    metrics = {\n",
    "        \"pred_class\": pred_class,\n",
    "        \"p_full\": p_full,\n",
    "        \"p_keep\": p_keep,\n",
    "        \"p_comp\": p_comp,\n",
    "        \"sparsity\": sparsity,\n",
    "        \"fidelity_plus\": fidelity_plus,\n",
    "        \"fidelity_minus\": fidelity_minus,\n",
    "        \"expl_time_sec\": expl_time,\n",
    "        \"num_edges\": int(num_edges),\n",
    "        \"num_important_edges\": int(num_important),\n",
    "    }\n",
    "\n",
    "    return metrics, node_feat_mask, edge_mask\n",
    "\n",
    "\n",
    "\n",
    "test_graph_indices_to_explain = [i for i in range(37)]\n",
    "\n",
    "gcn_expl_results = []\n",
    "sage_expl_results = []\n",
    "gin_expl_results = []\n",
    "\n",
    "print(\"\\n=== GCN GNNExplainer Results ===\")\n",
    "for idx in test_graph_indices_to_explain:\n",
    "    data_ex = test_dataset[idx]\n",
    "    print(f\"\\nExplaining test graph index {idx} (GCN)...\")\n",
    "    metrics, node_mask, edge_mask = explain_graph_with_metrics(\n",
    "        model=gcn_model,\n",
    "        data=data_ex,\n",
    "        device=device,\n",
    "        threshold=0.5,\n",
    "        explainer_epochs=200,\n",
    "    )\n",
    "    print(metrics)\n",
    "    gcn_expl_results.append(metrics)\n",
    "\n",
    "print(\"\\n=== GraphSAGE GNNExplainer Results ===\")\n",
    "for idx in test_graph_indices_to_explain:\n",
    "    data_ex = test_dataset[idx]\n",
    "    print(f\"\\nExplaining test graph index {idx} (GraphSAGE)...\")\n",
    "    metrics, node_mask, edge_mask = explain_graph_with_metrics(\n",
    "        model=sage_model,\n",
    "        data=data_ex,\n",
    "        device=device,\n",
    "        threshold=0.5,\n",
    "        explainer_epochs=200,\n",
    "    )\n",
    "    print(metrics)\n",
    "    sage_expl_results.append(metrics)\n",
    "    \n",
    "print(\"\\n=== GIN GNNExplainer Results ===\")\n",
    "for idx in test_graph_indices_to_explain:\n",
    "    data_ex = test_dataset[idx]\n",
    "    print(f\"\\nExplaining test graph index {idx} (GIN)...\")\n",
    "    metrics, node_mask, edge_mask = explain_graph_with_metrics(\n",
    "        model=gin_model,\n",
    "        data=data_ex,\n",
    "        device=device,\n",
    "        threshold=0.5,\n",
    "        explainer_epochs=200,\n",
    "    )\n",
    "    print(metrics)\n",
    "    gin_expl_results.append(metrics)\n",
    "    \n",
    "gcn_expl_df = pd.DataFrame(gcn_expl_results)\n",
    "sage_expl_df = pd.DataFrame(sage_expl_results)\n",
    "gin_expl_df = pd.DataFrame(gin_expl_results)\n",
    "\n",
    "print(gcn_expl_df.head())\n",
    "print(sage_expl_df.head())\n",
    "print(gin_expl_df.head())\n",
    "\n",
    "gcn_expl_df.to_csv(r\"C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\gcn_expl_df.csv\")\n",
    "sage_expl_df.to_csv(r\"C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\sage_expl_df.csv\")\n",
    "gin_expl_df.to_csv(r\"C:\\Users\\default.LAPTOP-D71TUC29\\Documents\\School\\CS 6010 - Data Science Programming\\Project 3\\gin_expl_df.csv\")\n",
    "\n",
    "gcn_expl_csv = RESULTS_DIR / \"gcn_explanation_metrics.csv\"\n",
    "sage_expl_csv = RESULTS_DIR / \"sage_explanation_metrics.csv\"\n",
    "gcn_expl_df.to_csv(gcn_expl_csv, index=False)\n",
    "sage_expl_df.to_csv(sage_expl_csv, index=False)\n",
    "print(f\"Saved GCN explanation metrics -> {gcn_expl_csv}\")\n",
    "print(f\"Saved GraphSAGE explanation metrics -> {sage_expl_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2c375a09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>seed</th>\n",
       "      <th>support_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Best classical (LinearSVM)</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GCN</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GraphSAGE</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GIN</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>42</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        model   test_f1  seed  support_ratio\n",
       "0  Best classical (LinearSVM)  1.000000     1            0.1\n",
       "1                         GCN  0.806452    42            NaN\n",
       "2                   GraphSAGE  0.793103    42            NaN\n",
       "3                         GIN  0.846154    42            NaN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "comparison_rows = []\n",
    "\n",
    "# 1) Best classical run overall\n",
    "idx_max = classic_results[\"test_f1\"].idxmax()\n",
    "best_classic = classic_results.loc[idx_max]\n",
    "comparison_rows.append({\n",
    "    \"model\": f\"Best classical ({best_classic['model']})\",\n",
    "    \"test_f1\": best_classic[\"test_f1\"],\n",
    "    \"seed\": best_classic[\"seed\"],\n",
    "    \"support_ratio\": best_classic[\"support_ratio\"],\n",
    "})\n",
    "\n",
    "# 2) GNNs (values pulled from your GNN evaluation cells)\n",
    "comparison_rows.append({\"model\": \"GCN\",       \"test_f1\": gcn_test_f1,  \"seed\": SEED, \"support_ratio\": np.nan})\n",
    "comparison_rows.append({\"model\": \"GraphSAGE\",\"test_f1\": sage_test_f1, \"seed\": SEED, \"support_ratio\": np.nan})\n",
    "comparison_rows.append({\"model\": \"GIN\",      \"test_f1\": gin_test_f1,  \"seed\": SEED, \"support_ratio\": np.nan})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "display(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a840311a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# FIGURE 5: Explanation metrics\n",
    "# -------------------------------\n",
    "# WHAT:\n",
    "#   Compare average:\n",
    "#     - sparsity\n",
    "#     - fidelity_plus\n",
    "#     - fidelity_minus\n",
    "#   for GCN vs GraphSAGE explanations.\n",
    "#\n",
    "# WHY:\n",
    "#   Captures the compactness and faithfulness of explanations\n",
    "#   and lets you compare GNNs in the Q-4 discussion.\n",
    "\n",
    "\n",
    "def plot_explanation_metrics(gcn_expl_df, sage_expl_df, gin_expl_df):\n",
    "    # Compute mean metrics for each model\n",
    "    metrics = [\"sparsity\", \"fidelity_plus\", \"fidelity_minus\"]\n",
    "\n",
    "    gcn_means = gcn_expl_df[metrics].mean()\n",
    "    sage_means = sage_expl_df[metrics].mean()\n",
    "    gin_means = gin_expl_df[metrics].mean()\n",
    "\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.25\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    ax.bar(x - width, gcn_means[metrics], width, label=\"GCN\")\n",
    "    ax.bar(x, sage_means[metrics], width, label=\"GraphSAGE\")\n",
    "    ax.bar(x + width, gin_means[metrics], width, label=\"GIN\")\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([\"Sparsity\", \"Fidelity+\", \"Fidelity−\"])\n",
    "    ax.set_ylim(0, 1.0)\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Explanation quality metrics (GNNExplainer)\")\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example call:\n",
    "plot_explanation_metrics(gcn_expl_df, sage_expl_df, gin_expl_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cd9eb066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# FIGURE 1: Pipeline diagram\n",
    "# -------------------------------\n",
    "# WHAT:\n",
    "#   Simple left-to-right diagram that shows:\n",
    "#     MUTAG → (Branch A) gSpan + classic ML\n",
    "#           → (Branch B) GNNs + explanations\n",
    "#\n",
    "# HOW:\n",
    "#   Use Matplotlib shapes (rectangles + arrows) so you can\n",
    "#   generate it directly in Python and export as PNG / PDF.\n",
    "\n",
    "\n",
    "def plot_pipeline_diagram():\n",
    "    # Create a wide horizontal figure\n",
    "    fig, ax = plt.subplots(figsize=(12, 4))\n",
    "    \n",
    "    # Make sure the background is white (useful if you're in dark mode)\n",
    "    fig.patch.set_facecolor(\"white\")\n",
    "    ax.set_facecolor(\"white\")\n",
    "    \n",
    "    # Turn off axes, we just want shapes\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 1) Helper to draw labeled boxes\n",
    "    # ---------------------------------------\n",
    "    def draw_box(x, y, text, width=2.2, height=0.9, fontsize=10):\n",
    "        \"\"\"\n",
    "        Draw a rectangle with a centered text label.\n",
    "\n",
    "        x, y = lower-left corner of the box\n",
    "        width, height = box size\n",
    "        text = label to show inside\n",
    "        \"\"\"\n",
    "        rect = plt.Rectangle(\n",
    "            (x, y),\n",
    "            width,\n",
    "            height,\n",
    "            fill=False,            # no fill, just outline\n",
    "            linewidth=2,\n",
    "            edgecolor=\"black\"      # EXPLICIT edge color so it always shows\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(\n",
    "            x + width / 2,\n",
    "            y + height / 2,\n",
    "            text,\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            fontsize=fontsize\n",
    "        )\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 2) Draw the boxes (pipeline stages)\n",
    "    # ---------------------------------------\n",
    "\n",
    "    # Main data source box\n",
    "    draw_box(0, 1.5, \"MUTAG\\n(graph dataset)\")\n",
    "\n",
    "    # Branch A: classic ML\n",
    "    draw_box(3, 2.5, \"gSpan\\n(frequent subgraphs)\")\n",
    "    draw_box(6, 2.5, \"Subgraph feature\\nmatrix (X)\")\n",
    "    draw_box(9, 3.1, \"SVM\")\n",
    "    draw_box(9, 1.9, \"Random Forest\")\n",
    "\n",
    "    # Branch B: GNNs\n",
    "    draw_box(3, 0.5, \"GCN\")\n",
    "    draw_box(6, 0.5, \"GraphSAGE\")\n",
    "    draw_box(9, 0.5, \"GNNExplainer\\n(explanations)\")\n",
    "\n",
    "    # Final comparison box\n",
    "    draw_box(11.8, 1.5, \"Comparisons\\n(Q1–Q4)\", width=2.2, height=1.2)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 3) Draw arrows (explicit color)\n",
    "    # ---------------------------------------\n",
    "    arrow_style = dict(arrowstyle=\"->\", linewidth=1.5, color=\"black\")\n",
    "\n",
    "    # Arrows from MUTAG to both branches\n",
    "    ax.annotate(\"\", xy=(3, 2.95), xytext=(2.2, 2.1), arrowprops=arrow_style)\n",
    "    ax.annotate(\"\", xy=(3, 0.95), xytext=(2.2, 1.9), arrowprops=arrow_style)\n",
    "\n",
    "    # Branch A arrows: gSpan → X → SVM/RF\n",
    "    ax.annotate(\"\", xy=(6, 2.95), xytext=(5.2, 2.95), arrowprops=arrow_style)\n",
    "    ax.annotate(\"\", xy=(9, 3.55), xytext=(8.2, 2.95), arrowprops=arrow_style)\n",
    "    ax.annotate(\"\", xy=(9, 2.35), xytext=(8.2, 2.95), arrowprops=arrow_style)\n",
    "\n",
    "    # Branch B arrows: MUTAG → GCN → GraphSAGE → GNNExplainer\n",
    "    ax.annotate(\"\", xy=(3, 0.95), xytext=(2.2, 1.9), arrowprops=arrow_style)\n",
    "    ax.annotate(\"\", xy=(6, 0.95), xytext=(5.2, 0.95), arrowprops=arrow_style)\n",
    "    ax.annotate(\"\", xy=(9, 0.95), xytext=(8.2, 0.95), arrowprops=arrow_style)\n",
    "\n",
    "    # Arrows into final comparison box\n",
    "    ax.annotate(\"\", xy=(11.8, 2.2), xytext=(11.2, 3.3), arrowprops=arrow_style)\n",
    "    ax.annotate(\"\", xy=(11.8, 1.8), xytext=(11.2, 0.9), arrowprops=arrow_style)\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # 4) Make sure everything is in view\n",
    "    # ---------------------------------------\n",
    "    # Explicitly set limits so boxes & arrows aren't cropped\n",
    "    ax.set_xlim(-0.5, 14.5)\n",
    "    ax.set_ylim(0, 4)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call the function to create the figure\n",
    "plot_pipeline_diagram()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "296b14c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FIGURE 1: MUTAG dataset overview\n",
    "#   - Histogram of #nodes per graph\n",
    "#   - Histogram of #edges per graph\n",
    "#   - Bar chart of class balance\n",
    "# ---------------------------------------------------------\n",
    "# ASSUMPTION:\n",
    "#   You already have MUTAG loaded as:\n",
    "#       dataset = TUDataset(root=\"data\", name=\"MUTAG\")\n",
    "#   If not, uncomment and run the lines below:\n",
    "#\n",
    "# from torch_geometric.datasets import TUDataset\n",
    "# dataset = TUDataset(root=\"data\", name=\"MUTAG\")\n",
    "\n",
    "# ----- 1) Collect simple per-graph stats -----\n",
    "num_nodes = [data.num_nodes for data in dataset]     # number of nodes in each graph\n",
    "num_edges = [data.num_edges for data in dataset]     # number of edges in each graph\n",
    "labels    = [int(data.y.item()) for data in dataset] # graph labels: 0/1 for MUTAG\n",
    "\n",
    "label_counts = Counter(labels)  # how many graphs in each class\n",
    "\n",
    "# ----- 2) Create the figure with 3 subplots -----\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# (a) Histogram of #nodes per graph\n",
    "axes[0].hist(num_nodes, bins=10, edgecolor=\"black\")\n",
    "axes[0].set_title(\"Distribution of #nodes per graph\")\n",
    "axes[0].set_xlabel(\"#nodes\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# (b) Histogram of #edges per graph\n",
    "axes[1].hist(num_edges, bins=10, edgecolor=\"black\")\n",
    "axes[1].set_title(\"Distribution of #edges per graph\")\n",
    "axes[1].set_xlabel(\"#edges\")\n",
    "axes[1].set_ylabel(\"Count\")\n",
    "\n",
    "# (c) Class balance bar chart\n",
    "classes = sorted(label_counts.keys())\n",
    "counts  = [label_counts[c] for c in classes]\n",
    "\n",
    "axes[2].bar([str(c) for c in classes], counts, edgecolor=\"black\")\n",
    "axes[2].set_title(\"Class distribution (MUTAG)\")\n",
    "axes[2].set_xlabel(\"Class label\")\n",
    "axes[2].set_ylabel(\"#graphs\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "818d5150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FIGURE 2: Classic ML ablation over min_support\n",
    "#   - Left y-axis: F1 score vs min_support (SVM + RF)\n",
    "#   - Right y-axis: number of features vs min_support\n",
    "# ---------------------------------------------------------\n",
    "# ASSUMPTION:\n",
    "#   You already have ablation_df from ablation_over_supports().\n",
    "#   Inspect it with:\n",
    "#       print(ablation_df.head())\n",
    "#   to make sure the column names match.\n",
    "\n",
    "def plot_classic_ablation(ablation_df):\n",
    "    # Sort by support so the curves are monotone in x\n",
    "    ablation_sorted = ablation_df.sort_values(\"support\")\n",
    "\n",
    "    x = ablation_sorted[\"support\"]\n",
    "\n",
    "    # Create figure with twin y-axis:\n",
    "    #   ax1: F1 scores for SVM/RF\n",
    "    #   ax2: number of features\n",
    "    fig, ax1 = plt.subplots(figsize=(7, 4))\n",
    "\n",
    "    # ----- 1) Plot F1 scores on left y-axis -----\n",
    "    ax1.plot(x, ablation_sorted[\"svm_f1\"],\n",
    "             marker=\"o\", linestyle=\"-\", label=\"SVM F1\")\n",
    "    ax1.plot(x, ablation_sorted[\"rf_f1\"],\n",
    "             marker=\"s\", linestyle=\"--\", label=\"RF F1\")\n",
    "\n",
    "    ax1.set_xlabel(\"min_support (absolute #graphs)\")\n",
    "    ax1.set_ylabel(\"F1 score\")\n",
    "    ax1.set_ylim(0.0, 1.0)\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "\n",
    "    # ----- 2) Plot #features on right y-axis -----\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(x, ablation_sorted[\"n_features\"],\n",
    "             marker=\"^\", linestyle=\"-.\", color=\"gray\", label=\"#features\")\n",
    "\n",
    "    ax2.set_ylabel(\"#frequent subgraph features\")\n",
    "    # Optional: force integer ticks if you want\n",
    "    # ax2.yaxis.get_major_locator().set_params(integer=True)\n",
    "\n",
    "    # Title that explains the story in one line\n",
    "    plt.title(\"Effect of min_support on F1 and feature dimensionality\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call it:\n",
    "plot_classic_ablation(ablation_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "63eab62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_class</th>\n",
       "      <th>p_full</th>\n",
       "      <th>p_keep</th>\n",
       "      <th>p_comp</th>\n",
       "      <th>sparsity</th>\n",
       "      <th>fidelity_plus</th>\n",
       "      <th>fidelity_minus</th>\n",
       "      <th>expl_time_sec</th>\n",
       "      <th>num_edges</th>\n",
       "      <th>num_important_edges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.668149</td>\n",
       "      <td>0.768809</td>\n",
       "      <td>0.638324</td>\n",
       "      <td>0.613636</td>\n",
       "      <td>1.150655</td>\n",
       "      <td>0.044638</td>\n",
       "      <td>0.461493</td>\n",
       "      <td>44</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.789170</td>\n",
       "      <td>0.844298</td>\n",
       "      <td>0.828970</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>1.069856</td>\n",
       "      <td>-0.050433</td>\n",
       "      <td>0.419495</td>\n",
       "      <td>38</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.731861</td>\n",
       "      <td>0.811975</td>\n",
       "      <td>0.770752</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.109467</td>\n",
       "      <td>-0.053140</td>\n",
       "      <td>0.429827</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.519120</td>\n",
       "      <td>0.596624</td>\n",
       "      <td>0.590463</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>1.149297</td>\n",
       "      <td>-0.137429</td>\n",
       "      <td>0.428525</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.565174</td>\n",
       "      <td>0.525624</td>\n",
       "      <td>0.747844</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.930021</td>\n",
       "      <td>-0.323210</td>\n",
       "      <td>0.422850</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0.760478</td>\n",
       "      <td>0.822699</td>\n",
       "      <td>0.776463</td>\n",
       "      <td>0.816667</td>\n",
       "      <td>1.081819</td>\n",
       "      <td>-0.021020</td>\n",
       "      <td>0.442215</td>\n",
       "      <td>60</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0.715549</td>\n",
       "      <td>0.754209</td>\n",
       "      <td>0.799072</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.054028</td>\n",
       "      <td>-0.116725</td>\n",
       "      <td>0.425852</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0.698812</td>\n",
       "      <td>0.761284</td>\n",
       "      <td>0.752148</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.089398</td>\n",
       "      <td>-0.076324</td>\n",
       "      <td>0.423147</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0.524958</td>\n",
       "      <td>0.689180</td>\n",
       "      <td>0.596233</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.312830</td>\n",
       "      <td>-0.135773</td>\n",
       "      <td>0.425375</td>\n",
       "      <td>48</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0.713366</td>\n",
       "      <td>0.849879</td>\n",
       "      <td>0.716560</td>\n",
       "      <td>0.659091</td>\n",
       "      <td>1.191364</td>\n",
       "      <td>-0.004477</td>\n",
       "      <td>0.432330</td>\n",
       "      <td>44</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0.636507</td>\n",
       "      <td>0.752492</td>\n",
       "      <td>0.728358</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.182220</td>\n",
       "      <td>-0.144304</td>\n",
       "      <td>0.422693</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0.718091</td>\n",
       "      <td>0.780664</td>\n",
       "      <td>0.761831</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>1.087139</td>\n",
       "      <td>-0.060913</td>\n",
       "      <td>0.437196</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0.726989</td>\n",
       "      <td>0.819346</td>\n",
       "      <td>0.732953</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>1.127041</td>\n",
       "      <td>-0.008204</td>\n",
       "      <td>0.426761</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0.643762</td>\n",
       "      <td>0.743654</td>\n",
       "      <td>0.601482</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.155169</td>\n",
       "      <td>0.065677</td>\n",
       "      <td>0.422726</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0.762663</td>\n",
       "      <td>0.845407</td>\n",
       "      <td>0.790275</td>\n",
       "      <td>0.710526</td>\n",
       "      <td>1.108493</td>\n",
       "      <td>-0.036205</td>\n",
       "      <td>0.423602</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.667236</td>\n",
       "      <td>0.712215</td>\n",
       "      <td>0.752623</td>\n",
       "      <td>0.617647</td>\n",
       "      <td>1.067411</td>\n",
       "      <td>-0.127970</td>\n",
       "      <td>0.442214</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0.731907</td>\n",
       "      <td>0.817374</td>\n",
       "      <td>0.782597</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>1.116773</td>\n",
       "      <td>-0.069258</td>\n",
       "      <td>0.426262</td>\n",
       "      <td>40</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0.735094</td>\n",
       "      <td>0.793526</td>\n",
       "      <td>0.771196</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>1.079489</td>\n",
       "      <td>-0.049113</td>\n",
       "      <td>0.465310</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0.800259</td>\n",
       "      <td>0.856525</td>\n",
       "      <td>0.820968</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.070309</td>\n",
       "      <td>-0.025878</td>\n",
       "      <td>0.490678</td>\n",
       "      <td>44</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0.813701</td>\n",
       "      <td>0.869461</td>\n",
       "      <td>0.819863</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.068527</td>\n",
       "      <td>-0.007574</td>\n",
       "      <td>0.479800</td>\n",
       "      <td>54</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0.800184</td>\n",
       "      <td>0.862253</td>\n",
       "      <td>0.825558</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>1.077568</td>\n",
       "      <td>-0.031710</td>\n",
       "      <td>0.442577</td>\n",
       "      <td>44</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0.788478</td>\n",
       "      <td>0.863122</td>\n",
       "      <td>0.833364</td>\n",
       "      <td>0.638889</td>\n",
       "      <td>1.094668</td>\n",
       "      <td>-0.056927</td>\n",
       "      <td>0.427965</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0.769797</td>\n",
       "      <td>0.808505</td>\n",
       "      <td>0.790048</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>1.050283</td>\n",
       "      <td>-0.026306</td>\n",
       "      <td>0.430271</td>\n",
       "      <td>66</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0.713421</td>\n",
       "      <td>0.839824</td>\n",
       "      <td>0.729111</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.177179</td>\n",
       "      <td>-0.021993</td>\n",
       "      <td>0.422647</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0.799063</td>\n",
       "      <td>0.865049</td>\n",
       "      <td>0.835168</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>1.082580</td>\n",
       "      <td>-0.045185</td>\n",
       "      <td>0.441398</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0.813717</td>\n",
       "      <td>0.833977</td>\n",
       "      <td>0.835100</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>1.024898</td>\n",
       "      <td>-0.026278</td>\n",
       "      <td>0.428288</td>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0.575696</td>\n",
       "      <td>0.319341</td>\n",
       "      <td>0.484575</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.554703</td>\n",
       "      <td>0.158280</td>\n",
       "      <td>0.424916</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0.516796</td>\n",
       "      <td>0.648555</td>\n",
       "      <td>0.593496</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>1.254954</td>\n",
       "      <td>-0.148413</td>\n",
       "      <td>0.425626</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0.777718</td>\n",
       "      <td>0.871485</td>\n",
       "      <td>0.844345</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>1.120567</td>\n",
       "      <td>-0.085670</td>\n",
       "      <td>0.430675</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0.597288</td>\n",
       "      <td>0.726606</td>\n",
       "      <td>0.612762</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>1.216509</td>\n",
       "      <td>-0.025908</td>\n",
       "      <td>0.419464</td>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>0.508020</td>\n",
       "      <td>0.680630</td>\n",
       "      <td>0.483873</td>\n",
       "      <td>0.615385</td>\n",
       "      <td>1.339770</td>\n",
       "      <td>0.047532</td>\n",
       "      <td>0.428857</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>0.734156</td>\n",
       "      <td>0.804691</td>\n",
       "      <td>0.736283</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.096077</td>\n",
       "      <td>-0.002897</td>\n",
       "      <td>0.433174</td>\n",
       "      <td>50</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>0.559854</td>\n",
       "      <td>0.677527</td>\n",
       "      <td>0.579014</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>1.210184</td>\n",
       "      <td>-0.034222</td>\n",
       "      <td>0.446151</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>0.768389</td>\n",
       "      <td>0.804765</td>\n",
       "      <td>0.781117</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>1.047341</td>\n",
       "      <td>-0.016564</td>\n",
       "      <td>0.417962</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>0.782199</td>\n",
       "      <td>0.848028</td>\n",
       "      <td>0.814536</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>1.084159</td>\n",
       "      <td>-0.041342</td>\n",
       "      <td>0.450923</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>0.629582</td>\n",
       "      <td>0.801378</td>\n",
       "      <td>0.627670</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>1.272874</td>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.426364</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>0.631964</td>\n",
       "      <td>0.692069</td>\n",
       "      <td>0.758659</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>1.095109</td>\n",
       "      <td>-0.200479</td>\n",
       "      <td>0.431327</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pred_class    p_full    p_keep    p_comp  sparsity  fidelity_plus  \\\n",
       "0            1  0.668149  0.768809  0.638324  0.613636       1.150655   \n",
       "1            1  0.789170  0.844298  0.828970  0.526316       1.069856   \n",
       "2            1  0.731861  0.811975  0.770752  0.500000       1.109467   \n",
       "3            1  0.519120  0.596624  0.590463  0.642857       1.149297   \n",
       "4            1  0.565174  0.525624  0.747844  0.545455       0.930021   \n",
       "5            1  0.760478  0.822699  0.776463  0.816667       1.081819   \n",
       "6            1  0.715549  0.754209  0.799072  0.500000       1.054028   \n",
       "7            1  0.698812  0.761284  0.752148  0.750000       1.089398   \n",
       "8            1  0.524958  0.689180  0.596233  0.833333       1.312830   \n",
       "9            1  0.713366  0.849879  0.716560  0.659091       1.191364   \n",
       "10           1  0.636507  0.752492  0.728358  0.666667       1.182220   \n",
       "11           1  0.718091  0.780664  0.761831  0.617647       1.087139   \n",
       "12           1  0.726989  0.819346  0.732953  0.710526       1.127041   \n",
       "13           1  0.643762  0.743654  0.601482  0.600000       1.155169   \n",
       "14           1  0.762663  0.845407  0.790275  0.710526       1.108493   \n",
       "15           1  0.667236  0.712215  0.752623  0.617647       1.067411   \n",
       "16           1  0.731907  0.817374  0.782597  0.725000       1.116773   \n",
       "17           1  0.735094  0.793526  0.771196  0.720000       1.079489   \n",
       "18           1  0.800259  0.856525  0.820968  0.636364       1.070309   \n",
       "19           1  0.813701  0.869461  0.819863  0.777778       1.068527   \n",
       "20           1  0.800184  0.862253  0.825558  0.636364       1.077568   \n",
       "21           1  0.788478  0.863122  0.833364  0.638889       1.094668   \n",
       "22           1  0.769797  0.808505  0.790048  0.818182       1.050283   \n",
       "23           1  0.713421  0.839824  0.729111  0.750000       1.177179   \n",
       "24           1  0.799063  0.865049  0.835168  0.681818       1.082580   \n",
       "25           1  0.813717  0.833977  0.835100  0.851852       1.024898   \n",
       "26           0  0.575696  0.319341  0.484575  0.666667       0.554703   \n",
       "27           1  0.516796  0.648555  0.593496  0.681818       1.254954   \n",
       "28           1  0.777718  0.871485  0.844345  0.676471       1.120567   \n",
       "29           1  0.597288  0.726606  0.612762  0.684211       1.216509   \n",
       "30           1  0.508020  0.680630  0.483873  0.615385       1.339770   \n",
       "31           1  0.734156  0.804691  0.736283  0.700000       1.096077   \n",
       "32           1  0.559854  0.677527  0.579014  0.785714       1.210184   \n",
       "33           1  0.768389  0.804765  0.781117  0.266667       1.047341   \n",
       "34           1  0.782199  0.848028  0.814536  0.764706       1.084159   \n",
       "35           1  0.629582  0.801378  0.627670  0.785714       1.272874   \n",
       "36           1  0.631964  0.692069  0.758659  0.678571       1.095109   \n",
       "\n",
       "    fidelity_minus  expl_time_sec  num_edges  num_important_edges  \n",
       "0         0.044638       0.461493         44                   17  \n",
       "1        -0.050433       0.419495         38                   18  \n",
       "2        -0.053140       0.429827         22                   11  \n",
       "3        -0.137429       0.428525         28                   10  \n",
       "4        -0.323210       0.422850         22                   10  \n",
       "5        -0.021020       0.442215         60                   11  \n",
       "6        -0.116725       0.425852         34                   17  \n",
       "7        -0.076324       0.423147         44                   11  \n",
       "8        -0.135773       0.425375         48                    8  \n",
       "9        -0.004477       0.432330         44                   15  \n",
       "10       -0.144304       0.422693         24                    8  \n",
       "11       -0.060913       0.437196         34                   13  \n",
       "12       -0.008204       0.426761         38                   11  \n",
       "13        0.065677       0.422726         20                    8  \n",
       "14       -0.036205       0.423602         38                   11  \n",
       "15       -0.127970       0.442214         34                   13  \n",
       "16       -0.069258       0.426262         40                   11  \n",
       "17       -0.049113       0.465310         50                   14  \n",
       "18       -0.025878       0.490678         44                   16  \n",
       "19       -0.007574       0.479800         54                   12  \n",
       "20       -0.031710       0.442577         44                   16  \n",
       "21       -0.056927       0.427965         36                   13  \n",
       "22       -0.026306       0.430271         66                   12  \n",
       "23       -0.021993       0.422647         36                    9  \n",
       "24       -0.045185       0.441398         44                   14  \n",
       "25       -0.026278       0.428288         54                    8  \n",
       "26        0.158280       0.424916         24                    8  \n",
       "27       -0.148413       0.425626         22                    7  \n",
       "28       -0.085670       0.430675         34                   11  \n",
       "29       -0.025908       0.419464         38                   12  \n",
       "30        0.047532       0.428857         26                   10  \n",
       "31       -0.002897       0.433174         50                   15  \n",
       "32       -0.034222       0.446151         56                   12  \n",
       "33       -0.016564       0.417962         30                   22  \n",
       "34       -0.041342       0.450923         34                    8  \n",
       "35        0.003036       0.426364         28                    6  \n",
       "36       -0.200479       0.431327         28                    9  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gcn_expl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "693e5ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           model   family  accuracy        f1   roc_auc\n",
      "0            SVM  Classic  0.763158  0.800000  0.838462\n",
      "1  Random Forest  Classic  0.710526  0.755556  0.853846\n",
      "2            GCN      GNN  0.684211  0.806452  0.701538\n",
      "3      GraphSAGE      GNN  0.684211  0.793103  0.698462\n",
      "4            GIN      GNN  0.789474  0.846154  0.833846\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# FIGURE 3: Model comparison (Classic vs GNN)\n",
    "#   Bar chart of Accuracy / F1 / ROC-AUC for:\n",
    "#     - SVM\n",
    "#     - RandomForest\n",
    "#     - GCN\n",
    "#     - GraphSAGE\n",
    "# ---------------------------------------------------------\n",
    "# ASSUMPTION:\n",
    "#   You have metric dicts similar to:\n",
    "#       svm_metrics  = {\"accuracy\": ..., \"f1\": ..., \"roc_auc\": ...}\n",
    "#       rf_metrics   = {\"accuracy\": ..., \"f1\": ..., \"roc_auc\": ...}\n",
    "#       gcn_metrics  = {\"accuracy\": ..., \"f1\": ..., \"roc_auc\": ...}\n",
    "#       sage_metrics = {\"accuracy\": ..., \"f1\": ..., \"roc_auc\": ...}\n",
    "#\n",
    "#   If your GNN metrics are separate variables like:\n",
    "#       gcn_test_acc, gcn_test_f1, ...\n",
    "#   you can manually create the dicts below.\n",
    "\n",
    "# ----- 1) Build a small summary table from metrics dicts -----\n",
    "model_results = [\n",
    "    {\n",
    "        \"model\": \"SVM\",\n",
    "        \"family\": \"Classic\",\n",
    "        \"accuracy\": svm_metrics[\"accuracy\"],\n",
    "        \"f1\":       svm_metrics[\"f1\"],\n",
    "        \"roc_auc\":  svm_metrics.get(\"roc_auc\", np.nan),\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"Random Forest\",\n",
    "        \"family\": \"Classic\",\n",
    "        \"accuracy\": rf_metrics[\"accuracy\"],\n",
    "        \"f1\":       rf_metrics[\"f1\"],\n",
    "        \"roc_auc\":  rf_metrics.get(\"roc_auc\", np.nan),\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"GCN\",\n",
    "        \"family\": \"GNN\",\n",
    "        \"accuracy\": gcn_metrics[\"accuracy\"],\n",
    "        \"f1\":       gcn_metrics[\"f1\"],\n",
    "        \"roc_auc\":  gcn_metrics.get(\"roc_auc\", np.nan),\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"GraphSAGE\",\n",
    "        \"family\": \"GNN\",\n",
    "        \"accuracy\": sage_metrics[\"accuracy\"],\n",
    "        \"f1\":       sage_metrics[\"f1\"],\n",
    "        \"roc_auc\":  sage_metrics.get(\"roc_auc\", np.nan),\n",
    "    },\n",
    "    {\n",
    "        \"model\": \"GIN\",\n",
    "        \"family\": \"GNN\",\n",
    "        \"accuracy\": gin_metrics[\"accuracy\"],\n",
    "        \"f1\":       gin_metrics[\"f1\"],\n",
    "        \"roc_auc\":  gin_metrics.get(\"roc_auc\", np.nan),\n",
    "    },\n",
    "]\n",
    "\n",
    "perf_df = pd.DataFrame(model_results)\n",
    "\n",
    "print(perf_df)\n",
    "\n",
    "# ----- 2) Plot grouped bars for each metric -----\n",
    "metrics = [\"accuracy\", \"f1\", \"roc_auc\"]\n",
    "x = np.arange(len(perf_df))  # one bar group per model\n",
    "width = 0.22                 # bar width\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i]\n",
    "\n",
    "    # Plot bars for this metric\n",
    "    ax.bar(\n",
    "        x,\n",
    "        perf_df[metric],\n",
    "        width,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(perf_df[\"model\"], rotation=20, ha=\"right\")\n",
    "\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    ax.set_ylabel(metric.capitalize())\n",
    "    ax.set_title(f\"{metric.capitalize()} by model\")\n",
    "\n",
    "    # Annotate bars with values\n",
    "    for xi, val in zip(x, perf_df[metric]):\n",
    "        if np.isnan(val):\n",
    "            continue\n",
    "        ax.text(xi, val + 0.01, f\"{val:.2f}\", ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "\n",
    "plt.suptitle(\"Classic ML vs GNN performance on MUTAG\")\n",
    "plt.tight_layout(rect=[0, 0.0, 1, 0.94])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b8595b3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(gcn_fpr,  gcn_tpr,  label=f\"GCN (AUC = {gcn_auc:.2f})\")\n",
    "plt.plot(sage_fpr, sage_tpr, label=f\"GraphSAGE (AUC = {sage_auc:.2f})\")\n",
    "plt.plot(gin_fpr,  gin_tpr,  label=f\"GIN (AUC = {gin_auc:.2f})\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", label=\"Random (AUC = 0.50)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC curves for GNN models on MUTAG\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8d04272f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_gnn_training_curves(gcn_history=None, sage_history=None, gin_history=None):\n",
    "    \"\"\"\n",
    "    Plot validation accuracy and loss over epochs for each GNN model.\n",
    "    \"\"\"\n",
    "    histories = {\n",
    "        \"GCN\": gcn_history,\n",
    "        \"GraphSAGE\": sage_history,\n",
    "        \"GIN\": gin_history,\n",
    "    }\n",
    "\n",
    "    # Filter out None histories\n",
    "    histories = {k: v for k, v in histories.items() if v is not None}\n",
    "\n",
    "    # --- Validation accuracy ---\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for name, hist in histories.items():\n",
    "        df = pd.DataFrame(hist)\n",
    "        plt.plot(df[\"epoch\"], df[\"val_acc\"], label=name)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Validation accuracy\")\n",
    "    plt.title(\"GNN validation accuracy over epochs\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Validation loss ---\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    for name, hist in histories.items():\n",
    "        df = pd.DataFrame(hist)\n",
    "        plt.plot(df[\"epoch\"], df[\"val_loss\"], label=name)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Validation loss\")\n",
    "    plt.title(\"GNN validation loss over epochs\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Call it (pass gin_history if you have it):\n",
    "plot_gnn_training_curves(gcn_history, sage_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "67b687de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x400 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def plot_gnn_ablation_heatmaps(ablation_results, metric=\"test_f1\", agg=\"max\"):\n",
    "    \"\"\"\n",
    "    For each model_type, plot a heatmap of metric vs (hidden_channels, dropout).\n",
    "\n",
    "    - If there are multiple runs per (hidden_channels, dropout) (e.g. different lrs),\n",
    "      we aggregate them first using `agg` ('max' or 'mean').\n",
    "    \"\"\"\n",
    "    model_types = sorted(ablation_results[\"model_type\"].unique())\n",
    "    n_models = len(model_types)\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5 * n_models, 4), sharey=True)\n",
    "\n",
    "    if n_models == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    last_im = None  # to hook into colorbar later\n",
    "\n",
    "    for ax, mtype in zip(axes, model_types):\n",
    "        sub = ablation_results[ablation_results[\"model_type\"] == mtype].copy()\n",
    "\n",
    "        # Aggregate over duplicate (hidden, dropout) combos\n",
    "        if agg == \"max\":\n",
    "            sub_agg = (\n",
    "                sub.groupby([\"hidden_channels\", \"dropout\"], as_index=False)[metric]\n",
    "                   .max()\n",
    "            )\n",
    "        elif agg == \"mean\":\n",
    "            sub_agg = (\n",
    "                sub.groupby([\"hidden_channels\", \"dropout\"], as_index=False)[metric]\n",
    "                   .mean()\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"agg must be 'max' or 'mean'\")\n",
    "\n",
    "        # Pivot: rows = hidden_channels, cols = dropout\n",
    "        pivot = sub_agg.pivot(\n",
    "            index=\"hidden_channels\",\n",
    "            columns=\"dropout\",\n",
    "            values=metric,\n",
    "        )\n",
    "\n",
    "        hidden_vals = pivot.index.values\n",
    "        dropout_vals = pivot.columns.values\n",
    "\n",
    "        im = ax.imshow(pivot.values, aspect=\"auto\", origin=\"lower\")\n",
    "        last_im = im  # keep reference for colorbar\n",
    "\n",
    "        ax.set_xticks(np.arange(len(dropout_vals)))\n",
    "        ax.set_xticklabels(dropout_vals)\n",
    "        ax.set_yticks(np.arange(len(hidden_vals)))\n",
    "        ax.set_yticklabels(hidden_vals)\n",
    "        ax.set_xlabel(\"Dropout\")\n",
    "        ax.set_title(f\"{mtype.upper()} {metric} ({agg})\")\n",
    "\n",
    "        # Annotate cells with metric values\n",
    "        for i in range(len(hidden_vals)):\n",
    "            for j in range(len(dropout_vals)):\n",
    "                val = pivot.values[i, j]\n",
    "                if not np.isnan(val):\n",
    "                    ax.text(j, i, f\"{val:.2f}\", ha=\"center\", va=\"center\", fontsize=8)\n",
    "\n",
    "    axes[0].set_ylabel(\"Hidden channels\")\n",
    "\n",
    "    # Make room on the right for the colorbar\n",
    "    fig.subplots_adjust(right=0.88)  # squeeze plots a bit to the left\n",
    "\n",
    "    # Dedicated axis for colorbar so it never overlaps\n",
    "    cbar_ax = fig.add_axes([0.90, 0.15, 0.02, 0.7])  # [left, bottom, width, height]\n",
    "    fig.colorbar(last_im, cax=cbar_ax, label=metric)\n",
    "\n",
    "    plt.suptitle(f\"GNN ablation ({metric}) over hidden size and dropout\")\n",
    "    plt.show()\n",
    "plot_gnn_ablation_heatmaps(ablation_results, metric=\"test_f1\", agg=\"max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "e82e95e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_gnn_runtime_vs_quality(ablation_results, metric=\"test_f1\"):\n",
    "    \"\"\"\n",
    "    Scatter plot: train_time_sec vs chosen quality metric, colored by model_type.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(6, 4))\n",
    "\n",
    "    for mtype, sub in ablation_results.groupby(\"model_type\"):\n",
    "        plt.scatter(\n",
    "            sub[\"train_time_sec\"],\n",
    "            sub[metric],\n",
    "            label=mtype.upper(),\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Training time (seconds)\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(f\"GNN efficiency vs {metric}\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example call:\n",
    "plot_gnn_runtime_vs_quality(ablation_results, metric=\"test_f1\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
